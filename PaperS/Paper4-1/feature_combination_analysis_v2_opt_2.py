# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, average_precision_score
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
import itertools
import warnings
import os
import time
import pickle
# from joblib import Parallel, delayed  # –í–∏–¥–∞–ª–µ–Ω–æ joblib

warnings.filterwarnings('ignore')

# === –î–û–î–ê–ù–û: –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Ü–∏–∫–ª—ñ—á–Ω–∏—Ö –æ–∑–Ω–∞–∫ (—è–∫ —É analyse.py) ===
def identify_cyclical_features(feature_names):
    cyclical_pairs = {}
    sin_features = [f for f in feature_names if f.endswith('_sin')]
    for sin_feature in sin_features:
        base_name = sin_feature[:-4]
        cos_feature = f"{base_name}_cos"
        if cos_feature in feature_names:
            cyclical_pairs[base_name] = [sin_feature, cos_feature]
    return cyclical_pairs

def preprocess_features_for_analysis(X):
    feature_names = X.columns
    cyclical_pairs = identify_cyclical_features(feature_names)
    X_processed = pd.DataFrame()
    for feature in feature_names:
        is_part_of_pair = False
        for pair in cyclical_pairs.values():
            if feature in pair:
                is_part_of_pair = True
                break
        if not is_part_of_pair:
            X_processed[feature] = X[feature]
    for base_name, pair in cyclical_pairs.items():
        sin_feature, cos_feature = pair
        X_processed[base_name] = np.sqrt(X[sin_feature]**2 + X[cos_feature]**2)
    return X_processed

class FeatureCombinationAnalyzer:
    def __init__(self, data_path, random_state=42, top_features_count=5):
        self.data_path = data_path
        self.random_state = random_state
        self.top_features_count = top_features_count
        self.optimal_params = {
            'subsample': 1.0,
            'reg_lambda': 2.0,
            'reg_alpha': 0.5,
            'n_estimators': 800,
            'min_child_weight': 3,
            'max_depth': 7,
            'learning_rate': 0.1,
            'gamma': 0.2,
            'colsample_bytree': 0.7,
            'objective': 'binary:logistic',
            'eval_metric': 'aucpr',
            'random_state': random_state,
            'n_jobs': -1,
            'verbosity': 0,
            'tree_method': 'gpu_hist',
            'predictor': 'gpu_predictor',
        }
        self.data = None
        self.feature_names = None
        self.top_feature_names = None
        self.feature_importance_scores = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = None
        self.results = []
        self.best_combinations = {}
        self.extended_results = []  # –î–ª—è —Ä–æ–∑—à–∏—Ä–µ–Ω–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π
        self.results_dir = f"feature_analysis_results_v2_opt_2"
        os.makedirs(self.results_dir, exist_ok=True)
        print(f"üî¨ === –ê–ù–ê–õ–Ü–ó –ö–û–ú–ë–Ü–ù–ê–¶–Ü–ô –¢–û–ü-{self.top_features_count} –û–ó–ù–ê–ö –ó –û–ü–¢–ò–ú–ê–õ–¨–ù–ò–ú–ò –ü–ê–†–ê–ú–ï–¢–†–ê–ú–ò ===")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {self.results_dir}/")
        print(f"üéØ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–∫—Ä–∞—â—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ hyperparameter research")
        print(f"‚ö° –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è: –∞–Ω–∞–ª—ñ–∑ —Ç—ñ–ª—å–∫–∏ —Ç–æ–ø-{self.top_features_count} –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –æ–∑–Ω–∞–∫")
        print(f"üñºÔ∏è –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ —è–∫ PNG —Ñ–∞–π–ª–∏ (non-interactive —Ä–µ–∂–∏–º)")

    def load_and_prepare_data(self):
        print(f"\nüîÑ === –ï–¢–ê–ü 0: –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –¢–ê –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ò–• ===")
        print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑: {self.data_path}")
        self.data = pd.read_csv(self.data_path)
        print(f"üìä –†–æ–∑–º—ñ—Ä –¥–∞–Ω–∏—Ö: {self.data.shape}")
        if 'is_successful' not in self.data.columns:
            raise ValueError("‚ùå –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞ 'is_successful' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞!")
        # –í—ñ–¥–æ–∫—Ä–µ–º–ª—é—î–º–æ —Ü—ñ–ª—å–æ–≤—É –∑–º—ñ–Ω–Ω—É
        X = self.data.drop('is_successful', axis=1)
        y = self.data['is_successful']
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ª–∏—à–µ —á–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
        X = X.select_dtypes(exclude=['object'])
        # === –î–û–î–ê–ù–û: –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ü–∏–∫–ª—ñ—á–Ω–∏—Ö –æ–∑–Ω–∞–∫ ===
        X_processed = preprocess_features_for_analysis(X)
        self.feature_names = list(X_processed.columns)
        print(f"üîç –ó–∞–ª–∏—à–∏–ª–æ—Å—å —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ –ø—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏ —Ü–∏–∫–ª—ñ—á–Ω–∏—Ö: {len(self.feature_names)}")
        # –î—ñ–ª–∏–º–æ –Ω–∞ train/test
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X_processed, y, test_size=0.3, random_state=self.random_state, stratify=y
        )
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        print(f"‚úÖ –î–∞–Ω—ñ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")

    def select_top_features(self):
        print(f"\nüîÑ === –ï–¢–ê–ü 0.5: –í–ò–ó–ù–ê–ß–ï–ù–ù–Ø –¢–û–ü-{self.top_features_count} –û–ó–ù–ê–ö ===")
        importance_model = XGBClassifier(**self.optimal_params)
        importance_model.fit(self.X_train_scaled, self.y_train)
        feature_importance = importance_model.feature_importances_
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        print(f"üìä –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å —É—Å—ñ—Ö –æ–∑–Ω–∞–∫:")
        for i, (_, row) in enumerate(importance_df.iterrows(), 1):
            print(f"  {i:2d}. {row['feature']:<35} {row['importance']:.6f}")
        self.top_feature_names = importance_df.head(self.top_features_count)['feature'].tolist()
        self.feature_importance_scores = importance_df.head(self.top_features_count)
        self.top_feature_indices = [self.feature_names.index(feature) for feature in self.top_feature_names]
        self.X_train_top = self.X_train_scaled[:, self.top_feature_indices]
        self.X_test_top = self.X_test_scaled[:, self.top_feature_indices]
        print(f"‚úÖ –î–∞–Ω—ñ –æ–Ω–æ–≤–ª–µ–Ω—ñ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ —Ç–æ–ø-{self.top_features_count} –æ–∑–Ω–∞–∫–∞–º–∏")

    def test_feature_combination(self, feature_indices, combination_size):
        X_train_combo = self.X_train_top[:, feature_indices]
        X_test_combo = self.X_test_top[:, feature_indices]
        feature_combo_names = [self.top_feature_names[i] for i in feature_indices]
        model = XGBClassifier(**self.optimal_params)
        # –ë–µ–∑ –∫—Ä–æ—Å-–≤–∞–ª—ñ–¥–∞—Ü—ñ—ó: —Ç—ñ–ª—å–∫–∏ train/test
        model.fit(X_train_combo, self.y_train)
        y_pred = model.predict(X_test_combo)
        y_pred_proba = model.predict_proba(X_test_combo)[:, 1]
        metrics = {
            'combination_size': combination_size,
            'feature_indices': feature_indices,
            'feature_names': feature_combo_names,
            'feature_names_str': ' + '.join(feature_combo_names),
            # CV –º–µ—Ç—Ä–∏–∫–∏ –≤–∏–¥–∞–ª–µ–Ω–æ
            # Test –º–µ—Ç—Ä–∏–∫–∏
            'test_f1': f1_score(self.y_test, y_pred),
            'test_accuracy': accuracy_score(self.y_test, y_pred),
            'test_precision': precision_score(self.y_test, y_pred),
            'test_recall': recall_score(self.y_test, y_pred),
            'test_auc_roc': roc_auc_score(self.y_test, y_pred_proba),
            'test_auc_pr': average_precision_score(self.y_test, y_pred_proba),
            # Overfitting gap –Ω–µ —Ä–∞—Ö—É—î–º–æ
        }
        return metrics

    def analyze_all_combinations(self):
        print(f"\nüîÑ === –ï–¢–ê–ü 1: –ê–ù–ê–õ–Ü–ó –í–°–Ü–• –ö–û–ú–ë–Ü–ù–ê–¶–Ü–ô –¢–û–ü-{self.top_features_count} –û–ó–ù–ê–ö ===")
        n_features = self.top_features_count
        # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó (—ñ–Ω–¥–µ–∫—Å–∏ —Ç–∞ —Ä–æ–∑–º—ñ—Ä)
        all_combinations = [(comb, size) for size in range(1, n_features + 1)
                            for comb in itertools.combinations(range(n_features), size)]
        total_combinations = len(all_combinations)
        print(f"üî¢ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π: {total_combinations}")
        print(f"‚ö° –¢–æ–ø –æ–∑–Ω–∞–∫–∏: {', '.join(self.top_feature_names)}")
        start_time = time.time()
        # === –ó–∞–º—ñ—Å—Ç—å Parallel ‚Äî –∑–≤–∏—á–∞–π–Ω–∏–π —Ü–∏–∫–ª ===
        results = []
        counter = total_combinations
        for comb, size in all_combinations:
            print(f"{counter}: {comb}")
            metrics = self.test_feature_combination(comb, size)
            results.append(metrics)
            counter -= 1
        self.results = results
        self.results_df = pd.DataFrame(self.results)
        for size in range(1, n_features + 1):
            size_results = self.results_df[self.results_df['combination_size'] == size]
            if not size_results.empty:
                best_idx = size_results['test_auc_pr'].idxmax()
                self.best_combinations[size] = self.results_df.loc[best_idx]
        total_time = time.time() - start_time
        print(f"\n‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"üìä –û–±—Ä–æ–±–ª–µ–Ω–æ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π: {total_combinations}")

        # === –î–û–î–ê–ù–û: –†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑ –ü–û–°–õ–Ü–î–û–í–ù–ò–ú –¥–æ–¥–∞–≤–∞–Ω–Ω—è–º –∑–∞–ª–∏—à–µ–Ω–∏—Ö –æ–∑–Ω–∞–∫ ===
        print(f"\nüîÑ === –ï–¢–ê–ü 1.5: –ü–û–°–õ–Ü–î–û–í–ù–ï –î–û–î–ê–í–ê–ù–ù–Ø –ó–ê–õ–ò–®–ï–ù–ò–• –û–ó–ù–ê–ö –î–û –¢–û–ü-{self.top_features_count} ===")
        all_features = list(self.feature_names)
        top_set = set(self.top_feature_names)
        rest_features = [f for f in all_features if f not in top_set]
        print(f"–ó–∞–ª–∏—à–µ–Ω—ñ –æ–∑–Ω–∞–∫–∏: {rest_features}")
        self.extended_results = []
        extended_features = list(self.top_feature_names)
        for i, add_feature in enumerate(rest_features):
            extended_features.append(add_feature)
            indices = [self.feature_names.index(f) for f in extended_features]
            X_train_ext = self.X_train_scaled[:, indices]
            X_test_ext = self.X_test_scaled[:, indices]
            model = XGBClassifier(**self.optimal_params)
            model.fit(X_train_ext, self.y_train)
            y_pred = model.predict(X_test_ext)
            y_pred_proba = model.predict_proba(X_test_ext)[:, 1]
            metrics = {
                'combination_size': len(extended_features),
                'feature_indices': indices,
                'feature_names': list(extended_features),
                'feature_names_str': ' + '.join(extended_features),
                'test_f1': f1_score(self.y_test, y_pred),
                'test_accuracy': accuracy_score(self.y_test, y_pred),
                'test_precision': precision_score(self.y_test, y_pred),
                'test_recall': recall_score(self.y_test, y_pred),
                'test_auc_roc': roc_auc_score(self.y_test, y_pred_proba),
                'test_auc_pr': average_precision_score(self.y_test, y_pred_proba),
            }
            self.extended_results.append(metrics)
        self.extended_results_df = pd.DataFrame(self.extended_results)
        print(f"‚úÖ –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑ –∑ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∏–º –Ω–∞—Ä–æ—â—É–≤–∞–Ω–Ω—è–º –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    def _plot_aucpr_vs_num_features(self):
        """Scatter plot: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ vs AUC-PR"""
        plt.figure(figsize=(12, 7))
        x = self.results_df['combination_size']
        y = self.results_df['test_auc_pr']
        plt.scatter(x, y, alpha=0.5, c=x, cmap='viridis', edgecolor='k')
        plt.xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ —É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó', fontsize=14)
        plt.ylabel('AUC-PR (test_auc_pr)', fontsize=14)
        plt.title('–ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å AUC-PR –≤—ñ–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ —É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó', fontsize=16, fontweight='bold')
        # –î–æ–¥–∞—î–º–æ padding –ø–æ –æ—Å—ñ Y
        y_min, y_max = y.min(), y.max()
        y_range = y_max - y_min
        pad = y_range * 0.05 if y_range > 0 else 0.01
        plt.xlim(0.5, self.top_features_count + 0.5)
        plt.ylim(y_min - pad, y_max + pad)
        plt.xticks(range(1, self.top_features_count + 1))
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/aucpr_vs_num_features.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_aucpr_vs_num_features_extended(self):
        """Scatter plot: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ vs AUC-PR (—Ä–æ–∑—à–∏—Ä–µ–Ω–∏–π)"""
        plt.figure(figsize=(12, 7))
        # –û—Å–Ω–æ–≤–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó
        x1 = self.results_df['combination_size']
        y1 = self.results_df['test_auc_pr']
        # –†–æ–∑—à–∏—Ä–µ–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó
        x2 = self.extended_results_df['combination_size'] if hasattr(self, 'extended_results_df') else []
        y2 = self.extended_results_df['test_auc_pr'] if hasattr(self, 'extended_results_df') else []
        # === –û–±'—î–¥–Ω—É—î–º–æ –≤—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ –æ–¥–∏–Ω –¥–∞—Ç–∞—Ñ—Ä–µ–π–º ===
        if len(x2) > 0:
            df_all = pd.concat([self.results_df, self.extended_results_df], ignore_index=True)
        else:
            df_all = self.results_df.copy()
        x = df_all['combination_size']
        y = df_all['test_auc_pr']
        y_min, y_max = y.min(), y.max()  # –í–∏–∑–Ω–∞—á–∞—î–º–æ –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —É –ø—ñ–¥–ø–∏—Å–∞—Ö
        plt.scatter(x, y, alpha=0.7, c=x, cmap='viridis', edgecolor='k', marker='o', s=80)
        # === –î–æ–¥–∞—î–º–æ –ø—ñ–¥–ø–∏—Å–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å AUC-PR –¥–ª—è –∫–æ–∂–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ ===
        for size in sorted(df_all['combination_size'].unique()):
            size_df = df_all[df_all['combination_size'] == size]
            if not size_df.empty:
                idx_max = size_df['test_auc_pr'].idxmax()
                row = size_df.loc[idx_max]
                x_ = row['combination_size']
                y_ = row['test_auc_pr']
                y_offset = (y_max - y_min) * 0.03 if y_max > y_min else 0.01
                plt.text(x_, y_ + y_offset, f'{y_:.3f}', fontsize=8, color='black',
                         ha='center', va='bottom',
                         bbox=dict(facecolor='white', alpha=0.5, edgecolor='none', boxstyle='round,pad=0.08'))
        plt.xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ —É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó', fontsize=14)
        plt.ylabel('AUC-PR (test_auc_pr)', fontsize=14)
        plt.title('AUC-PR –¥–ª—è –≤—Å—ñ—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –æ–∑–Ω–∞–∫', fontsize=16, fontweight='bold')
        # –î–æ–¥–∞—î–º–æ padding –ø–æ –æ—Å—ñ Y
        y_min, y_max = y.min(), y.max()
        y_range = y_max - y_min
        pad = y_range * 0.05 if y_range > 0 else 0.01
        plt.xlim(0.5, df_all['combination_size'].max() + 0.5)
        plt.ylim(y_min - pad, y_max + pad * 2)  # –î–æ–¥–∞—î–º–æ –±—ñ–ª—å—à–µ –º—ñ—Å—Ü—è –∑–≤–µ—Ä—Ö—É
        plt.xticks(range(1, df_all['combination_size'].max() + 1))
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/aucpr_vs_num_features_extended.png', dpi=300, bbox_inches='tight')
        plt.close()

    def create_visualizations(self):
        print(f"\nüîÑ === –ï–¢–ê–ü 2: –°–¢–í–û–†–ï–ù–ù–Ø –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–ô ===")
        print(f"üñºÔ∏è –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ matplotlib backend 'Agg' (non-interactive)")
        try:
            print(f"  üìä –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ –º–µ—Ç—Ä–∏–∫ –ø–æ —Ä–æ–∑–º—ñ—Ä–∞—Ö...")
            self._plot_metrics_by_size()
            print(f"  üèÜ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ —Ç–æ–ø –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π...")
            self._plot_top_combinations()
            print(f"  üéØ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ–∑—É —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫...")
            self._plot_individual_features()
            print(f"  üî• –°—Ç–≤–æ—Ä–µ–Ω–Ω—è heatmap –∫–æ—Ä–µ–ª—è—Ü—ñ—ó...")
            self._plot_metrics_correlation()
            print(f"  üìà –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ–∑—É –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è...")
            self._plot_overfitting_analysis()
            print(f"  üü¢ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è scatter-–≥—Ä–∞—Ñ—ñ–∫–∞ AUC-PR vs –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫...")
            self._plot_aucpr_vs_num_features()
            print(f"  üü£ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä–æ–∑—à–∏—Ä–µ–Ω–æ–≥–æ scatter-–≥—Ä–∞—Ñ—ñ–∫–∞ AUC-PR vs –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫...")
            self._plot_aucpr_vs_num_features_extended()
            print(f"‚úÖ –í—Å—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó —Å—Ç–≤–æ—Ä–µ–Ω–æ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–æ!")
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π: {e}")
            print(f"‚ö†Ô∏è –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –±–µ–∑ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π...")
            return False
        return True

    def _plot_metrics_by_size(self):
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        fig.suptitle('–ê–Ω–∞–ª—ñ–∑ –º–µ—Ç—Ä–∏–∫ –ø–æ —Ä–æ–∑–º—ñ—Ä–∞—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –æ–∑–Ω–∞–∫', fontsize=16, fontweight='bold')
        metrics = ['test_f1', 'test_auc_pr', 'test_auc_roc']
        titles = ['F1 Score', 'AUC-PR (Primary)', 'AUC-ROC']
        for i, (metric, title) in enumerate(zip(metrics, titles)):
            ax = axes[i]
            sizes = sorted(self.results_df['combination_size'].unique())
            data_by_size = [self.results_df[self.results_df['combination_size'] == size][metric]
                           for size in sizes]
            bp = ax.boxplot(data_by_size, labels=sizes, patch_artist=True)
            colors = plt.cm.viridis(np.linspace(0, 1, len(sizes)))
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
            ax.set_title(title, fontweight='bold')
            ax.set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –≤ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó')
            ax.set_ylabel(title)
            ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/metrics_by_combination_size.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_top_combinations(self):
        top_combinations = self.results_df.nlargest(10, 'test_auc_pr')
        fig, ax1 = plt.subplots(1, 1, figsize=(12, 8))
        bars1 = ax1.barh(range(len(top_combinations)), top_combinations['test_auc_pr'],
                        color=plt.cm.viridis(np.linspace(0, 1, len(top_combinations))))
        ax1.set_yticks(range(len(top_combinations)))
        ax1.set_yticklabels([f"{row['combination_size']} –æ–∑–Ω–∞–∫:\n{row['feature_names_str'][:50]}..."
                            if len(row['feature_names_str']) > 50
                            else f"{row['combination_size']} –æ–∑–Ω–∞–∫:\n{row['feature_names_str']}"
                            for _, row in top_combinations.iterrows()], fontsize=10)
        ax1.set_xlabel('Test AUC-PR Score')
        ax1.set_title('–¢–æ–ø-10 –Ω–∞–π–∫—Ä–∞—â–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –æ–∑–Ω–∞–∫ (AUC-PR)', fontweight='bold')
        ax1.grid(True, alpha=0.3)
        for i, bar in enumerate(bars1):
            width = bar.get_width()
            ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2,
                    f'{width:.4f}', ha='left', va='center', fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/top_combinations.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_individual_features(self):
        single_features = self.results_df[self.results_df['combination_size'] == 1].copy()
        single_features = single_features.sort_values('test_auc_pr', ascending=True)
        fig, ax1 = plt.subplots(1, 1, figsize=(12, 8))
        bars = ax1.barh(range(len(single_features)), single_features['test_auc_pr'],
                       color=plt.cm.RdYlGn(np.linspace(0.3, 1, len(single_features))))
        ax1.set_yticks(range(len(single_features)))
        ax1.set_yticklabels(single_features['feature_names_str'], fontsize=12)
        ax1.set_xlabel('Test AUC-PR Score')
        ax1.set_title('–†–µ–π—Ç–∏–Ω–≥ —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ (AUC-PR)', fontweight='bold', fontsize=14)
        ax1.grid(True, alpha=0.3)
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2,
                    f'{width:.4f}', ha='left', va='center', fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/individual_features_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_metrics_correlation(self):
        metrics_cols = ['test_f1', 'test_auc_pr', 'test_auc_roc', 'test_accuracy',
                       'test_precision', 'test_recall']
        correlation_matrix = self.results_df[metrics_cols].corr()
        plt.figure(figsize=(10, 8))
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                   square=True, linewidths=0.5, cbar_kws={"shrink": .8}, fmt='.3f')
        plt.title('–ö–æ—Ä–µ–ª—è—Ü—ñ—è –º—ñ–∂ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —è–∫–æ—Å—Ç—ñ', fontsize=16, fontweight='bold', pad=20)
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/metrics_correlation.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_overfitting_analysis(self):
        # –í–∏–¥–∞–ª—è—î–º–æ —Ü–µ–π –≥—Ä–∞—Ñ—ñ–∫, –±–æ overfitting gap –±—ñ–ª—å—à–µ –Ω–µ —Ä–∞—Ö—É—î—Ç—å—Å—è
        pass

    def generate_report(self):
        print(f"\nüîÑ === –ï–¢–ê–ü 3: –ì–ï–ù–ï–†–ê–¶–Ü–Ø –ó–í–Ü–¢–£ ===")
        self.results_df.to_csv(f'{self.results_dir}/all_combinations_results.csv', index=False)
        top_10 = self.results_df.nlargest(10, 'test_auc_pr')
        top_10.to_csv(f'{self.results_dir}/top_10_combinations.csv', index=False)
        report_path = f'{self.results_dir}/analysis_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# üî¨ –ó–í–Ü–¢ –ê–ù–ê–õ–Ü–ó–£ –ö–û–ú–ë–Ü–ù–ê–¶–Ü–ô –û–ó–ù–ê–ö\n\n")
            f.write("## üìä –ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è\n")
            f.write(f"- –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ —É –¥–∞—Ç–∞—Å–µ—Ç—ñ: {len(self.feature_names)}\n")
            f.write(f"- –¢–æ–ø –æ–∑–Ω–∞–∫ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É: {self.top_features_count}\n")
            f.write(f"- –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π: {len(self.results_df)}\n")
            f.write(f"- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ XGBoost\n")
            f.write(f"- –û—Å–Ω–æ–≤–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó: AUC-PR (Area Under Precision-Recall Curve)\n\n")
            f.write("## üèÜ –í–∏–±—Ä–∞–Ω—ñ —Ç–æ–ø –æ–∑–Ω–∞–∫–∏ (–∑–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é XGBoost)\n")
            for i, (_, row) in enumerate(self.feature_importance_scores.iterrows(), 1):
                f.write(f"{i}. **{row['feature']}** - –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å: {row['importance']:.6f}\n")
            f.write("\n")
            f.write("## üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ XGBoost\n")
            for param, value in self.optimal_params.items():
                f.write(f"- {param}: {value}\n")
            f.write("\n")
            f.write("## üèÜ –¢–û–ü-5 –ù–ê–ô–ö–†–ê–©–ò–• –ö–û–ú–ë–Ü–ù–ê–¶–Ü–ô (–∑–∞ AUC-PR)\n\n")
            top_5 = self.results_df.nlargest(5, 'test_auc_pr')
            for i, (_, row) in enumerate(top_5.iterrows(), 1):
                f.write(f"### {i}. –ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è –∑ {row['combination_size']} –æ–∑–Ω–∞–∫\n")
                f.write(f"- **–û–∑–Ω–∞–∫–∏**: {row['feature_names_str']}\n")
                f.write(f"- **Test AUC-PR**: {row['test_auc_pr']:.6f} (Primary)\n")
                f.write(f"- **Test F1 Score**: {row['test_f1']:.6f}\n")
                f.write(f"- **Test AUC-ROC**: {row['test_auc_roc']:.6f}\n")
                f.write(f"- **Test Accuracy**: {row['test_accuracy']:.6f}\n")
                # CV —Ç–∞ overfitting gap –Ω–µ –≤–∏–≤–æ–¥–∏–º–æ
            single_features = self.results_df[self.results_df['combination_size'] == 1].nlargest(5, 'test_auc_pr')
            f.write("## ü•á –¢–û–ü-5 –Ü–ù–î–ò–í–Ü–î–£–ê–õ–¨–ù–ò–• –û–ó–ù–ê–ö (–∑–∞ AUC-PR)\n\n")
            for i, (_, row) in enumerate(single_features.iterrows(), 1):
                f.write(f"### {i}. {row['feature_names_str']}\n")
                f.write(f"- **Test AUC-PR**: {row['test_auc_pr']:.6f} (Primary)\n")
                f.write(f"- **Test F1 Score**: {row['test_f1']:.6f}\n")
                f.write(f"- **Test AUC-ROC**: {row['test_auc_roc']:.6f}\n")
                f.write(f"- **Test Accuracy**: {row['test_accuracy']:.6f}\n\n")
            f.write("## üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –†–û–ó–ú–Ü–†–ê–• –ö–û–ú–ë–Ü–ù–ê–¶–Ü–ô\n\n")
            f.write("| –†–æ–∑–º—ñ—Ä | –ö—ñ–ª—å–∫—ñ—Å—Ç—å | –°–µ—Ä–µ–¥–Ω—ñ–π AUC-PR | –ù–∞–π–∫—Ä–∞—â–∏–π AUC-PR | –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è |\n")
            f.write("|--------|-----------|-----------------|------------------|-----------------------|\n")
            for size in sorted(self.results_df['combination_size'].unique()):
                size_data = self.results_df[self.results_df['combination_size'] == size]
                f.write(f"| {size} | {len(size_data)} | {size_data['test_auc_pr'].mean():.6f} | {size_data['test_auc_pr'].max():.6f} | {size_data['test_auc_pr'].std():.6f} |\n")
            best_overall = self.results_df.loc[self.results_df['test_auc_pr'].idxmax()]
            best_single = single_features.iloc[0]
            f.write("\n## üéØ –ö–õ–Æ–ß–û–í–Ü –í–ò–°–ù–û–í–ö–ò\n\n")
            f.write("### 1. –ù–∞–π–∫—Ä–∞—â–∞ –∑–∞–≥–∞–ª—å–Ω–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è\n")
            f.write(f"- **{best_overall['combination_size']} –æ–∑–Ω–∞–∫**: {best_overall['feature_names_str']}\n")
            f.write(f"- **AUC-PR**: {best_overall['test_auc_pr']:.6f} (Primary)\n")
            f.write(f"- **F1 Score**: {best_overall['test_f1']:.6f}\n")
            improvement = ((best_overall['test_auc_pr'] / best_single['test_auc_pr'] - 1) * 100)
            f.write(f"- –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤—ñ–¥–Ω–æ—Å–Ω–æ –Ω–∞–π–∫—Ä–∞—â–æ—ó —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω–æ—ó –æ–∑–Ω–∞–∫–∏: {improvement:.2f}%\n\n")
            f.write("### 2. –ù–∞–π–∫—Ä–∞—â–∞ —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω–∞ –æ–∑–Ω–∞–∫–∞\n")
            f.write(f"- **{best_single['feature_names_str']}**\n")
            f.write(f"- **AUC-PR**: {best_single['test_auc_pr']:.6f} (Primary)\n")
            f.write(f"- **F1 Score**: {best_single['test_f1']:.6f}\n\n")
            # Overfitting gap –Ω–µ –≤–∏–≤–æ–¥–∏–º–æ
            f.write("\n## üìÅ –°—Ç–≤–æ—Ä–µ–Ω—ñ —Ñ–∞–π–ª–∏\n")
            f.write("- `all_combinations_results.csv` - –ü–æ–≤–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—Å—ñ—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π\n")
            f.write("- `top_10_combinations.csv` - –¢–æ–ø-10 –Ω–∞–π–∫—Ä–∞—â–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π (–∑–∞ AUC-PR)\n")
            f.write("- `feature_importance_all.csv` - –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫\n")
            f.write("- `top_features_selected.csv` - –í–∏–±—Ä–∞–Ω—ñ —Ç–æ–ø –æ–∑–Ω–∞–∫–∏\n")
            f.write("- –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó —É —Ñ–æ—Ä–º–∞—Ç—ñ PNG\n")
        print(f"üìÑ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {report_path}")
        print(f"üíæ CSV —Ñ–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –ø–∞–ø—Ü—ñ: {self.results_dir}/")

    def run_analysis(self):
        print(f"üöÄ === –ü–û–ß–ê–¢–û–ö –ê–ù–ê–õ–Ü–ó–£ –ö–û–ú–ë–Ü–ù–ê–¶–Ü–ô –û–ó–ù–ê–ö ===")
        start_time = time.time()
        try:
            self.load_and_prepare_data()
            self.select_top_features()
            self.analyze_all_combinations()
            visualization_success = self.create_visualizations()
            if not visualization_success:
                print(f"‚ö†Ô∏è –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω—ñ —á–µ—Ä–µ–∑ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏")
                print(f"üìä –ê–ª–µ –≤—Å—ñ –¥–∞–Ω—ñ —Ç–∞ –∑–≤—ñ—Ç–∏ –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ!")
            self.generate_report()
            total_time = time.time() - start_time
            print(f"\n‚úÖ === –ê–ù–ê–õ–Ü–ó –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–Ü–®–ù–û ===")
            print(f"‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
            print(f"üìä –ü—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–æ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π: {len(self.results_df)}")
            print(f"üèÜ –ù–∞–π–∫—Ä–∞—â–∏–π AUC-PR Score: {self.results_df['test_auc_pr'].max():.6f}")
            print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {self.results_dir}/")
            print(f"\nü•á –¢–û–ü-3 –ù–ê–ô–ö–†–ê–©–ò–• –ö–û–ú–ë–Ü–ù–ê–¶–Ü–ô (–∑–∞ AUC-PR):")
            top_3 = self.results_df.nlargest(3, 'test_auc_pr')
            for i, (_, row) in enumerate(top_3.iterrows(), 1):
                print(f"{i}. {row['feature_names_str']} (AUC-PR: {row['test_auc_pr']:.6f})")
        except Exception as e:
            print(f"‚ùå –ü–û–ú–ò–õ–ö–ê: {e}")
            raise

if __name__ == '__main__':
    analyzer = FeatureCombinationAnalyzer(
        data_path=r'D:\PROJECTs\MY\ScientificCalculations\SC\ScientificCalculations\PaperS\Paper4\preprocessed_data2.csv',
        random_state=42,
        top_features_count=12
    )
    analyzer.load_and_prepare_data()
    analyzer.select_top_features()
    analyzer.analyze_all_combinations()
    analyzer.create_visualizations()  # –ó–∞ –ø–æ—Ç—Ä–µ–±–∏
    analyzer.generate_report()        # –ó–∞ –ø–æ—Ç—Ä–µ–±–∏