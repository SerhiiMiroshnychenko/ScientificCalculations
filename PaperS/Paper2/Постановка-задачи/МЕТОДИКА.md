Для проведення дослідження було зібрано історичні дані за період 2017-2025 років для 7849 клієнтів у B2B-сегменті,
що охоплює 86794 замовлення, з яких 32031 класифіковано як неуспішні та 54763 як успішні.
Для аналізу було відібрано 19 числових характеристик замовлень.

Запропонована методологія статистичного аналізу ґрунтується на комплексному підході, що включає попередню обробку даних,
статистичний аналіз та моделювання прогнозування. Початковий етап передбачає завантаження даних з CSV-файлу, 
після чого здійснюється ретельна перевірка та обробка пропущених значень. З метою забезпечення логічної цілісності 
даних проводиться заміна від'ємних значень нулями для тих характеристик, де негативні значення не мають фізичного 
або бізнесового сенсу [1 - посилання що підтверджує такий підхід], а також виявлення та обробка викидів.

Наступним етапом дослідницької процедури є стратифіковане розбиття даних на тренувальну та тестову вибірки відповідно
до цільової змінної успішності замовлення. Тренувальна вибірка формується з 70% від загального обсягу даних і становить
60755 записів, тоді як тестова вибірка включає решту 30% даних (26039 записів) [2 - посилання що обґрунтовує таке розділення].

Центральним компонентом методології є аналіз розподілу даних та обґрунтований вибір відповідних статистичних тестів. 
Оцінка нормальності розподілу для кожної досліджуваної ознаки здійснюється за допомогою трьох статистичних тестів: 
Шапіро-Вілка [3 - посилання на опис тесту Шапіро-Вілка], 
Д'Агостіно-Пірсона [4 - посилання на опис тесту Д'Агостіно-Пірсона]
та Андерсона-Дарлінга [5 - посилання на опис тесту Андерсона-Дарлінга]. 
Автоматичний вибір між параметричними та непараметричними методами аналізу базується на результатах тестів нормальності,
що забезпечує методологічну гнучкість та статистичну коректність.

Статистичний аналіз значущості ознак диференціюється залежно від характеру розподілу даних. Для нормально розподілених 
даних застосовується t-тест [6 - посилання на опис t-тесту] в поєднанні з 
коефіцієнтом Cohen's d для оцінки розміру ефекту [7 - посилання на опис коефіцієнта Cohen's d].
У випадку ненормального розподілу використовується Mann-Whitney U тест [8 - посилання на опис U тест Mann-Whitney] 
та показник AUC (Area Under Curve) для визначення розміру ефекту [9 - посилання на опис показник AUC (Area Under Curve)].
Статистично значущі ознаки ідентифікуються на основі p-значень з рівнем значущості α = 0.05 [10 - посилання на обгрунтування такого значення],
після чого розраховується вага кожної значущої ознаки відповідно до розміру ефекту.

Побудова статистичної моделі прогнозування передбачає визначення порогових значень для кожної статистично значущої
ознаки та розрахунок ймовірності успішності замовлення на основі зважених внесків цих ознак.
Класифікація замовлень здійснюється за пороговим значенням ймовірності 0.5 [11 - посилання на обґрунтування такого значення].

Оцінка ефективності розробленої моделі включає розрахунок комплексу метрик: точність (accuracy),
збалансована точність (balanced accuracy), прецизійність (precision), повнота (recall), F1-міра, ROC AUC та
специфічність (specificity) [12 - посилання на опис всіх цих метрик].

Особливістю запропонованої методології є автоматичний вибір методів статистичного аналізу на основі характеру
розподілу даних. Це дозволяє отримати більш точні та надійні результати в умовах різнорідних даних, 
характерних для реальних бізнес-процесів [13 - посилання на те що реальні бізнес-процеси мають різнорідні данні].

Для виявлення ненормального розподілу використовувались три різні тести: тест Шапіро-Вілка, тест Д'Агостіно-Пірсона
та тест Андерсона-Дарлінга. Визначення нормальності для кожної групи (якщо хоча б 2 тести підтверджують)
дозволило автоматично обрати між параметричними та непараметричними методами аналізу.
Для обробки викидів застосовано метод IQR (Interquartile Range) [14 - посилання на опис методу IQR (Interquartile Range)]
з порогом 1.5, що є стандартним підходом у статистичному аналізі [15 - посилання яке підтверджує це].
Викиди були не видалені, а обмежені (вінсоризовані), що дозволило зберегти
обсяг даних при одночасному зменшенні впливу екстремальних значень [16 - посилання на обґрунтування такого підходу].

# УВАГА: ПОСИЛАТИСЯ ТІЛЬКИ НА НАУКОВІ СТАТТІ ЧЕРЕЗ DOI. Загальні посилання на Вікіпедію або сайти - НЕ ПРИПУСТИМІ!

