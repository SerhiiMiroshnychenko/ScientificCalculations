"""
üî¨ –ê–ù–ê–õ–Ü–ó –¢–ê –í–ò–†–Ü–®–ï–ù–ù–Ø –ú–£–õ–¨–¢–ò–ö–û–õ–Ü–ù–ï–ê–†–ù–û–°–¢–Ü –í –û–ó–ù–ê–ö–ê–•

–¶–µ–π —Å–∫—Ä–∏–ø—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ–π –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏ —Ç–∞ –∑–∞—Å—Ç–æ—Å–æ–≤—É—î
—Å—É—á–∞—Å–Ω—ñ –º–µ—Ç–æ–¥–∏ –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º–∏ –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ.

–ï—Ç–∞–ø–∏ —Ä–æ–±–æ—Ç–∏:
1. –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∞–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏ (–∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è, —Ç–µ–ø–ª–æ–≤–æ–≤–∞ –∫–∞—Ä—Ç–∞)
2. VIF (Variance Inflation Factor) –∞–Ω–∞–ª—ñ–∑ –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ
3. –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è:
   - PCA (Principal Component Analysis)
   - ICA (Independent Component Analysis)
   - Ridge/Lasso/Elastic Net —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
   - Feature selection –º–µ—Ç–æ–¥–∏
4. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Ä—ñ–∑–Ω–∏—Ö –ø—ñ–¥—Ö–æ–¥—ñ–≤
5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —â–æ–¥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è

–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 2024
"""

import pandas as pd
import numpy as np

# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è non-interactive backend –¥–ª—è matplotlib
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, average_precision_score, roc_curve, precision_recall_curve
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA, FastICA
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from statsmodels.stats.outliers_influence import variance_inflation_factor
import warnings
import os
import time
from itertools import combinations

warnings.filterwarnings('ignore')

class MulticollinearityAnalyzer:
    """
    –ö–ª–∞—Å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É —Ç–∞ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ
    """
    
    def __init__(self, data_path, random_state=42, correlation_threshold=0.8, vif_threshold=5.0):
        self.data_path = data_path
        self.random_state = random_state
        self.correlation_threshold = correlation_threshold  # –ü–æ—Ä—ñ–≥ –¥–ª—è —Å–∏–ª—å–Ω–æ—ó –∫–æ—Ä–µ–ª—è—Ü—ñ—ó
        self.vif_threshold = vif_threshold  # –ü–æ—Ä—ñ–≥ VIF –¥–ª—è –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ
        
        # –û–ü–¢–ò–ú–ê–õ–¨–ù–Ü –ü–ê–†–ê–ú–ï–¢–†–ò XGBOOST
        self.optimal_params = {
            'subsample': 1.0,
            'reg_lambda': 2.0,
            'reg_alpha': 0.5,
            'n_estimators': 800,
            'min_child_weight': 3,
            'max_depth': 7,
            'learning_rate': 0.1,
            'gamma': 0.2,
            'colsample_bytree': 0.7,
            'objective': 'binary:logistic',
            'eval_metric': 'aucpr',
            'random_state': random_state,
            'n_jobs': -1,
            'verbosity': 0
        }
        
        # –ó–º—ñ–Ω–Ω—ñ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        self.data = None
        self.feature_names = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = None
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É
        self.correlation_matrix = None
        self.vif_scores = None
        self.highly_correlated_pairs = []
        self.high_vif_features = []
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ä—ñ–∑–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤
        self.method_results = {}
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞–ø–∫–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        self.results_dir = "multicollinearity_analysis_results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        print(f"üî¨ === –ê–ù–ê–õ–Ü–ó –¢–ê –í–ò–†–Ü–®–ï–ù–ù–Ø –ú–£–õ–¨–¢–ò–ö–û–õ–Ü–ù–ï–ê–†–ù–û–°–¢–Ü ===")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {self.results_dir}/")
        print(f"üéØ –ü–æ—Ä—ñ–≥ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó: {self.correlation_threshold}")
        print(f"üéØ –ü–æ—Ä—ñ–≥ VIF: {self.vif_threshold}")
        print(f"üñºÔ∏è –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ —è–∫ PNG —Ñ–∞–π–ª–∏")

    def load_and_prepare_data(self):
        """
        –ï–¢–ê–ü 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
        """
        print(f"\nüîÑ === –ï–¢–ê–ü 1: –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –¢–ê –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ò–• ===")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
        print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑: {self.data_path}")
        self.data = pd.read_csv(self.data_path)
        print(f"üìä –†–æ–∑–º—ñ—Ä –¥–∞–Ω–∏—Ö: {self.data.shape}")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
        if 'is_successful' not in self.data.columns:
            raise ValueError("‚ùå –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞ 'is_successful' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞!")
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∑–Ω–∞–∫
        exclude_columns = ['order_id', 'is_successful', 'create_date', 'partner_id']
        
        numeric_features = [
            'order_amount', 'order_messages', 'order_changes',
            'partner_success_rate', 'partner_total_orders', 'partner_order_age_days',
            'partner_avg_amount', 'partner_success_avg_amount', 'partner_fail_avg_amount',
            'partner_total_messages', 'partner_success_avg_messages', 'partner_fail_avg_messages',
            'partner_avg_changes', 'partner_success_avg_changes', 'partner_fail_avg_changes'
        ]
        
        # –í—ñ–¥–±–∏—Ä–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ç—ñ –æ–∑–Ω–∞–∫–∏, —â–æ —Ä–µ–∞–ª—å–Ω–æ —î –≤ –¥–∞–Ω–∏—Ö
        self.feature_names = [col for col in numeric_features if col in self.data.columns]
        
        print(f"üîç –ó–Ω–∞–π–¥–µ–Ω–æ –æ–∑–Ω–∞–∫: {len(self.feature_names)}")
        print(f"üìã –°–ø–∏—Å–æ–∫ –æ–∑–Ω–∞–∫: {self.feature_names}")
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
        print(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –æ–∑–Ω–∞–∫...")
        data_with_features = self._create_additional_features(self.data)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å–ø–∏—Å–æ–∫ –æ–∑–Ω–∞–∫ –≤–∫–ª—é—á–∞—é—á–∏ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ
        additional_features = [col for col in data_with_features.columns
                             if col.endswith('_diff') or col.endswith('_ratio')]
        all_features = self.feature_names + additional_features
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ X —Ç–∞ y
        X = data_with_features[all_features]
        y = data_with_features['is_successful']
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å–ø–∏—Å–æ–∫ –æ–∑–Ω–∞–∫
        self.feature_names = all_features
        
        print(f"üéØ –†–æ–∑–ø–æ–¥—ñ–ª —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó:")
        print(f"  ‚Ä¢ –£—Å–ø—ñ—à–Ω—ñ –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è: {y.sum()} ({y.mean():.1%})")
        print(f"  ‚Ä¢ –ù–µ—É—Å–ø—ñ—à–Ω—ñ –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è: {(~y.astype(bool)).sum()} ({(1-y.mean()):.1%})")
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test (70%/30%)
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.3, random_state=self.random_state, stratify=y
        )
        
        print(f"üìä –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö:")
        print(f"  ‚Ä¢ –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ –≤–∏–±—ñ—Ä–∫–∞: {self.X_train.shape[0]} –∑—Ä–∞–∑–∫—ñ–≤")
        print(f"  ‚Ä¢ –¢–µ—Å—Ç–æ–≤–∞ –≤–∏–±—ñ—Ä–∫–∞: {self.X_test.shape[0]} –∑—Ä–∞–∑–∫—ñ–≤")
        
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –æ–∑–Ω–∞–∫
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print(f"‚úÖ –î–∞–Ω—ñ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")

    def _create_additional_features(self, df):
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ (—è–∫ –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É —Å–∫—Ä–∏–ø—Ç—ñ)
        """
        data_copy = df.copy()
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
        if 'partner_success_avg_amount' in data_copy.columns and 'partner_fail_avg_amount' in data_copy.columns:
            data_copy['success_fail_amount_diff'] = data_copy['partner_success_avg_amount'] - data_copy['partner_fail_avg_amount']
            data_copy['success_fail_amount_ratio'] = data_copy['partner_success_avg_amount'] / (data_copy['partner_fail_avg_amount'] + 1e-8)
        
        if 'partner_success_avg_messages' in data_copy.columns and 'partner_fail_avg_messages' in data_copy.columns:
            data_copy['success_fail_messages_diff'] = data_copy['partner_success_avg_messages'] - data_copy['partner_fail_avg_messages']
            data_copy['success_fail_messages_ratio'] = data_copy['partner_success_avg_messages'] / (data_copy['partner_fail_avg_messages'] + 1e-8)
        
        if 'partner_success_avg_changes' in data_copy.columns and 'partner_fail_avg_changes' in data_copy.columns:
            data_copy['success_fail_changes_diff'] = data_copy['partner_success_avg_changes'] - data_copy['partner_fail_avg_changes']
            data_copy['success_fail_changes_ratio'] = data_copy['partner_success_avg_changes'] / (data_copy['partner_fail_avg_changes'] + 1e-8)
        
        # –ó–∞–º—ñ–Ω—é—î–º–æ inf —Ç–∞ -inf –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ NaN, –ø–æ—Ç—ñ–º –Ω–∞ 0
        data_copy = data_copy.replace([np.inf, -np.inf], np.nan).fillna(0)
        
        return data_copy 

    def analyze_correlation(self):
        """
        –ï–¢–ê–ü 2: –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏
        """
        print(f"\nüîÑ === –ï–¢–ê–ü 2: –ê–ù–ê–õ–Ü–ó –ö–û–†–ï–õ–Ø–¶–Ü–á –ú–Ü–ñ –û–ó–ù–ê–ö–ê–ú–ò ===")
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ
        print(f"üìä –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ...")
        self.correlation_matrix = pd.DataFrame(
            self.X_train_scaled, 
            columns=self.feature_names
        ).corr()
        
        # –ü–æ—à—É–∫ —Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—é—é—á–∏—Ö –ø–∞—Ä
        print(f"üîç –ü–æ—à—É–∫ –ø–∞—Ä –∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é > {self.correlation_threshold}...")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –≤–µ—Ä—Ö–Ω—ñ–π —Ç—Ä–∏–∫—É—Ç–Ω–∏–∫ –º–∞—Ç—Ä–∏—Ü—ñ (–±–µ–∑ –¥—ñ–∞–≥–æ–Ω–∞–ª—ñ)
        mask = np.triu(np.ones_like(self.correlation_matrix, dtype=bool), k=1)
        high_corr_pairs = []
        
        for i in range(len(self.feature_names)):
            for j in range(i+1, len(self.feature_names)):
                corr_value = abs(self.correlation_matrix.iloc[i, j])
                if corr_value > self.correlation_threshold:
                    high_corr_pairs.append({
                        'feature1': self.feature_names[i],
                        'feature2': self.feature_names[j],
                        'correlation': self.correlation_matrix.iloc[i, j],
                        'abs_correlation': corr_value
                    })
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –∞–±—Å–æ–ª—é—Ç–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º –∫–æ—Ä–µ–ª—è—Ü—ñ—ó
        self.highly_correlated_pairs = sorted(high_corr_pairs, 
                                            key=lambda x: x['abs_correlation'], 
                                            reverse=True)
        
        print(f"üéØ –ó–Ω–∞–π–¥–µ–Ω–æ {len(self.highly_correlated_pairs)} –ø–∞—Ä –∑ –≤–∏—Å–æ–∫–æ—é –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é:")
        for pair in self.highly_correlated_pairs:
            print(f"  ‚Ä¢ {pair['feature1']} ‚Üî {pair['feature2']}: {pair['correlation']:.4f}")
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ
        self._plot_correlation_matrix()
        
        print(f"‚úÖ –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    def analyze_vif(self):
        """
        –ï–¢–ê–ü 3: VIF (Variance Inflation Factor) –∞–Ω–∞–ª—ñ–∑
        """
        print(f"\nüîÑ === –ï–¢–ê–ü 3: VIF –ê–ù–ê–õ–Ü–ó –ú–£–õ–¨–¢–ò–ö–û–õ–Ü–ù–ï–ê–†–ù–û–°–¢–Ü ===")
        
        print(f"üìä –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ VIF –¥–ª—è –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫...")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame –¥–ª—è VIF —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É
        vif_data = pd.DataFrame(self.X_train_scaled, columns=self.feature_names)
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ VIF –¥–ª—è –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏
        vif_scores = []
        for i, feature in enumerate(self.feature_names):
            try:
                vif_value = variance_inflation_factor(vif_data.values, i)
                vif_scores.append({
                    'feature': feature,
                    'vif': vif_value
                })
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É VIF –¥–ª—è {feature}: {e}")
                vif_scores.append({
                    'feature': feature,
                    'vif': np.inf
                })
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        self.vif_scores = pd.DataFrame(vif_scores).sort_values('vif', ascending=False)
        
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ–∑–Ω–∞–∫–∏ –∑ –≤–∏—Å–æ–∫–∏–º VIF
        self.high_vif_features = self.vif_scores[
            self.vif_scores['vif'] > self.vif_threshold
        ]['feature'].tolist()
        
        print(f"üìã VIF –æ—Ü—ñ–Ω–∫–∏ –¥–ª—è –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫:")
        for _, row in self.vif_scores.iterrows():
            status = "üî¥ –í–ò–°–û–ö–ò–ô" if row['vif'] > self.vif_threshold else "üü¢ –ù–û–†–ú–ê–õ–¨–ù–ò–ô"
            print(f"  ‚Ä¢ {row['feature']:<35} VIF: {row['vif']:.2f} {status}")
        
        print(f"\nüéØ –û–∑–Ω–∞–∫–∏ –∑ VIF > {self.vif_threshold}: {len(self.high_vif_features)}")
        for feature in self.high_vif_features:
            vif_value = self.vif_scores[self.vif_scores['feature'] == feature]['vif'].iloc[0]
            print(f"  ‚Ä¢ {feature}: {vif_value:.2f}")
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó VIF
        self._plot_vif_scores()
        
        print(f"‚úÖ VIF –∞–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    def _plot_correlation_matrix(self):
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ
        """
        print(f"üé® –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ...")
        
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
            
            # –ü–æ–≤–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è
            sns.heatmap(self.correlation_matrix, 
                       annot=True, 
                       cmap='coolwarm', 
                       center=0, 
                       square=True,
                       fmt='.2f',
                       cbar_kws={'shrink': 0.8},
                       ax=ax1)
            ax1.set_title('–ü–æ–≤–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è –æ–∑–Ω–∞–∫', fontsize=14, fontweight='bold')
            ax1.tick_params(axis='x', rotation=45)
            ax1.tick_params(axis='y', rotation=0)
            
            # –ú–∞—Ç—Ä–∏—Ü—è —Ç—ñ–ª—å–∫–∏ —Å–∏–ª—å–Ω–∏—Ö –∫–æ—Ä–µ–ª—è—Ü—ñ–π
            high_corr_matrix = self.correlation_matrix.copy()
            mask = np.abs(high_corr_matrix) < self.correlation_threshold
            high_corr_matrix[mask] = 0
            
            sns.heatmap(high_corr_matrix, 
                       annot=True, 
                       cmap='coolwarm', 
                       center=0, 
                       square=True,
                       fmt='.2f',
                       cbar_kws={'shrink': 0.8},
                       ax=ax2)
            ax2.set_title(f'–°–∏–ª—å–Ω—ñ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó (|r| > {self.correlation_threshold})', fontsize=14, fontweight='bold')
            ax2.tick_params(axis='x', rotation=45)
            ax2.tick_params(axis='y', rotation=0)
            
            plt.tight_layout()
            plt.savefig(f'{self.results_dir}/correlation_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {self.results_dir}/correlation_analysis.png")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∫–æ—Ä–µ–ª—è—Ü—ñ—ó: {e}")

    def _plot_vif_scores(self):
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó VIF –æ—Ü—ñ–Ω–æ–∫
        """
        print(f"üé® –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó VIF –æ—Ü—ñ–Ω–æ–∫...")
        
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
            
            # –ì—Ä–∞—Ñ–∏–∫ VIF –¥–ª—è –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫
            colors = ['red' if vif > self.vif_threshold else 'green' for vif in self.vif_scores['vif']]
            
            ax1.barh(range(len(self.vif_scores)), self.vif_scores['vif'], color=colors, alpha=0.7)
            ax1.set_yticks(range(len(self.vif_scores)))
            ax1.set_yticklabels(self.vif_scores['feature'], fontsize=10)
            ax1.axvline(x=self.vif_threshold, color='red', linestyle='--', linewidth=2, 
                       label=f'–ü–æ—Ä—ñ–≥ VIF = {self.vif_threshold}')
            ax1.set_xlabel('VIF Score')
            ax1.set_title('VIF –æ—Ü—ñ–Ω–∫–∏ –¥–ª—è –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫', fontsize=14, fontweight='bold')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # –ì—Ä–∞—Ñ–∏–∫ —Ç—ñ–ª—å–∫–∏ –≤–∏—Å–æ–∫–∏—Ö VIF (—è–∫—â–æ —î)
            if len(self.high_vif_features) > 0:
                high_vif_data = self.vif_scores[self.vif_scores['vif'] > self.vif_threshold]
                
                ax2.barh(range(len(high_vif_data)), high_vif_data['vif'], color='red', alpha=0.7)
                ax2.set_yticks(range(len(high_vif_data)))
                ax2.set_yticklabels(high_vif_data['feature'], fontsize=10)
                ax2.axvline(x=self.vif_threshold, color='red', linestyle='--', linewidth=2,
                           label=f'–ü–æ—Ä—ñ–≥ VIF = {self.vif_threshold}')
                ax2.set_xlabel('VIF Score')
                ax2.set_title(f'–û–∑–Ω–∞–∫–∏ –∑ –≤–∏—Å–æ–∫–∏–º VIF (> {self.vif_threshold})', fontsize=14, fontweight='bold')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
            else:
                ax2.text(0.5, 0.5, '–ù–µ–º–∞—î –æ–∑–Ω–∞–∫ –∑ –≤–∏—Å–æ–∫–∏–º VIF', 
                        ha='center', va='center', transform=ax2.transAxes, fontsize=12)
                ax2.set_title('–û–∑–Ω–∞–∫–∏ –∑ –≤–∏—Å–æ–∫–∏–º VIF', fontsize=14, fontweight='bold')
            
            plt.tight_layout()
            plt.savefig(f'{self.results_dir}/vif_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {self.results_dir}/vif_analysis.png")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó VIF: {e}")

    def evaluate_model(self, X_train, X_test, y_train, y_test, method_name="baseline"):
        """
        –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –∑ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫–æ–º —É—Å—ñ—Ö –º–µ—Ç—Ä–∏–∫
        """
        # Cross-validation –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        model = XGBClassifier(**self.optimal_params)
        
        # CV –º–µ—Ç—Ä–∏–∫–∏
        cv_auc_roc_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')
        cv_auc_pr_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='average_precision')
        cv_f1_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç –Ω–∞ –≤—ñ–¥–∫–ª–∞–¥–µ–Ω—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # –¢–µ—Å—Ç–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
        test_auc_roc = roc_auc_score(y_test, y_pred_proba)
        test_auc_pr = average_precision_score(y_test, y_pred_proba)
        test_f1 = f1_score(y_test, y_pred)
        test_accuracy = accuracy_score(y_test, y_pred)
        test_precision = precision_score(y_test, y_pred)
        test_recall = recall_score(y_test, y_pred)
        
        return {
            'method': method_name,
            'n_features': X_train.shape[1],
            'cv_auc_roc_mean': cv_auc_roc_scores.mean(),
            'cv_auc_roc_std': cv_auc_roc_scores.std(),
            'cv_auc_pr_mean': cv_auc_pr_scores.mean(),
            'cv_auc_pr_std': cv_auc_pr_scores.std(),
            'cv_f1_mean': cv_f1_scores.mean(),
            'cv_f1_std': cv_f1_scores.std(),
            'test_auc_roc': test_auc_roc,
            'test_auc_pr': test_auc_pr,
            'test_f1': test_f1,
            'test_accuracy': test_accuracy,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'overfitting_gap_auc_roc': cv_auc_roc_scores.mean() - test_auc_roc,
            'overfitting_gap_auc_pr': cv_auc_pr_scores.mean() - test_auc_pr,
            'overfitting_gap_f1': cv_f1_scores.mean() - test_f1
        } 

    def apply_pca_method(self, n_components=None):
        """
        –ú–ï–¢–û–î 1: PCA (Principal Component Analysis)
        """
        print(f"\nüîÑ === –ú–ï–¢–û–î 1: PCA (PRINCIPAL COMPONENT ANALYSIS) ===")
        
        if n_components is None:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è 95% –¥–∏—Å–ø–µ—Ä—Å—ñ—ó
            n_components = 0.95
        
        print(f"üéØ –¶—ñ–ª—å–æ–≤–∞ –¥–∏—Å–ø–µ—Ä—Å—ñ—è: {n_components}")
        
        # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è PCA
        pca = PCA(n_components=n_components, random_state=self.random_state)
        X_train_pca = pca.fit_transform(self.X_train_scaled)
        X_test_pca = pca.transform(self.X_test_scaled)
        
        print(f"üìä –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤: {pca.n_components_}")
        print(f"üìä –ü–æ—è—Å–Ω–µ–Ω–∞ –¥–∏—Å–ø–µ—Ä—Å—ñ—è: {pca.explained_variance_ratio_.sum():.4f}")
        
        # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
        results = self.evaluate_model(X_train_pca, X_test_pca, self.y_train, self.y_test, "PCA")
        self.method_results['PCA'] = results
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ PCA
        results['explained_variance_ratio'] = pca.explained_variance_ratio_.sum()
        results['n_components'] = pca.n_components_
        results['original_features'] = len(self.feature_names)
        
        print(f"‚úÖ PCA –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(self.feature_names)} ‚Üí {pca.n_components_} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤")
        
        return X_train_pca, X_test_pca, pca

    def apply_ica_method(self, n_components=None):
        """
        –ú–ï–¢–û–î 2: ICA (Independent Component Analysis)
        """
        print(f"\nüîÑ === –ú–ï–¢–û–î 2: ICA (INDEPENDENT COMPONENT ANALYSIS) ===")
        
        if n_components is None:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–∞–∫—É –∂ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ —è–∫ —É PCA
            if 'PCA' in self.method_results:
                n_components = self.method_results['PCA']['n_components']
            else:
                n_components = min(10, len(self.feature_names))
        
        print(f"üéØ –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤: {n_components}")
        
        # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è ICA
        ica = FastICA(n_components=n_components, random_state=self.random_state, max_iter=1000)
        X_train_ica = ica.fit_transform(self.X_train_scaled)
        X_test_ica = ica.transform(self.X_test_scaled)
        
        # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
        results = self.evaluate_model(X_train_ica, X_test_ica, self.y_train, self.y_test, "ICA")
        self.method_results['ICA'] = results
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ ICA
        results['n_components'] = n_components
        results['original_features'] = len(self.feature_names)
        
        print(f"‚úÖ ICA –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(self.feature_names)} ‚Üí {n_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤")
        
        return X_train_ica, X_test_ica, ica

    def apply_ridge_method(self, alpha=1.0):
        """
        –ú–ï–¢–û–î 3: Ridge —Ä–µ–≥—Ä–µ—Å—ñ—è (L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è)
        """
        print(f"\nüîÑ === –ú–ï–¢–û–î 3: RIDGE –†–ï–ì–†–ï–°–Ü–Ø (L2 –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–Ü–Ø) ===")
        
        print(f"üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó Œ±: {alpha}")
        
        # –ú–æ–¥–∏—Ñ—ñ–∫—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ XGBoost –¥–ª—è L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
        ridge_params = self.optimal_params.copy()
        ridge_params['reg_lambda'] = alpha
        
        # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –∑ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        model = XGBClassifier(**ridge_params)
        
        # CV –º–µ—Ç—Ä–∏–∫–∏
        cv_auc_roc_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=cv, scoring='roc_auc')
        cv_auc_pr_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=cv, scoring='average_precision')
        cv_f1_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=cv, scoring='f1')
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç
        model.fit(self.X_train_scaled, self.y_train)
        y_pred = model.predict(self.X_test_scaled)
        y_pred_proba = model.predict_proba(self.X_test_scaled)[:, 1]
        
        # –¢–µ—Å—Ç–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
        test_auc_roc = roc_auc_score(self.y_test, y_pred_proba)
        test_auc_pr = average_precision_score(self.y_test, y_pred_proba)
        test_f1 = f1_score(self.y_test, y_pred)
        test_accuracy = accuracy_score(self.y_test, y_pred)
        test_precision = precision_score(self.y_test, y_pred)
        test_recall = recall_score(self.y_test, y_pred)
        
        results = {
            'method': 'Ridge',
            'n_features': len(self.feature_names),
            'cv_auc_roc_mean': cv_auc_roc_scores.mean(),
            'cv_auc_roc_std': cv_auc_roc_scores.std(),
            'cv_auc_pr_mean': cv_auc_pr_scores.mean(),
            'cv_auc_pr_std': cv_auc_pr_scores.std(),
            'cv_f1_mean': cv_f1_scores.mean(),
            'cv_f1_std': cv_f1_scores.std(),
            'test_auc_roc': test_auc_roc,
            'test_auc_pr': test_auc_pr,
            'test_f1': test_f1,
            'test_accuracy': test_accuracy,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'overfitting_gap_auc_roc': cv_auc_roc_scores.mean() - test_auc_roc,
            'overfitting_gap_auc_pr': cv_auc_pr_scores.mean() - test_auc_pr,
            'overfitting_gap_f1': cv_f1_scores.mean() - test_f1,
            'regularization_alpha': alpha
        }
        
        self.method_results['Ridge'] = results
        
        print(f"‚úÖ Ridge –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑ Œ± = {alpha}")
        
        return model

    def apply_lasso_method(self, alpha=1.0):
        """
        –ú–ï–¢–û–î 4: Lasso —Ä–µ–≥—Ä–µ—Å—ñ—è (L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è)
        """
        print(f"\nüîÑ === –ú–ï–¢–û–î 4: LASSO –†–ï–ì–†–ï–°–Ü–Ø (L1 –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–Ü–Ø) ===")
        
        print(f"üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó Œ±: {alpha}")
        
        # –ú–æ–¥–∏—Ñ—ñ–∫—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ XGBoost –¥–ª—è L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
        lasso_params = self.optimal_params.copy()
        lasso_params['reg_alpha'] = alpha
        
        # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –∑ L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        model = XGBClassifier(**lasso_params)
        
        # CV –º–µ—Ç—Ä–∏–∫–∏
        cv_auc_roc_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=cv, scoring='roc_auc')
        cv_auc_pr_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=cv, scoring='average_precision')
        cv_f1_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=cv, scoring='f1')
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç
        model.fit(self.X_train_scaled, self.y_train)
        y_pred = model.predict(self.X_test_scaled)
        y_pred_proba = model.predict_proba(self.X_test_scaled)[:, 1]
        
        # –¢–µ—Å—Ç–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
        test_auc_roc = roc_auc_score(self.y_test, y_pred_proba)
        test_auc_pr = average_precision_score(self.y_test, y_pred_proba)
        test_f1 = f1_score(self.y_test, y_pred)
        test_accuracy = accuracy_score(self.y_test, y_pred)
        test_precision = precision_score(self.y_test, y_pred)
        test_recall = recall_score(self.y_test, y_pred)
        
        results = {
            'method': 'Lasso',
            'n_features': len(self.feature_names),
            'cv_auc_roc_mean': cv_auc_roc_scores.mean(),
            'cv_auc_roc_std': cv_auc_roc_scores.std(),
            'cv_auc_pr_mean': cv_auc_pr_scores.mean(),
            'cv_auc_pr_std': cv_auc_pr_scores.std(),
            'cv_f1_mean': cv_f1_scores.mean(),
            'cv_f1_std': cv_f1_scores.std(),
            'test_auc_roc': test_auc_roc,
            'test_auc_pr': test_auc_pr,
            'test_f1': test_f1,
            'test_accuracy': test_accuracy,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'overfitting_gap_auc_roc': cv_auc_roc_scores.mean() - test_auc_roc,
            'overfitting_gap_auc_pr': cv_auc_pr_scores.mean() - test_auc_pr,
            'overfitting_gap_f1': cv_f1_scores.mean() - test_f1,
            'regularization_alpha': alpha
        }
        
        self.method_results['Lasso'] = results
        
        print(f"‚úÖ Lasso –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑ Œ± = {alpha}")
        
        return model

    def apply_elastic_net_method(self, alpha=1.0, l1_ratio=0.5):
        """
        –ú–ï–¢–û–î 5: Elastic Net (–∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è L1 + L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó)
        """
        print(f"\nüîÑ === –ú–ï–¢–û–î 5: ELASTIC NET (L1 + L2 –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–Ü–Ø) ===")
        
        print(f"üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó Œ±: {alpha}")
        print(f"üéØ L1 ratio: {l1_ratio}")
        
        # –ú–æ–¥–∏—Ñ—ñ–∫—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ XGBoost –¥–ª—è Elastic Net
        elastic_params = self.optimal_params.copy()
        elastic_params['reg_alpha'] = alpha * l1_ratio  # L1 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        elastic_params['reg_lambda'] = alpha * (1 - l1_ratio)  # L2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        
        # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –∑ Elastic Net —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        model = XGBClassifier(**elastic_params)
        
        # CV –º–µ—Ç—Ä–∏–∫–∏
        cv_auc_roc_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=cv, scoring='roc_auc')
        cv_auc_pr_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=cv, scoring='average_precision')
        cv_f1_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=cv, scoring='f1')
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç
        model.fit(self.X_train_scaled, self.y_train)
        y_pred = model.predict(self.X_test_scaled)
        y_pred_proba = model.predict_proba(self.X_test_scaled)[:, 1]
        
        # –¢–µ—Å—Ç–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
        test_auc_roc = roc_auc_score(self.y_test, y_pred_proba)
        test_auc_pr = average_precision_score(self.y_test, y_pred_proba)
        test_f1 = f1_score(self.y_test, y_pred)
        test_accuracy = accuracy_score(self.y_test, y_pred)
        test_precision = precision_score(self.y_test, y_pred)
        test_recall = recall_score(self.y_test, y_pred)
        
        results = {
            'method': 'Elastic Net',
            'n_features': len(self.feature_names),
            'cv_auc_roc_mean': cv_auc_roc_scores.mean(),
            'cv_auc_roc_std': cv_auc_roc_scores.std(),
            'cv_auc_pr_mean': cv_auc_pr_scores.mean(),
            'cv_auc_pr_std': cv_auc_pr_scores.std(),
            'cv_f1_mean': cv_f1_scores.mean(),
            'cv_f1_std': cv_f1_scores.std(),
            'test_auc_roc': test_auc_roc,
            'test_auc_pr': test_auc_pr,
            'test_f1': test_f1,
            'test_accuracy': test_accuracy,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'overfitting_gap_auc_roc': cv_auc_roc_scores.mean() - test_auc_roc,
            'overfitting_gap_auc_pr': cv_auc_pr_scores.mean() - test_auc_pr,
            'overfitting_gap_f1': cv_f1_scores.mean() - test_f1,
            'regularization_alpha': alpha,
            'l1_ratio': l1_ratio
        }
        
        self.method_results['Elastic Net'] = results
        
        print(f"‚úÖ Elastic Net –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑ Œ± = {alpha}, l1_ratio = {l1_ratio}")
        
        return model

    def apply_feature_selection_vif(self):
        """
        –ú–ï–¢–û–î 6: –í–∏–¥–∞–ª–µ–Ω–Ω—è –æ–∑–Ω–∞–∫ –∑ –≤–∏—Å–æ–∫–∏–º VIF
        """
        print(f"\nüîÑ === –ú–ï–¢–û–î 6: –í–ò–î–ê–õ–ï–ù–ù–Ø –û–ó–ù–ê–ö –ó –í–ò–°–û–ö–ò–ú VIF ===")
        
        if self.vif_scores is None:
            print("‚ö†Ô∏è –°–ø–æ—á–∞—Ç–∫—É –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–∫–æ–Ω–∞—Ç–∏ VIF –∞–Ω–∞–ª—ñ–∑!")
            return None, None
        
        # –û–∑–Ω–∞–∫–∏ –∑ –Ω–æ—Ä–º–∞–ª—å–Ω–∏–º VIF
        low_vif_features = self.vif_scores[
            self.vif_scores['vif'] <= self.vif_threshold
        ]['feature'].tolist()
        
        print(f"üéØ –í–∏–¥–∞–ª—è—î–º–æ {len(self.high_vif_features)} –æ–∑–Ω–∞–∫ –∑ –≤–∏—Å–æ–∫–∏–º VIF")
        print(f"üéØ –ó–∞–ª–∏—à–∞—î–º–æ {len(low_vif_features)} –æ–∑–Ω–∞–∫ –∑ –Ω–æ—Ä–º–∞–ª—å–Ω–∏–º VIF")
        
        # –Ü–Ω–¥–µ–∫—Å–∏ –æ–∑–Ω–∞–∫ –∑ –Ω–∏–∑—å–∫–∏–º VIF
        low_vif_indices = [self.feature_names.index(feature) for feature in low_vif_features]
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –±–µ–∑ –æ–∑–Ω–∞–∫ –∑ –≤–∏—Å–æ–∫–∏–º VIF
        X_train_vif = self.X_train_scaled[:, low_vif_indices]
        X_test_vif = self.X_test_scaled[:, low_vif_indices]
        
        # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
        results = self.evaluate_model(X_train_vif, X_test_vif, self.y_train, self.y_test, "VIF Selection")
        self.method_results['VIF Selection'] = results
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
        results['removed_features'] = len(self.high_vif_features)
        results['remaining_features'] = len(low_vif_features)
        results['original_features'] = len(self.feature_names)
        
        print(f"‚úÖ VIF Selection –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(self.feature_names)} ‚Üí {len(low_vif_features)} –æ–∑–Ω–∞–∫")
        
        return X_train_vif, X_test_vif

    def apply_correlation_removal(self):
        """
        –ú–ï–¢–û–î 7: –í–∏–¥–∞–ª–µ–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –∑ –ø–∞—Ä–∏ —Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—é—é—á–∏—Ö –æ–∑–Ω–∞–∫
        """
        print(f"\nüîÑ === –ú–ï–¢–û–î 7: –í–ò–î–ê–õ–ï–ù–ù–Ø –ö–û–†–ï–õ–Æ–Æ–ß–ò–• –û–ó–ù–ê–ö ===")
        
        if not self.highly_correlated_pairs:
            print("‚úÖ –ù–µ–º–∞—î —Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—é—é—á–∏—Ö –ø–∞—Ä –æ–∑–Ω–∞–∫ –¥–ª—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è")
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
            results = self.evaluate_model(self.X_train_scaled, self.X_test_scaled, 
                                        self.y_train, self.y_test, "Correlation Removal")
            self.method_results['Correlation Removal'] = results
            return self.X_train_scaled, self.X_test_scaled
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ–∑–Ω–∞–∫–∏ –¥–ª—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è
        features_to_remove = set()
        
        # –î–ª—è –∫–æ–∂–Ω–æ—ó –ø–∞—Ä–∏ –≤–∏–¥–∞–ª—è—î–º–æ –æ–∑–Ω–∞–∫—É –∑ –º–µ–Ω—à–æ—é –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é
        # –°–ø–æ—á–∞—Ç–∫—É –æ—Ç—Ä–∏–º—É—î–º–æ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –∑ XGBoost
        importance_model = XGBClassifier(**self.optimal_params)
        importance_model.fit(self.X_train_scaled, self.y_train)
        feature_importance = importance_model.feature_importances_
        
        importance_dict = {feature: importance for feature, importance 
                          in zip(self.feature_names, feature_importance)}
        
        for pair in self.highly_correlated_pairs:
            feature1 = pair['feature1']
            feature2 = pair['feature2']
            
            # –í–∏–¥–∞–ª—è—î–º–æ –æ–∑–Ω–∞–∫—É –∑ –º–µ–Ω—à–æ—é –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é
            if importance_dict[feature1] > importance_dict[feature2]:
                features_to_remove.add(feature2)
                print(f"  ‚Ä¢ –í–∏–¥–∞–ª—è—î–º–æ {feature2} (–≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å: {importance_dict[feature2]:.4f})")
                print(f"    –ó–∞–ª–∏—à–∞—î–º–æ {feature1} (–≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å: {importance_dict[feature1]:.4f})")
            else:
                features_to_remove.add(feature1)
                print(f"  ‚Ä¢ –í–∏–¥–∞–ª—è—î–º–æ {feature1} (–≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å: {importance_dict[feature1]:.4f})")
                print(f"    –ó–∞–ª–∏—à–∞—î–º–æ {feature2} (–≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å: {importance_dict[feature2]:.4f})")
        
        # –û–∑–Ω–∞–∫–∏, —â–æ –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è
        remaining_features = [f for f in self.feature_names if f not in features_to_remove]
        remaining_indices = [self.feature_names.index(f) for f in remaining_features]
        
        print(f"üéØ –í–∏–¥–∞–ª—è—î–º–æ {len(features_to_remove)} –æ–∑–Ω–∞–∫")
        print(f"üéØ –ó–∞–ª–∏—à–∞—î–º–æ {len(remaining_features)} –æ–∑–Ω–∞–∫")
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        X_train_corr = self.X_train_scaled[:, remaining_indices]
        X_test_corr = self.X_test_scaled[:, remaining_indices]
        
        # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
        results = self.evaluate_model(X_train_corr, X_test_corr, self.y_train, self.y_test, "Correlation Removal")
        self.method_results['Correlation Removal'] = results
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
        results['removed_features'] = len(features_to_remove)
        results['remaining_features'] = len(remaining_features)
        results['original_features'] = len(self.feature_names)
        
        print(f"‚úÖ Correlation Removal –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(self.feature_names)} ‚Üí {len(remaining_features)} –æ–∑–Ω–∞–∫")
        
        return X_train_corr, X_test_corr

    def create_comparison_visualizations(self):
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö –º–µ—Ç–æ–¥—ñ–≤
        """
        print(f"\nüé® === –°–¢–í–û–†–ï–ù–ù–Ø –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–ô –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø ===")
        
        if not self.method_results:
            print("‚ö†Ô∏è –ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó")
            return
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î Baseline –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
            if 'Baseline' not in self.method_results:
                # –î–æ–¥–∞–≤–∞–Ω–Ω—è baseline (–æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ)
                baseline_results = self.evaluate_model(self.X_train_scaled, self.X_test_scaled,
                                                     self.y_train, self.y_test, "Baseline")
                self.method_results['Baseline'] = baseline_results

            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            results_df = pd.DataFrame(self.method_results).T

            # –û—Ç—Ä–∏–º—É—î–º–æ –º–µ—Ç–æ–¥–∏ –∑ —Ñ–∞–∫—Ç–∏—á–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            methods = list(results_df.index)

            print(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ –º–µ—Ç–æ–¥—ñ–≤ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó: {len(methods)}")
            print(f"üìã –ú–µ—Ç–æ–¥–∏: {methods}")

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –≤—Å—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ —î –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
            required_columns = ['cv_auc_roc_mean', 'cv_auc_roc_std', 'cv_auc_pr_mean', 'cv_auc_pr_std',
                              'cv_f1_mean', 'cv_f1_std', 'test_auc_roc', 'test_auc_pr', 'test_f1',
                              'n_features', 'overfitting_gap_auc_pr']

            missing_columns = [col for col in required_columns if col not in results_df.columns]
            if missing_columns:
                print(f"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
                return

            # 1. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ', fontsize=16, fontweight='bold')

            # AUC-ROC CV
            ax1 = axes[0, 0]
            print(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–∞—Ä—á–∞—Ä—Ç–∞ AUC-ROC –¥–ª—è {len(methods)} –º–µ—Ç–æ–¥—ñ–≤")
            bars1 = ax1.bar(range(len(methods)), results_df['cv_auc_roc_mean'],
                           yerr=results_df['cv_auc_roc_std'], capsize=5, alpha=0.7)
            ax1.set_title('AUC-ROC (Cross-Validation)', fontweight='bold')
            ax1.set_ylabel('AUC-ROC Score')
            ax1.set_xticks(range(len(methods)))
            ax1.set_xticklabels(methods, rotation=45)
            ax1.grid(True, alpha=0.3)

            # AUC-PR CV
            ax2 = axes[0, 1]
            bars2 = ax2.bar(range(len(methods)), results_df['cv_auc_pr_mean'],
                           yerr=results_df['cv_auc_pr_std'], capsize=5, alpha=0.7, color='orange')
            ax2.set_title('AUC-PR (Cross-Validation)', fontweight='bold')
            ax2.set_ylabel('AUC-PR Score')
            ax2.set_xticks(range(len(methods)))
            ax2.set_xticklabels(methods, rotation=45)
            ax2.grid(True, alpha=0.3)

            # F1 CV
            ax3 = axes[0, 2]
            bars3 = ax3.bar(range(len(methods)), results_df['cv_f1_mean'],
                           yerr=results_df['cv_f1_std'], capsize=5, alpha=0.7, color='green')
            ax3.set_title('F1-Score (Cross-Validation)', fontweight='bold')
            ax3.set_ylabel('F1 Score')
            ax3.set_xticks(range(len(methods)))
            ax3.set_xticklabels(methods, rotation=45)
            ax3.grid(True, alpha=0.3)

            # Test –º–µ—Ç—Ä–∏–∫–∏
            ax4 = axes[1, 0]
            test_metrics = ['test_auc_roc', 'test_auc_pr', 'test_f1']

            x = np.arange(len(methods))
            width = 0.25

            ax4.bar(x - width, results_df['test_auc_roc'].values, width, label='AUC-ROC', alpha=0.7)
            ax4.bar(x, results_df['test_auc_pr'].values, width, label='AUC-PR', alpha=0.7)
            ax4.bar(x + width, results_df['test_f1'].values, width, label='F1-Score', alpha=0.7)

            ax4.set_title('–¢–µ—Å—Ç–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏', fontweight='bold')
            ax4.set_ylabel('Score')
            ax4.set_xticks(x)
            ax4.set_xticklabels(methods, rotation=45)
            ax4.legend()
            ax4.grid(True, alpha=0.3)

            # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
            ax5 = axes[1, 1]
            bars5 = ax5.bar(range(len(methods)), results_df['n_features'], alpha=0.7, color='purple')
            ax5.set_title('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫', fontweight='bold')
            ax5.set_ylabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫')
            ax5.set_xticks(range(len(methods)))
            ax5.set_xticklabels(methods, rotation=45)
            ax5.grid(True, alpha=0.3)

            # –î–æ–¥–∞–≤–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –Ω–∞ —Å—Ç–æ–≤–ø—Ü—ñ
            for i, bar in enumerate(bars5):
                height = bar.get_height()
                ax5.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{int(height)}', ha='center', va='bottom')

            # Overfitting gap (AUC-PR)
            ax6 = axes[1, 2]
            gap_colors = ['red' if gap > 0.05 else 'green' for gap in results_df['overfitting_gap_auc_pr']]
            bars6 = ax6.bar(range(len(methods)), results_df['overfitting_gap_auc_pr'],
                           alpha=0.7, color=gap_colors)
            ax6.set_title('Overfitting Gap (AUC-PR)', fontweight='bold')
            ax6.set_ylabel('CV - Test AUC-PR')
            ax6.set_xticks(range(len(methods)))
            ax6.set_xticklabels(methods, rotation=45)
            ax6.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax6.axhline(y=0.05, color='red', linestyle='--', alpha=0.5, label='–ü–æ—Ä—ñ–≥ (0.05)')
            ax6.legend()
            ax6.grid(True, alpha=0.3)

            plt.tight_layout()
            plt.savefig(f'{self.results_dir}/method_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()

            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {self.results_dir}/method_comparison.png")

            # 2. ROC —Ç–∞ PR –∫—Ä–∏–≤—ñ –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö –º–µ—Ç–æ–¥—ñ–≤
            self._plot_best_methods_curves()

            # 3. –¢–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            self._save_results_table()

        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π: {e}")

    def _plot_best_methods_curves(self):
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è ROC —Ç–∞ PR –∫—Ä–∏–≤–∏—Ö –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö –º–µ—Ç–æ–¥—ñ–≤
        """
        print(f"üé® –°—Ç–≤–æ—Ä–µ–Ω–Ω—è ROC —Ç–∞ PR –∫—Ä–∏–≤–∏—Ö...")

        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            results_df = pd.DataFrame(self.method_results).T

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –º–µ—Ç–æ–¥—ñ–≤
            if len(results_df) == 0:
                print("‚ö†Ô∏è –ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫—Ä–∏–≤–∏—Ö")
                return

            # –í–∏–±–∏—Ä–∞—î–º–æ –¥–æ 3 –Ω–∞–π–∫—Ä–∞—â–∏—Ö –º–µ—Ç–æ–¥—ñ–≤ –∑–∞ AUC-PR (–∞–±–æ –º–µ–Ω—à–µ, —è–∫—â–æ –º–µ—Ç–æ–¥—ñ–≤ –º–µ–Ω—à–µ)
            n_methods = min(3, len(results_df))
            best_methods = results_df.nlargest(n_methods, 'test_auc_pr').index.tolist()

            print(f"üìä –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫—Ä–∏–≤–∏—Ö –¥–ª—è {len(best_methods)} –Ω–∞–π–∫—Ä–∞—â–∏—Ö –º–µ—Ç–æ–¥—ñ–≤: {best_methods}")

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']

            for i, method in enumerate(best_methods):
                # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ –Ω–µ –≤–∏—Ö–æ–¥–∏–º–æ –∑–∞ –º–µ–∂—ñ –∫–æ–ª—å–æ—Ä—ñ–≤
                color = colors[i % len(colors)]
                # –û—Ç—Ä–∏–º—É—î–º–æ –¥–∞–Ω—ñ –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –∫—Ä–∏–≤–∏—Ö
                if method == 'Baseline':
                    X_test_method = self.X_test_scaled
                elif method == 'PCA':
                    pca = PCA(n_components=0.95, random_state=self.random_state)
                    pca.fit(self.X_train_scaled)
                    X_test_method = pca.transform(self.X_test_scaled)
                elif method == 'VIF Selection':
                    low_vif_features = self.vif_scores[
                        self.vif_scores['vif'] <= self.vif_threshold
                    ]['feature'].tolist()
                    low_vif_indices = [self.feature_names.index(f) for f in low_vif_features]
                    X_test_method = self.X_test_scaled[:, low_vif_indices]
                else:
                    X_test_method = self.X_test_scaled  # –î–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ–π–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤

                # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è —Ü—å–æ–≥–æ –º–µ—Ç–æ–¥—É
                if method in ['Ridge', 'Lasso', 'Elastic Net']:
                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
                    model_params = self.optimal_params.copy()
                    if method == 'Ridge':
                        model_params['reg_lambda'] = 1.0
                    elif method == 'Lasso':
                        model_params['reg_alpha'] = 1.0
                    elif method == 'Elastic Net':
                        model_params['reg_alpha'] = 0.5
                        model_params['reg_lambda'] = 0.5
                    model = XGBClassifier(**model_params)
                    model.fit(self.X_train_scaled, self.y_train)
                    y_pred_proba = model.predict_proba(X_test_method)[:, 1]
                else:
                    model = XGBClassifier(**self.optimal_params)
                    if method == 'PCA':
                        model.fit(pca.transform(self.X_train_scaled), self.y_train)
                    elif method == 'VIF Selection':
                        model.fit(self.X_train_scaled[:, low_vif_indices], self.y_train)
                    else:
                        model.fit(self.X_train_scaled, self.y_train)
                    y_pred_proba = model.predict_proba(X_test_method)[:, 1]

                # ROC –∫—Ä–∏–≤–∞
                fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)
                roc_auc = roc_auc_score(self.y_test, y_pred_proba)
                ax1.plot(fpr, tpr, color=color, lw=2,
                        label=f'{method} (AUC = {roc_auc:.3f})')

                # PR –∫—Ä–∏–≤–∞
                precision, recall, _ = precision_recall_curve(self.y_test, y_pred_proba)
                pr_auc = average_precision_score(self.y_test, y_pred_proba)
                ax2.plot(recall, precision, color=color, lw=2,
                        label=f'{method} (AUC = {pr_auc:.3f})')
            
            # –û—Ñ–æ—Ä–º–ª–µ–Ω–Ω—è ROC –∫—Ä–∏–≤–æ—ó
            ax1.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)
            ax1.set_xlim([0.0, 1.0])
            ax1.set_ylim([0.0, 1.05])
            ax1.set_xlabel('False Positive Rate')
            ax1.set_ylabel('True Positive Rate')
            ax1.set_title('ROC –∫—Ä–∏–≤—ñ –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö –º–µ—Ç–æ–¥—ñ–≤', fontweight='bold')
            ax1.legend(loc="lower right")
            ax1.grid(True, alpha=0.3)
            
            # –û—Ñ–æ—Ä–º–ª–µ–Ω–Ω—è PR –∫—Ä–∏–≤–æ—ó
            baseline_precision = self.y_test.sum() / len(self.y_test)
            ax2.axhline(y=baseline_precision, color='k', linestyle='--', lw=1, alpha=0.5,
                       label=f'Baseline ({baseline_precision:.3f})')
            ax2.set_xlim([0.0, 1.0])
            ax2.set_ylim([0.0, 1.05])
            ax2.set_xlabel('Recall')
            ax2.set_ylabel('Precision')
            ax2.set_title('Precision-Recall –∫—Ä–∏–≤—ñ –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö –º–µ—Ç–æ–¥—ñ–≤', fontweight='bold')
            ax2.legend(loc="lower left")
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f'{self.results_dir}/best_methods_curves.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {self.results_dir}/best_methods_curves.png")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –∫—Ä–∏–≤–∏—Ö: {e}")

    def _save_results_table(self):
        """
        –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–æ—ó —Ç–∞–±–ª–∏—Ü—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        """
        print(f"üìä –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–æ—ó —Ç–∞–±–ª–∏—Ü—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤...")
        
        try:
            results_df = pd.DataFrame(self.method_results).T
            
            # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ AUC-PR
            results_df = results_df.sort_values('test_auc_pr', ascending=False)
            
            # –û–∫—Ä—É–≥–ª—é—î–º–æ —á–∏—Å–ª–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            numeric_columns = results_df.select_dtypes(include=[np.number]).columns
            results_df[numeric_columns] = results_df[numeric_columns].round(4)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ CSV
            results_df.to_csv(f'{self.results_dir}/detailed_results.csv')
            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {self.results_dir}/detailed_results.csv")
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ Excel –∑ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è–º
            with pd.ExcelWriter(f'{self.results_dir}/detailed_results.xlsx', engine='openpyxl') as writer:
                results_df.to_excel(writer, sheet_name='Results', index=True)
            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {self.results_dir}/detailed_results.xlsx")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —Ç–∞–±–ª–∏—Ü—ñ: {e}")

    def generate_recommendations(self):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É
        """
        print(f"\nüìù === –ì–ï–ù–ï–†–ê–¶–Ü–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–ô ===")
        
        if not self.method_results:
            print("‚ö†Ô∏è –ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
            return
        
        results_df = pd.DataFrame(self.method_results).T
        
        # –ù–∞–π–∫—Ä–∞—â–∏–π –º–µ—Ç–æ–¥ –∑–∞ AUC-PR
        best_method = results_df.loc[results_df['test_auc_pr'].idxmax()]
        
        # –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        print(f"üèÜ –ù–ê–ô–ö–†–ê–©–ò–ô –ú–ï–¢–û–î: {best_method.name}")
        print(f"   ‚Ä¢ Test AUC-PR: {best_method['test_auc_pr']:.4f}")
        print(f"   ‚Ä¢ Test AUC-ROC: {best_method['test_auc_roc']:.4f}")
        print(f"   ‚Ä¢ Test F1: {best_method['test_f1']:.4f}")
        print(f"   ‚Ä¢ –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {best_method['n_features']}")
        print(f"   ‚Ä¢ Overfitting gap: {best_method['overfitting_gap_auc_pr']:.4f}")
        
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–æ–±–ª–µ–º –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ
        if len(self.highly_correlated_pairs) > 0:
            recommendations.append(f"üîç –í–∏—è–≤–ª–µ–Ω–æ {len(self.highly_correlated_pairs)} –ø–∞—Ä —Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—é—é—á–∏—Ö –æ–∑–Ω–∞–∫")
        
        if len(self.high_vif_features) > 0:
            recommendations.append(f"‚ö†Ô∏è –ó–Ω–∞–π–¥–µ–Ω–æ {len(self.high_vif_features)} –æ–∑–Ω–∞–∫ –∑ –≤–∏—Å–æ–∫–∏–º VIF > {self.vif_threshold}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        baseline_auc_pr = results_df.loc['Baseline', 'test_auc_pr']
        improvement = best_method['test_auc_pr'] - baseline_auc_pr
        
        if improvement > 0.01:
            recommendations.append(f"‚úÖ –ú–µ—Ç–æ–¥ {best_method.name} –ø–æ–∫—Ä–∞—â—É—î AUC-PR –Ω–∞ {improvement:.4f}")
        elif improvement > -0.01:
            recommendations.append(f"‚öñÔ∏è –ú–µ—Ç–æ–¥ {best_method.name} –ø–æ–∫–∞–∑—É—î —Å—Ö–æ–∂—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ baseline")
        else:
            recommendations.append(f"‚ùå –ú–µ—Ç–æ–¥–∏ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ –ø–æ–≥—ñ—Ä—à—É—é—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏")
        
        # –°–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        if best_method.name == 'PCA':
            recommendations.append("üìä PCA —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –∫–æ–ª–∏ –≤–∞–∂–ª–∏–≤–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å, –∞–ª–µ –≤—Ç—Ä–∞—á–∞—î—Ç—å—Å—è —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å")
        elif best_method.name in ['Ridge', 'Lasso', 'Elastic Net']:
            recommendations.append("üéõÔ∏è –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫ –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –≤–ø–ª–∏–≤—É")
        elif best_method.name == 'VIF Selection':
            recommendations.append("üéØ –í–∏–¥–∞–ª–µ–Ω–Ω—è –æ–∑–Ω–∞–∫ –∑ –≤–∏—Å–æ–∫–∏–º VIF —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–æ—Å—Ç—ñ")
        
        print(f"\nüìã –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á:")
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π —É —Ñ–∞–π–ª
        with open(f'{self.results_dir}/recommendations.txt', 'w', encoding='utf-8') as f:
            f.write("–ê–ù–ê–õ–Ü–ó –ú–£–õ–¨–¢–ò–ö–û–õ–Ü–ù–ï–ê–†–ù–û–°–¢–Ü - –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"–ù–∞–π–∫—Ä–∞—â–∏–π –º–µ—Ç–æ–¥: {best_method.name}\n")
            f.write(f"Test AUC-PR: {best_method['test_auc_pr']:.4f}\n")
            f.write(f"–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤—ñ–¥–Ω–æ—Å–Ω–æ baseline: {improvement:.4f}\n\n")
            f.write("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:\n")
            for i, rec in enumerate(recommendations, 1):
                f.write(f"{i}. {rec}\n")
        
        print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {self.results_dir}/recommendations.txt")

    def run_analysis(self, data_path_param=None):
        """
        –û—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫—É –ø–æ–≤–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ
        """
        start_time = time.time()
        
        if data_path_param:
            self.data_path = data_path_param
        
        print(f"üöÄ === –ü–û–ß–ê–¢–û–ö –ê–ù–ê–õ–Ü–ó–£ –ú–£–õ–¨–¢–ò–ö–û–õ–Ü–ù–ï–ê–†–ù–û–°–¢–Ü ===")
        print(f"‚è∞ –ß–∞—Å –ø–æ—á–∞—Ç–∫—É: {time.strftime('%H:%M:%S')}")
        
        try:
            # –ï–¢–ê–ü 1: –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
            self.load_and_prepare_data()
            
            # –ï–¢–ê–ü 2: –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó
            self.analyze_correlation()
            
            # –ï–¢–ê–ü 3: VIF –∞–Ω–∞–ª—ñ–∑
            self.analyze_vif()
            
            # –ï–¢–ê–ü 4: –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤
            print(f"\nüîÑ === –ï–¢–ê–ü 4: –ó–ê–°–¢–û–°–£–í–ê–ù–ù–Ø –ú–ï–¢–û–î–Ü–í –í–ò–†–Ü–®–ï–ù–ù–Ø ===")
            
            # Baseline (–¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è)
            print(f"üìä –û—Ü—ñ–Ω–∫–∞ baseline –º–æ–¥–µ–ª—ñ...")
            baseline_results = self.evaluate_model(self.X_train_scaled, self.X_test_scaled, 
                                                  self.y_train, self.y_test, "Baseline")
            self.method_results['Baseline'] = baseline_results
            
            # –ú–µ—Ç–æ–¥–∏ –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ
            self.apply_pca_method()
            self.apply_ica_method()
            
            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ–π–Ω—ñ –º–µ—Ç–æ–¥–∏
            self.apply_ridge_method(alpha=1.0)
            self.apply_lasso_method(alpha=1.0)
            self.apply_elastic_net_method(alpha=1.0, l1_ratio=0.5)
            
            # Feature selection –º–µ—Ç–æ–¥–∏
            self.apply_feature_selection_vif()
            self.apply_correlation_removal()
            
            # –ï–¢–ê–ü 5: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
            print(f"\nüîÑ === –ï–¢–ê–ü 5: –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –¢–ê –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø ===")
            self.create_comparison_visualizations()
            
            # –ï–¢–ê–ü 6: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
            print(f"\nüîÑ === –ï–¢–ê–ü 6: –ì–ï–ù–ï–†–ê–¶–Ü–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–ô ===")
            self.generate_recommendations()
            
            execution_time = time.time() - start_time
            print(f"\n‚úÖ === –ê–ù–ê–õ–Ü–ó –ó–ê–í–ï–†–®–ï–ù–û ===")
            print(f"‚è±Ô∏è –ß–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {execution_time:.1f} —Å–µ–∫—É–Ω–¥")
            print(f"üìÅ –£—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {self.results_dir}/")
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –∞–Ω–∞–ª—ñ–∑—É: {e}")
            raise

# –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–ø—É—Å–∫—É –∞–Ω–∞–ª—ñ–∑—É
if __name__ == "__main__":
    # –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –∑ –¥–∞–Ω–∏–º–∏ - –∑–º—ñ–Ω—ñ—Ç—å –Ω–∞ –≤–∞—à
    DATA_PATH = "b2b.csv"  # –ó–∞–º—ñ–Ω—ñ—Ç—å –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —à–ª—è—Ö
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –∑–∞–ø—É—Å–∫ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞
    analyzer = MulticollinearityAnalyzer(
        data_path=DATA_PATH,
        random_state=42,
        correlation_threshold=0.8,  # –ü–æ—Ä—ñ–≥ –¥–ª—è —Å–∏–ª—å–Ω–æ—ó –∫–æ—Ä–µ–ª—è—Ü—ñ—ó
        vif_threshold=5.0  # –ü–æ—Ä—ñ–≥ VIF –¥–ª—è –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ
    )
    
    analyzer.run_analysis() 