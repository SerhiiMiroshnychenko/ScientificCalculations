{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Практичне заняття $№$ $4$\n",
    "на тему\n",
    "## «*Нелінійний парний та множинний регресійний аналіз даних. Оцінка метричних показників*»"
   ],
   "id": "584d16cbc5aee63d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Мета заняття:\n",
    "ознайомлення здобувачів вищої освіти з теоретичними та практичними аспектами нелінійного множинного регресійного аналізу для оцінки взаємозв'язків між залежною змінною та кількома незалежними змінними.\n",
    "\n",
    "### Завдання:\n",
    "\n",
    "1. За спостережуваними даними побудувати рівняння нелінійної множинної регресії методом найменших квадратів.\n",
    "2. Графічно представити результати регресійного аналізу та спостережувані дані.\n",
    "3. Виконати інтерпретацію коефіцієнтів моделі.\n",
    "4. Оцінити оптимальність вибраної моделі із застосуванням наступних критеріїв:\n",
    "    - критерій Дарбіна-Уотсона для перевірки наявності автокореляції залишків;\n",
    "    - кореляційне відношення $\\eta$ для оцінки сили нелінійного зв'язку між змінними.\n",
    "\n",
    "Дані для 4 варіанту завдання:\n",
    "\n",
    "Формула моделі: $$y(x) = a_0 + a_1 \\cdot x_1 + a_2 \\cdot x_2 + a_3 \\cdot x_1^2 + a_4 \\cdot x_2^2 + rnd(b)$$\n",
    "\n",
    "Значення коефіцієнтів для Варіанту 4:\n",
    "- $a_0 = 3.4$\n",
    "- $a_1 = 2.7$\n",
    "- $a_2 = -0.5$\n",
    "- $a_3 = 0.45$\n",
    "- $a_4 = 0.25$\n",
    "- $rnd(b) = 1.10$"
   ],
   "id": "6885bf90983b24ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Опис процесу побудови нелінійної регресійної моделі\n",
    "\n",
    "Регресійний аналіз є потужним статистичним інструментом, який дозволяє моделювати та аналізувати взаємозв'язки між змінними. На відміну від лінійної регресії, нелінійна регресія дає можливість описувати більш складні залежності, які не можуть бути адекватно представлені лінійними функціями. Це особливо важливо при роботі з реальними даними, де взаємозв'язки часто мають нелінійний характер.\n",
    "\n",
    "У даному дослідженні розглядається процес побудови нелінійної множинної регресійної моделі для варіанту 4, яка має вигляд:\n",
    "\n",
    "$$y(x) = a_0 + a_1 \\cdot x_1 + a_2 \\cdot x_2 + a_3 \\cdot x_1^2 + a_4 \\cdot x_2^2 + rnd(b)$$\n",
    "\n",
    "де $a_0, a_1, a_2, a_3, a_4$ - коефіцієнти моделі, а $rnd(b)$ - випадкова складова з нормальним розподілом $N(0, b)$.\n",
    "\n",
    "Процес побудови нелінійної регресійної моделі складається з кількох послідовних етапів, кожен з яких має важливе значення для отримання адекватних результатів.\n",
    "\n",
    "На першому етапі було згенеровано синтетичні дані на основі заданої моделі. Для цього використовувалися наступні параметри:\n",
    "\n",
    "- Кількість спостережень: 100\n",
    "- Діапазон значень незалежних змінних: $x_1, x_2 \\in [-5, 5]$\n",
    "- Коефіцієнти моделі: $a_0 = 3.4$, $a_1 = 2.7$, $a_2 = -0.5$, $a_3 = 0.45$, $a_4 = 0.25$\n",
    "- Параметр шуму: $b = 1.10$\n",
    "\n",
    "Незалежні змінні $x_1$ та $x_2$ були згенеровані з використанням рівномірного розподілу на інтервалі $[-5, 5]$. Для кожної пари значень $(x_1, x_2)$ було обчислено відповідне значення залежної змінної $y$ за формулою моделі з додаванням випадкової складової $rnd(b)$, що має нормальний розподіл з нульовим математичним сподіванням та стандартним відхиленням $b = 1.10$.\n",
    "\n",
    "\n",
    "Для застосування методу найменших квадратів необхідно сформувати матрицю ознак $X$. У випадку нелінійної множинної регресії з квадратичними членами, матриця ознак має наступний вигляд:\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "1 & x_{11} & x_{21} & x_{11}^2 & x_{21}^2 \\\\\n",
    "1 & x_{12} & x_{22} & x_{12}^2 & x_{22}^2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{1n} & x_{2n} & x_{1n}^2 & x_{2n}^2\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "де $n$ - кількість спостережень.\n",
    "\n",
    "\n",
    "Для знаходження коефіцієнтів регресійної моделі використовується метод найменших квадратів (МНК). Згідно з цим методом, вектор коефіцієнтів $\\beta$ обчислюється за формулою:\n",
    "\n",
    "$$\\beta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "де $X^T$ - транспонована матриця ознак, $(X^T X)^{-1}$ - обернена матриця до добутку $X^T X$, а $y$ - вектор значень залежної змінної.\n",
    "\n",
    "Цей метод мінімізує суму квадратів відхилень між фактичними значеннями залежної змінної та значеннями, передбаченими моделлю.\n",
    "\n",
    "\n",
    "Після отримання коефіцієнтів регресійної моделі необхідно оцінити її якість. Для цього використовуються різні метрики, зокрема:\n",
    "\n",
    "- Коефіцієнт детермінації $R^2$, який показує, яку частку дисперсії залежної змінної пояснює модель.\n",
    "- Критерій Дарбіна-Уотсона для перевірки наявності автокореляції залишків.\n",
    "- Кореляційне відношення $\\eta$ для оцінки сили нелінійного зв'язку між змінними.\n",
    "\n",
    "\n",
    "У даному дослідженні було використано три різні підходи до побудови нелінійної регресійної моделі:\n",
    "\n",
    "1. Ручний (математичний) метод з використанням формул методу найменших квадратів.\n",
    "\n",
    "2. Використання бібліотеки statsmodels.api.OLS, яка реалізує метод найменших квадратів для побудови лінійних та нелінійних моделей.\n",
    "\n",
    "3. Використання бібліотеки sklearn.linear_model.LinearRegression у поєднанні з sklearn.preprocessing.PolynomialFeatures для генерації поліноміальних ознак.\n",
    "\n",
    "Порівняння цих методів дозволяє перевірити правильність реалізації ручного методу та оцінити ефективність різних програмних інструментів для регресійного аналізу."
   ],
   "id": "752cf083512cd6d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Генерація даних\n",
    "\n",
    "Для дослідження було згенеровано синтетичні дані на основі заданої моделі:\n",
    "\n",
    "$y(x) = a_0 + a_1 \\cdot x_1 + a_2 \\cdot x_2 + a_3 \\cdot x_1^2 + a_4 \\cdot x_2^2 + rnd(b)$\n",
    "\n",
    "де $rnd(b)$ - випадкова складова з нормальним розподілом $N(0, b)$.\n",
    "\n",
    "Кількість спостережень: 100\n",
    "\n",
    "Діапазон значень незалежних змінних: $x_1, x_2 \\in [-5, 5]$\n",
    "\n",
    "### Перші 5 рядків згенерованих даних:\n",
    "\n",
    "```\n",
    "         x1        x2          y     y_true     noise\n",
    "0 -1.254599 -4.685708   7.804683   8.552711 -0.748027\n",
    "1  4.507143  1.364104  24.749361  24.493882  0.255479\n",
    "2  2.319939 -1.856440  14.197982  13.875603  0.322380\n",
    "3  0.986585  0.085707   5.674983   6.460769 -0.785787\n",
    "4 -3.439814  4.075665   3.604327   1.551975  2.052352\n",
    "```\n",
    "\n",
    "### Статистичний опис згенерованих даних:\n",
    "\n",
    "```\n",
    "               x1          x2           y      y_true       noise\n",
    "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
    "mean    -0.298193   -0.021683    8.833739    8.714902    0.118837\n",
    "std      2.974894    2.931113    8.508968    8.547458    1.106291\n",
    "min     -4.944779   -4.930479   -1.829814   -0.857240   -2.227657\n",
    "25%     -3.067992   -2.579955    2.153906    1.657012   -0.786048\n",
    "50%     -0.358575    0.056249    5.600412    5.725869    0.197799\n",
    "75%      2.302031    2.661836   14.793470   14.930790    0.749390\n",
    "max      4.868869    4.856505   29.684672   30.587422    4.238005\n",
    "```\n",
    "\n",
    "### Візуалізація згенерованих даних:\n",
    "\n",
    "![Візуалізація згенерованих даних](generated_data_visualization.png)\n",
    "\n",
    "*Рис. 1. Візуалізація згенерованих даних: 3D-візуалізація, залежності y від x1 та x2, розподіл шуму*\n"
   ],
   "id": "d1a2caa964f8e0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Ручний (математичний) метод розрахунку\n",
    "\n",
    "Для побудови рівняння нелінійної множинної регресії використаємо метод найменших квадратів (МНК). Для нашої моделі $y(x) = a_0 + a_1 \\cdot x_1 + a_2 \\cdot x_2 + a_3 \\cdot x_1^2 + a_4 \\cdot x_2^2$ необхідно знайти коефіцієнти $a_0, a_1, a_2, a_3, a_4$, які мінімізують суму квадратів відхилень.\n",
    "\n",
    "### Матриця ознак X\n",
    "\n",
    "Для застосування методу найменших квадратів створюємо матрицю ознак X, де кожен рядок має вигляд [1, x1_i, x2_i, x1_i^2, x2_i^2]:\n",
    "\n",
    "```\n",
    "     1        x1        x2       x1^2       x2^2\n",
    "0  1.0 -1.254599 -4.685708   1.574018  21.955861\n",
    "1  1.0  4.507143  1.364104  20.314339   1.860780\n",
    "2  1.0  2.319939 -1.856440   5.382119   3.446370\n",
    "3  1.0  0.986585  0.085707   0.973350   0.007346\n",
    "4  1.0 -3.439814  4.075665  11.832318  16.611043\n",
    "```\n",
    "\n",
    "### Матриця X^T * X\n",
    "\n",
    "Обчислюємо добуток транспонованої матриці X на матрицю X:\n",
    "\n",
    "```\n",
    "            0           1           2             3             4\n",
    "0  100.000000  -29.819257   -2.168277    885.041382    850.597657\n",
    "1  -29.819257  885.041382  -28.732169   -496.823937   -368.242178\n",
    "2   -2.168277  -28.732169  850.597657    -17.801945    101.711736\n",
    "3  885.041382 -496.823937  -17.801945  13335.480579   6971.329072\n",
    "4  850.597657 -368.242178  101.711736   6971.329072  12293.226921\n",
    "```\n",
    "\n",
    "### Обернена матриця (X^T * X)^(-1)\n",
    "\n",
    "Обчислюємо обернену матрицю до X^T * X:\n",
    "\n",
    "```\n",
    "          0         1         2         3         4\n",
    "0  0.042106 -0.000381  0.000283 -0.001817 -0.001897\n",
    "1 -0.000381  0.001161  0.000036  0.000052  0.000031\n",
    "2  0.000283  0.000036  0.001181 -0.000002 -0.000027\n",
    "3 -0.001817  0.000052 -0.000002  0.000186  0.000022\n",
    "4 -0.001897  0.000031 -0.000027  0.000022  0.000201\n",
    "```\n",
    "\n",
    "### Вектор X^T * y\n",
    "\n",
    "Обчислюємо добуток транспонованої матриці X на вектор y:\n",
    "\n",
    "```\n",
    "[ 883.37385695 1948.15856246 -424.68491209 9531.10473433 8293.77382724]\n",
    "```\n",
    "\n",
    "### Отримані коефіцієнти регресії (ручний метод)\n",
    "\n",
    "Обчислюємо вектор коефіцієнтів β = (X^T * X)^(-1) * X^T * y:\n",
    "\n",
    "- a0 (вільний член) = 3.2849\n",
    "\n",
    "- a1 (коефіцієнт при x1) = 2.6660\n",
    "\n",
    "- a2 (коефіцієнт при x2) = -0.4242\n",
    "\n",
    "- a3 (коефіцієнт при x1^2) = 0.4520\n",
    "\n",
    "- a4 (коефіцієнт при x2^2) = 0.2744\n",
    "\n",
    "### Порівняння з вихідними коефіцієнтами\n",
    "\n",
    "### Оцінка якості моделі (ручний метод)\n",
    "\n",
    "Коефіцієнт детермінації R^2 = 0.9844\n",
    "\n",
    "Критерій Дарбіна-Уотсона = 2.1020\n",
    "\n",
    "Інтерпретація: Автокореляція залишків відсутня або незначна\n",
    "\n",
    "Кореляційне відношення η = 0.9922\n",
    "\n",
    "Інтерпретація: Сильний нелінійний зв'язок\n",
    "\n",
    "### Візуалізація результатів ручного методу:\n",
    "\n",
    "![Візуалізація результатів ручного методу](manual_method_results.png)\n",
    "\n",
    "*Рис. 2. Візуалізація результатів ручного методу: фактичні vs. прогнозні значення, розподіл залишків, QQ-Plot залишків, залишки vs. прогнозні значення*\n"
   ],
   "id": "5fffd5649acd247d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Метод statsmodels.api.OLS\n",
    "\n",
    "Для побудови нелінійної множинної регресії використаємо бібліотеку statsmodels, яка надає зручний інтерфейс для статистичного аналізу даних. Метод OLS (Ordinary Least Squares) реалізує метод найменших квадратів.\n",
    "\n",
    "### Матриця ознак X для statsmodels\n",
    "\n",
    "Для застосування методу OLS створюємо матрицю ознак X у вигляді DataFrame:\n",
    "\n",
    "```\n",
    "   const        x1        x2      x1_sq      x2_sq\n",
    "0      1 -1.254599 -4.685708   1.574018  21.955861\n",
    "1      1  4.507143  1.364104  20.314339   1.860780\n",
    "2      1  2.319939 -1.856440   5.382119   3.446370\n",
    "3      1  0.986585  0.085707   0.973350   0.007346\n",
    "4      1 -3.439814  4.075665  11.832318  16.611043\n",
    "```\n",
    "\n",
    "### Результати регресії (statsmodels.api.OLS)\n",
    "\n",
    "Результати регресійного аналізу за допомогою statsmodels.api.OLS:\n",
    "\n",
    "```\n",
    "\n",
    "R-squared:                       0.9844\n",
    "\n",
    "Adj. R-squared:                  0.9838\n",
    "\n",
    "F-statistic:                     1503.5061\n",
    "\n",
    "Prob (F-statistic):              6.1275e-85\n",
    "\n",
    "Log-Likelihood:                  -147.3211\n",
    "\n",
    "AIC:                             304.6423\n",
    "\n",
    "BIC:                             317.6681\n",
    "\n",
    "```\n",
    "\n",
    "### Отримані коефіцієнти регресії (statsmodels.api.OLS)\n",
    "\n",
    "- a0 (const) = 3.2849\n",
    "\n",
    "- a1 (x1) = 2.6660\n",
    "\n",
    "- a2 (x2) = -0.4242\n",
    "\n",
    "- a3 (x1_sq) = 0.4520\n",
    "\n",
    "- a4 (x2_sq) = 0.2744\n",
    "\n",
    "### Порівняння з вихідними коефіцієнтами\n",
    "\n",
    "| Коефіцієнт | Вихідне значення | Отримане значення | Різниця |\n",
    "|------------|------------------|-------------------|--------|\n",
    "| a0 | 3.4 | 3.2849 | 0.1151 |\n",
    "| a1 | 2.7 | 2.6660 | 0.0340 |\n",
    "| a2 | -0.5 | -0.4242 | 0.0758 |\n",
    "| a3 | 0.45 | 0.4520 | 0.0020 |\n",
    "| a4 | 0.25 | 0.2744 | 0.0244 |\n",
    "\n",
    "### Оцінка якості моделі (statsmodels.api.OLS)\n",
    "\n",
    "Коефіцієнт детермінації R^2 = 0.9844\n",
    "\n",
    "Критерій Дарбіна-Уотсона = 2.1020\n",
    "\n",
    "Інтерпретація: Автокореляція залишків відсутня або незначна\n",
    "\n",
    "Кореляційне відношення η = 0.9922\n",
    "\n",
    "Інтерпретація: Сильний нелінійний зв'язок\n",
    "\n",
    "### Візуалізація результатів statsmodels.api.OLS:\n",
    "\n",
    "![Візуалізація результатів statsmodels.api.OLS](statsmodels_method_results.png)\n",
    "\n",
    "*Рис. 3. Візуалізація результатів statsmodels.api.OLS: фактичні vs. прогнозні значення, розподіл залишків, QQ-Plot залишків, залишки vs. прогнозні значення*\n"
   ],
   "id": "bae8247fdbc44e1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Метод sklearn.linear_model.LinearRegression\n",
    "\n",
    "Для побудови нелінійної множинної регресії використаємо бібліотеку scikit-learn, яка надає зручний інтерфейс для машинного навчання. Для створення поліноміальних ознак використаємо PolynomialFeatures.\n",
    "\n",
    "### Матриця ознак X для sklearn\n",
    "\n",
    "Для застосування методу LinearRegression створюємо матрицю ознак X за допомогою PolynomialFeatures:\n",
    "\n",
    "```\n",
    "         x1        x2       x1^2      x1 x2       x2^2\n",
    "0 -1.254599 -4.685708   1.574018   5.878684  21.955861\n",
    "1  4.507143  1.364104  20.314339   6.148212   1.860780\n",
    "2  2.319939 -1.856440   5.382119  -4.306829   3.446370\n",
    "3  0.986585  0.085707   0.973350   0.084557   0.007346\n",
    "4 -3.439814  4.075665  11.832318 -14.019527  16.611043\n",
    "```\n",
    "\n",
    "### Отримані коефіцієнти регресії (sklearn.linear_model.LinearRegression)\n",
    "\n",
    "- a0 (intercept) = 3.2853\n",
    "\n",
    "- a1 (x1) = 2.6660\n",
    "\n",
    "- a2 (x2) = -0.4241\n",
    "\n",
    "- a3 (x1^2) = 0.4520\n",
    "\n",
    "- a4 (x2^2) = 0.2744\n",
    "\n",
    "### Порівняння з вихідними коефіцієнтами\n",
    "\n",
    "| Коефіцієнт | Вихідне значення | Отримане значення | Різниця |\n",
    "|------------|------------------|-------------------|--------|\n",
    "| a0 | 3.4 | 3.2853 | 0.1147 |\n",
    "| a1 | 2.7 | 2.6660 | 0.0340 |\n",
    "| a2 | -0.5 | -0.4241 | 0.0759 |\n",
    "| a3 | 0.45 | 0.4520 | 0.0020 |\n",
    "| a4 | 0.25 | 0.2744 | 0.0244 |\n",
    "\n",
    "### Оцінка якості моделі (sklearn.linear_model.LinearRegression)\n",
    "\n",
    "Коефіцієнт детермінації R^2 = 0.9844\n",
    "\n",
    "Критерій Дарбіна-Уотсона = 2.1019\n",
    "\n",
    "Інтерпретація: Автокореляція залишків відсутня або незначна\n",
    "\n",
    "Кореляційне відношення η = 0.9922\n",
    "\n",
    "Інтерпретація: Сильний нелінійний зв'язок\n",
    "\n",
    "### Візуалізація результатів sklearn.linear_model.LinearRegression:\n",
    "\n",
    "![Візуалізація результатів sklearn.linear_model.LinearRegression](sklearn_method_results.png)\n",
    "\n",
    "*Рис. 4. Візуалізація результатів sklearn.linear_model.LinearRegression: фактичні vs. прогнозні значення, розподіл залишків, QQ-Plot залишків, залишки vs. прогнозні значення*\n"
   ],
   "id": "dc9fceacd88e3a44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Отримання коефіцієнтів нелінійної регресійної моделі та їх інтерпретація\n",
    "\n",
    "У процесі дослідження нелінійної множинної регресійної моделі було отримано коефіцієнти за допомогою трьох різних методів: ручного розрахунку, використання бібліотеки statsmodels.api.OLS та sklearn.linear_model.LinearRegression. Усі три методи дали практично ідентичні результати, що підтверджує правильність їх реалізації.\n",
    "\n",
    "## Отримані коефіцієнти\n",
    "\n",
    "Вихідні значення коефіцієнтів для моделі $y(x) = a_0 + a_1 \\cdot x_1 + a_2 \\cdot x_2 + a_3 \\cdot x_1^2 + a_4 \\cdot x_2^2 + rnd(b)$ були задані як:\n",
    "\n",
    "- $a_0 = 3.4$ (вільний член)\n",
    "- $a_1 = 2.7$ (коефіцієнт при $x_1$)\n",
    "- $a_2 = -0.5$ (коефіцієнт при $x_2$)\n",
    "- $a_3 = 0.45$ (коефіцієнт при $x_1^2$)\n",
    "- $a_4 = 0.25$ (коефіцієнт при $x_2^2$)\n",
    "- $rnd(b) = 1.10$ (параметр шуму)\n",
    "\n",
    "У результаті застосування методу найменших квадратів було отримано наступні значення коефіцієнтів:\n",
    "\n",
    "| Коефіцієнт | Вихідне значення | Ручний метод | statsmodels.api.OLS | sklearn.linear_model.LinearRegression |\n",
    "|------------|------------------|--------------|-------------------|-----------------------------------|\n",
    "| $a_0$ | 3.4 | 3.2849 | 3.2849 | 3.2853 |\n",
    "| $a_1$ | 2.7 | 2.6660 | 2.6660 | 2.6660 |\n",
    "| $a_2$ | -0.5 | -0.4242 | -0.4242 | -0.4241 |\n",
    "| $a_3$ | 0.45 | 0.4520 | 0.4520 | 0.4520 |\n",
    "| $a_4$ | 0.25 | 0.2744 | 0.2744 | 0.2744 |\n",
    "\n",
    "Як видно з таблиці, отримані коефіцієнти дуже близькі до вихідних значень, що свідчить про високу точність методу найменших квадратів. Невеликі відхилення пояснюються наявністю випадкового шуму в даних.\n",
    "\n",
    "Інтерпретація отриманих коефіцієнтів регресійної моделі має важливе значення для розуміння характеру взаємозв'язків між змінними. Розглянемо детальну інтерпретацію кожного коефіцієнта:\n",
    "\n",
    "$a_0 \\approx 3.28$ - вільний член рівняння регресії. Він представляє очікуване значення залежної змінної $y$ при нульових значеннях незалежних змінних $x_1$ та $x_2$. Тобто, якщо $x_1 = 0$ і $x_2 = 0$, то $y \\approx 3.28$. Вільний член можна розглядати як базове значення залежної змінної за відсутності впливу факторів.\n",
    "\n",
    "$a_1 \\approx 2.67$ - коефіцієнт при змінній $x_1$. Він показує, як змінюється залежна змінна $y$ при зміні $x_1$ на одиницю, за умови, що інші змінні залишаються незмінними.\n",
    "\n",
    "Позитивне значення цього коефіцієнта вказує на те, що зі збільшенням $x_1$ значення $y$ також збільшується. Зокрема, при збільшенні $x_1$ на одиницю, $y$ збільшується приблизно на 2.67 одиниць.\n",
    "\n",
    "$a_2 \\approx -0.42$ - коефіцієнт при змінній $x_2$. Він показує, як змінюється залежна змінна $y$ при зміні $x_2$ на одиницю, за умови, що інші змінні залишаються незмінними. Від'ємне значення цього коефіцієнта вказує на те, що зі збільшенням $x_2$ значення $y$ зменшується. Зокрема, при збільшенні $x_2$ на одиницю, $y$ зменшується приблизно на 0.42 одиниці.\n",
    "\n",
    "$a_3 \\approx 0.45$ - коефіцієнт при квадратичному члені $x_1^2$. Він відображає нелінійний характер залежності між $x_1$ та $y$.\n",
    "\n",
    "Додатне значення цього коефіцієнта вказує на опуклу (параболічну вгору) залежність від $x_1$. Це означає, що вплив $x_1$ на $y$ посилюється зі збільшенням абсолютного значення $x_1$. Наприклад, при великих додатних або від'ємних значеннях $x_1$ внесок квадратичного члена стає більш значущим.\n",
    "\n",
    "$a_4 \\approx 0.27$ - коефіцієнт при квадратичному члені $x_2^2$. Аналогічно до $a_3$, він відображає нелінійний характер залежності між $x_2$ та $y$. Додатне значення цього коефіцієнта також вказує на опуклу (параболічну вгору) залежність від $x_2$.\n"
   ],
   "id": "6dd7fecb344e30ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Оцінка якості моделі за допомогою критерія Дарбіна-Уотсона та кореляційного відношення\n",
    "\n",
    "Для оцінки якості побудованої нелінійної регресійної моделі було використано кілька метрик, які дозволяють визначити адекватність моделі та силу зв'язку між змінними. Особливу увагу було приділено критерію Дарбіна-Уотсона для перевірки наявності автокореляції залишків та кореляційному відношенню для оцінки сили нелінійного зв'язку.\n",
    "\n",
    "## Коефіцієнт детермінації $R^2$\n",
    "\n",
    "Перш за все, було обчислено коефіцієнт детермінації $R^2$, який показує, яку частку дисперсії залежної змінної пояснює модель.\n",
    "\n",
    "Коефіцієнт детермінації обчислюється за формулою:\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "де $y_i$ - фактичні значення залежної змінної, $\\hat{y}_i$ - прогнозні значення, отримані за допомогою моделі, $\\bar{y}$ - середнє значення залежної змінної, $n$ - кількість спостережень.\n",
    "\n",
    "Для побудованої нелінійної регресійної моделі було отримано наступні значення коефіцієнта детермінації:\n",
    "\n",
    "| Метод | $R^2$ |\n",
    "|-------|-------|\n",
    "| Ручний метод | 0.9844 |\n",
    "| statsmodels.api.OLS | 0.9844 |\n",
    "| sklearn.linear_model.LinearRegression | 0.9844 |\n",
    "\n",
    "Значення $R^2 \\approx 0.9844$ свідчить про те, що побудована модель пояснює приблизно 98.44% дисперсії залежної змінної, що є дуже високим показником. Це вказує на відмінну якість моделі та її здатність точно прогнозувати значення залежної змінної на основі незалежних змінних.\n",
    "\n",
    "\n",
    "Критерій Дарбіна-Уотсона використовується для перевірки наявності автокореляції залишків моделі. Він обчислюється за формулою:\n",
    "\n",
    "$$DW = \\frac{\\sum_{i=2}^{n} (e_i - e_{i-1})^2}{\\sum_{i=1}^{n} e_i^2}$$\n",
    "\n",
    "де $e_i$ - залишки моделі, тобто різниця між фактичними та прогнозними значеннями: $e_i = y_i - \\hat{y}_i$.\n",
    "\n",
    "Значення критерію Дарбіна-Уотсона може варіюватися від 0 до 4:\n",
    "- Значення близько 2 вказує на відсутність автокореляції залишків.\n",
    "- Значення близько 0 вказує на позитивну автокореляцію.\n",
    "- Значення близько 4 вказує на негативну автокореляцію.\n",
    "\n",
    "Для побудованої нелінійної регресійної моделі було отримано наступні значення критерію Дарбіна-Уотсона:\n",
    "\n",
    "| Метод | Критерій Дарбіна-Уотсона |\n",
    "|-------|--------------------------|\n",
    "| Ручний метод | 2.1020 |\n",
    "| statsmodels.api.OLS | 2.1020 |\n",
    "| sklearn.linear_model.LinearRegression | 2.1019 |\n",
    "\n",
    "Отримані значення критерію Дарбіна-Уотсона близькі до 2, що свідчить про відсутність автокореляції залишків. Це є хорошим показником адекватності моделі, оскільки відсутність автокореляції залишків є однією з передумов коректного застосування методу найменших квадратів.\n",
    "\n",
    "Кореляційне відношення $\\eta$ використовується для оцінки сили нелінійного зв'язку між змінними. На відміну від коефіцієнта кореляції Пірсона, який вимірює лише лінійний зв'язок, кореляційне відношення здатне виявити будь-який функціональний зв'язок, включаючи нелінійний.\n",
    "\n",
    "Кореляційне відношення обчислюється за формулою:\n",
    "\n",
    "$$\\eta = \\sqrt{\\frac{\\sigma_{\\hat{y}}^2}{\\sigma_y^2}}$$\n",
    "\n",
    "де $\\sigma_{\\hat{y}}^2$ - дисперсія прогнозних значень, а $\\sigma_y^2$ - дисперсія фактичних значень залежної змінної.\n",
    "\n",
    "Значення кореляційного відношення знаходиться в діапазоні від 0 до 1:\n",
    "- Значення близько 0 вказує на відсутність зв'язку між змінними.\n",
    "- Значення близько 1 вказує на сильний функціональний зв'язок.\n",
    "\n",
    "Для побудованої нелінійної регресійної моделі було отримано наступні значення кореляційного відношення:\n",
    "\n",
    "| Метод | Кореляційне відношення $\\eta$ |\n",
    "|-------|-----------------------------|\n",
    "| Ручний метод | 0.9922 |\n",
    "| statsmodels.api.OLS | 0.9922 |\n",
    "| sklearn.linear_model.LinearRegression | 0.9922 |\n",
    "\n",
    "Отримані значення кореляційного відношення $\\eta \\approx 0.9922$ свідчать про дуже сильний нелінійний зв'язок між змінними. Це підтверджує адекватність вибору нелінійної регресійної моделі для опису взаємозв'язків між змінними.\n",
    "\n",
    "На основі проведеного аналізу можна зробити наступні висновки щодо якості побудованої нелінійної регресійної моделі:\n",
    "\n",
    "1. Високе значення коефіцієнта детермінації $R^2 \\approx 0.9844$ свідчить про те, що модель пояснює близько 98.44% дисперсії залежної змінної, що є дуже хорошим показником.\n",
    "\n",
    "2. Значення критерію Дарбіна-Уотсона близько 2.10 вказує на відсутність автокореляції залишків, що підтверджує коректність застосування методу найменших квадратів.\n",
    "\n",
    "3. Високе значення кореляційного відношення $\\eta \\approx 0.9922$ свідчить про сильний нелінійний зв'язок між змінними, що підтверджує адекватність вибору нелінійної моделі.\n"
   ],
   "id": "6e07c0bea0f97e39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Аналіз отриманих результатів\n",
    "\n",
    "У процесі дослідження нелінійної множинної регресійної моделі було отримано ряд важливих результатів, які потребують детального аналізу та інтерпретації. Розглянемо основні аспекти отриманих результатів та їх значення для розуміння досліджуваної моделі.\n",
    "\n",
    "## Порівняння методів регресійного аналізу\n",
    "\n",
    "У дослідженні було використано три різні методи для побудови нелінійної регресійної моделі: ручний (математичний) метод, метод statsmodels.api.OLS та метод sklearn.linear_model.LinearRegression.\n",
    "\n",
    "Порівняння результатів, отриманих за допомогою цих методів, дозволяє оцінити їх ефективність та точність. Як видно з таблиць порівняння коефіцієнтів та метрик якості моделі, усі три методи дали практично ідентичні результати, що підтверджує правильність їх реалізації та надійність отриманих результатів.\n",
    "\n",
    "Незначні відмінності у значеннях коефіцієнтів та метрик якості можуть бути пояснені особливостями реалізації алгоритмів та округленням числових значень. Проте ці відмінності настільки малі, що не впливають на загальні висновки щодо моделі.\n",
    "\n",
    "Одним із ключових аспектів аналізу результатів є оцінка точності відновлення коефіцієнтів моделі. Порівняння отриманих коефіцієнтів з вихідними значеннями дозволяє оцінити ефективність методу найменших квадратів для відновлення параметрів моделі за наявності випадкового шуму.\n",
    "\n",
    "Як видно з таблиці порівняння коефіцієнтів, отримані значення дуже близькі до вихідних. Найбільше відхилення спостерігається для вільного члена $a_0$ (різниця близько 0.115) та коефіцієнта при $x_2$ (різниця близько 0.076). Для інших коефіцієнтів відхилення менші.\n",
    "\n",
    "Такі невеликі відхилення є цілком очікуваними, враховуючи наявність випадкового шуму в даних з параметром $b = 1.10$.\n",
    "\n",
    "Аналіз метрик якості моделі дозволяє оцінити її адекватність та прогностичну здатність. Високе значення коефіцієнта детермінації $R^2 \\approx 0.9844$ свідчить про те, що побудована модель пояснює близько 98.44% дисперсії залежної змінної, що є дуже хорошим показником.\n",
    "\n",
    "Значення критерію Дарбіна-Уотсона близько 2.10 вказує на відсутність автокореляції залишків. Це важливий результат, оскільки відсутність автокореляції є однією з передумов коректного застосування методу найменших квадратів. Якщо б значення критерію суттєво відрізнялося від 2, це могло б свідчити про наявність систематичних помилок у моделі.\n",
    "\n",
    "Високе значення кореляційного відношення $\\eta \\approx 0.9922$ свідчить про сильний нелінійний зв'язок між змінними. Це підтверджує адекватність вибору нелінійної моделі для опису взаємозв'язків між змінними. Якщо б зв'язок був переважно лінійним, то включення квадратичних членів не дало б такого значного покращення якості моделі.\n",
    "\n",
    "\n",
    "Візуалізація результатів регресійного аналізу дозволяє наочно оцінити якість моделі та характер взаємозв'язків між змінними. Графіки фактичних та прогнозних значень показують високу точність прогнозування моделі, що підтверджується тісним розташуванням точок вздовж лінії ідеального прогнозу.\n",
    "\n",
    "Аналіз розподілу залишків показує, що вони мають приблизно нормальний розподіл з нульовим середнім значенням, що є важливою передумовою для коректного застосування методу найменших квадратів. QQ-Plot залишків також підтверджує нормальність їх розподілу, оскільки точки розташовуються близько до теоретичної лінії нормального розподілу.\n",
    "\n",
    "Графіки залишків відносно прогнозних значень не виявляють жодних систематичних патернів, що свідчить про відсутність гетероскедастичності (нерівномірності дисперсії залишків) та підтверджує адекватність моделі.\n",
    "\n",
    "3D-візуалізація регресійної поверхні дозволяє наочно оцінити характер залежності між змінними та порівняти різні методи побудови моделі.\n"
   ],
   "id": "77a7d70a34b0da0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Візуалізація порівняння методів:\n",
    "\n",
    "![Візуалізація порівняння методів](methods_comparison.png)\n",
    "\n",
    "*Рис. 5. Візуалізація порівняння методів: коефіцієнти, метрики якості, прогнозні значення, розподіл залишків*\n",
    "\n",
    "### 3D візуалізація порівняння методів:\n",
    "\n",
    "![3D візуалізація порівняння методів](3d_comparison.png)\n",
    "\n",
    "*Рис. 6. 3D візуалізація порівняння методів: істинна поверхня, ручний метод, statsmodels.api.OLS, sklearn.linear_model.LinearRegression*\n"
   ],
   "id": "36198efa0a7df6b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Висновки щодо адекватності розробленої моделі\n",
    "\n",
    "На основі проведеного нелінійного регресійного аналізу та оцінки метричних показників можна зробити ряд важливих висновків щодо адекватності розробленої моделі.\n",
    "\n",
    "Перш за все, слід відзначити високу точність відновлення коефіцієнтів моделі. Отримані значення коефіцієнтів дуже близькі до вихідних, що свідчить про ефективність методу найменших квадратів для оцінки параметрів нелінійної регресійної моделі. Незначні відхилення пояснюються наявністю випадкового шуму в даних і є цілком очікуваними.\n",
    "\n",
    "Високе значення коефіцієнта детермінації $R^2 \\approx 0.9844$ свідчить про те, що розроблена модель пояснює близько 98.44% дисперсії залежної змінної. Це є дуже високим показником, який вказує на відмінну якість моделі та її здатність точно прогнозувати значення залежної змінної на основі незалежних змінних.\n",
    "\n",
    "Значення критерію Дарбіна-Уотсона близько 2.10 вказує на відсутність автокореляції залишків, що є важливою передумовою для коректного застосування методу найменших квадратів. Відсутність автокореляції свідчить про те, що модель враховує всі систематичні залежності між змінними, і залишки є випадковими.\n",
    "\n",
    "Високе значення кореляційного відношення $\\eta \\approx 0.9922$ свідчить про сильний нелінійний зв'язок між змінними, що підтверджує адекватність вибору нелінійної моделі для опису взаємозв'язків між змінними. Це означає, що включення квадратичних членів у модель є обґрунтованим і суттєво покращує її якість.\n",
    "\n",
    "Аналіз розподілу залишків показує, що вони мають приблизно нормальний розподіл з нульовим середнім значенням, що є ще однією важливою передумовою для коректного застосування методу найменших квадратів. Відсутність систематичних патернів у графіках залишків відносно прогнозних значень свідчить про відсутність гетероскедастичності.\n",
    "\n",
    "Порівняння результатів, отриманих за допомогою трьох різних методів (ручного розрахунку, statsmodels.api.OLS та sklearn.linear_model.LinearRegression), показує їх практично повну ідентичність, що підтверджує правильність реалізації цих методів та надійність отриманих результатів.\n",
    "\n",
    "Візуалізація результатів регресійного аналізу дозволяє наочно оцінити якість моделі та характер взаємозв'язків між змінними. Графіки фактичних та прогнозних значень показують високу точність прогнозування моделі, що підтверджується тісним розташуванням точок вздовж лінії ідеального прогнозу.\n",
    "\n",
    "Таким чином, на основі всіх проведених оцінок можна зробити висновок, що розроблена нелінійна регресійна модель є адекватною і може бути використана для опису та прогнозування залежності між змінними.\n"
   ],
   "id": "245f37ad190353c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Можливі шляхи покращення розробленої нелінійної регресійної моделі\n",
    "\n",
    "Незважаючи на високу якість розробленої нелінійної регресійної моделі, існують певні можливості для її подальшого вдосконалення та покращення. Розглянемо основні напрямки, за якими можна розвивати та покращувати модель.\n",
    "\n",
    "## Включення взаємодій між змінними\n",
    "\n",
    "Одним із можливих шляхів покращення моделі є включення членів взаємодії між незалежними змінними. У поточній моделі враховуються лише лінійні та квадратичні ефекти кожної змінної окремо, але не враховуються їх взаємодії.\n",
    "\n",
    "Включення члена взаємодії $x_1 \\cdot x_2$ дозволить врахувати можливий ефект, коли вплив однієї змінної залежить від значення іншої змінної. Модель з урахуванням взаємодій може мати вигляд:\n",
    "\n",
    "$$y(x) = a_0 + a_1 \\cdot x_1 + a_2 \\cdot x_2 + a_3 \\cdot x_1^2 + a_4 \\cdot x_2^2 + a_5 \\cdot x_1 \\cdot x_2 + rnd(b)$$\n",
    "\n",
    "Включення такого члена може покращити якість моделі, особливо якщо між змінними існує суттєва взаємодія.\n",
    "\n",
    "Іншим можливим шляхом покращення моделі є включення поліномів вищих порядків. Поточна модель включає лише члени другого порядку ($x_1^2$ та $x_2^2$), але можна розглянути включення членів третього порядку ($x_1^3$, $x_2^3$) або навіть вищих порядків.\n",
    "\n",
    "Модель з поліномами третього порядку може мати вигляд:\n",
    "\n",
    "$$y(x) = a_0 + a_1 \\cdot x_1 + a_2 \\cdot x_2 + a_3 \\cdot x_1^2 + a_4 \\cdot x_2^2 + a_5 \\cdot x_1^3 + a_6 \\cdot x_2^3 + rnd(b)$$\n",
    "\n",
    "Включення таких членів може дозволити моделі краще описувати складні нелінійні залежності, особливо якщо вони мають характер кубічної або іншої поліноміальної функції вищого порядку.\n",
    "\n",
    "Важливим напрямком покращення моделі є застосування методів регуляризації, які дозволяють запобігти перенавчанню моделі, особливо при включенні великої кількості предикторів або поліномів високих порядків.\n",
    "\n",
    "Найпоширенішими методами регуляризації є:\n",
    "\n",
    "1. **Регуляризація L1 (Lasso)** - додає до функції втрат штраф, пропорційний сумі абсолютних значень коефіцієнтів. Це сприяє отриманню розріджених моделей, де багато коефіцієнтів дорівнюють нулю.\n",
    "\n",
    "2. **Регуляризація L2 (Ridge)** - додає до функції втрат штраф, пропорційний сумі квадратів коефіцієнтів. Це сприяє отриманню моделей з меншими за абсолютною величиною коефіцієнтами.\n",
    "\n",
    "3. **Еластична сітка (Elastic Net)** - комбінує регуляризацію L1 та L2, що дозволяє отримати переваги обох підходів.\n",
    "\n",
    "Застосування цих методів може допомогти уникнути перенавчання моделі та покращити її узагальнюючу здатність, особливо при роботі з реальними даними, де кількість спостережень може бути обмеженою.\n",
    "\n",
    "\n",
    "Ще одним важливим напрямком покращення моделі є аналіз впливових спостережень та викидів. Наявність аномальних значень у даних може суттєво впливати на оцінки коефіцієнтів моделі та погіршувати її якість."
   ],
   "id": "6faf1e89155fbcfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ДОДАТОК. Python скріпт комплексного розрахунку",
   "id": "3802f44762059d68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Комплексне рішення для практичного заняття\n",
    "\"Нелінійний парний та множинний регресійний аналіз даних. Оцінка метричних показників\"\n",
    "\n",
    "Це рішення порівнює різні підходи до виконання завдання:\n",
    "1. Ручний (математичний) розрахунок\n",
    "2. Використання statsmodels.api.OLS\n",
    "3. Використання sklearn.linear_model.LinearRegression\n",
    "4. Використання sklearn.preprocessing.PolynomialFeatures\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Бібліотеки для різних методів регресійного аналізу\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Налаштування для відтворюваності результатів\n",
    "np.random.seed(42)\n",
    "\n",
    "# Параметри для візуалізації\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Функція для збереження результатів у markdown-файл\n",
    "def append_to_md(filename, content):\n",
    "    with open(filename, 'a', encoding='utf-8') as f:\n",
    "        f.write(content + '\\n\\n')\n",
    "\n",
    "# Створення нового markdown-файлу для результатів\n",
    "md_filename = 'результати_аналізу.md'\n",
    "with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('# Результати нелінійного регресійного аналізу даних\\n\\n')\n",
    "    f.write('## Варіант 4\\n\\n')\n",
    "    f.write('### Формула моделі: $y(x) = a_0 + a_1 \\\\cdot x_1 + a_2 \\\\cdot x_2 + a_3 \\\\cdot x_1^2 + a_4 \\\\cdot x_2^2 + rnd(b)$\\n\\n')\n",
    "    f.write('### Значення коефіцієнтів:\\n')\n",
    "    f.write('- $a_0 = 3.4$\\n')\n",
    "    f.write('- $a_1 = 2.7$\\n')\n",
    "    f.write('- $a_2 = -0.5$\\n')\n",
    "    f.write('- $a_3 = 0.45$\\n')\n",
    "    f.write('- $a_4 = 0.25$\\n')\n",
    "    f.write('- $rnd(b) = 1.10$\\n\\n')\n",
    "\n",
    "print(\"Початок виконання комплексного аналізу...\")\n",
    "\n",
    "# ================= ЧАСТИНА 1: ГЕНЕРАЦІЯ ДАНИХ =================\n",
    "\n",
    "# Задані параметри моделі (Варіант 4)\n",
    "a0 = 3.4      # вільний член\n",
    "a1 = 2.7      # коефіцієнт при x1\n",
    "a2 = -0.5     # коефіцієнт при x2\n",
    "a3 = 0.45     # коефіцієнт при x1^2\n",
    "a4 = 0.25     # коефіцієнт при x2^2\n",
    "b = 1.10      # стандартне відхилення шуму\n",
    "\n",
    "# Кількість спостережень\n",
    "n_samples = 100\n",
    "\n",
    "# Генерація значень для незалежних змінних x1 і x2\n",
    "# Використовуємо рівномірний розподіл в діапазоні [-5, 5]\n",
    "x1 = np.random.uniform(-5, 5, n_samples)\n",
    "x2 = np.random.uniform(-5, 5, n_samples)\n",
    "\n",
    "# Обчислення істинних значень y без шуму\n",
    "y_true = a0 + a1*x1 + a2*x2 + a3*x1**2 + a4*x2**2\n",
    "\n",
    "# Додавання випадкового шуму з нормальним розподілом N(0, b)\n",
    "noise = np.random.normal(0, b, n_samples)\n",
    "y = y_true + noise\n",
    "\n",
    "# Створення DataFrame для зручності роботи з даними\n",
    "data = pd.DataFrame({\n",
    "    'x1': x1,\n",
    "    'x2': x2,\n",
    "    'y': y,\n",
    "    'y_true': y_true,\n",
    "    'noise': noise\n",
    "})\n",
    "\n",
    "# Виведення перших 5 рядків згенерованих даних\n",
    "print(\"\\nПерші 5 рядків згенерованих даних:\")\n",
    "print(data.head())\n",
    "\n",
    "# Збереження інформації про згенеровані дані у markdown-файл\n",
    "append_to_md(md_filename, '## 1. Генерація даних')\n",
    "append_to_md(md_filename, 'Для дослідження було згенеровано синтетичні дані на основі заданої моделі:')\n",
    "append_to_md(md_filename, '$y(x) = a_0 + a_1 \\\\cdot x_1 + a_2 \\\\cdot x_2 + a_3 \\\\cdot x_1^2 + a_4 \\\\cdot x_2^2 + rnd(b)$')\n",
    "append_to_md(md_filename, 'де $rnd(b)$ - випадкова складова з нормальним розподілом $N(0, b)$.')\n",
    "append_to_md(md_filename, f'Кількість спостережень: {n_samples}')\n",
    "append_to_md(md_filename, 'Діапазон значень незалежних змінних: $x_1, x_2 \\\\in [-5, 5]$')\n",
    "append_to_md(md_filename, '### Перші 5 рядків згенерованих даних:')\n",
    "append_to_md(md_filename, f'```\\n{data.head().to_string()}\\n```')\n",
    "\n",
    "# Статистичний опис згенерованих даних\n",
    "data_stats = data.describe()\n",
    "print(\"\\nСтатистичний опис згенерованих даних:\")\n",
    "print(data_stats)\n",
    "\n",
    "append_to_md(md_filename, '### Статистичний опис згенерованих даних:')\n",
    "append_to_md(md_filename, f'```\\n{data_stats.to_string()}\\n```')\n",
    "\n",
    "# Візуалізація згенерованих даних\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. 3D-візуалізація даних\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "scatter = ax1.scatter(x1, x2, y, c=y, cmap='viridis', alpha=0.7)\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax1.set_zlabel('y')\n",
    "ax1.set_title('3D візуалізація згенерованих даних')\n",
    "plt.colorbar(scatter, ax=ax1, shrink=0.5, aspect=5, label='Значення y')\n",
    "\n",
    "# 2. Залежність y від x1\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.scatter(x1, y, alpha=0.7)\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Залежність y від x1')\n",
    "\n",
    "# 3. Залежність y від x2\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.scatter(x2, y, alpha=0.7)\n",
    "ax3.set_xlabel('x2')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_title('Залежність y від x2')\n",
    "\n",
    "# 4. Розподіл шуму\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.hist(noise, bins=20, alpha=0.7)\n",
    "ax4.axvline(x=0, color='r', linestyle='--')\n",
    "ax4.set_xlabel('Шум')\n",
    "ax4.set_ylabel('Частота')\n",
    "ax4.set_title('Розподіл шуму')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('generated_data_visualization.png')\n",
    "\n",
    "append_to_md(md_filename, '### Візуалізація згенерованих даних:')\n",
    "append_to_md(md_filename, '![Візуалізація згенерованих даних](generated_data_visualization.png)')\n",
    "append_to_md(md_filename, '*Рис. 1. Візуалізація згенерованих даних: 3D-візуалізація, залежності y від x1 та x2, розподіл шуму*')\n",
    "\n",
    "print(\"\\nГенерація даних завершена. Дані візуалізовано та збережено у файл 'generated_data_visualization.png'\")\n",
    "\n",
    "# ================= ЧАСТИНА 2: РУЧНИЙ (МАТЕМАТИЧНИЙ) МЕТОД РОЗРАХУНКУ =================\n",
    "print(\"\\n\\nПочаток виконання ручного (математичного) методу розрахунку...\")\n",
    "append_to_md(md_filename, '## 2. Ручний (математичний) метод розрахунку')\n",
    "append_to_md(md_filename, 'Для побудови рівняння нелінійної множинної регресії використаємо метод найменших квадратів (МНК). Для нашої моделі $y(x) = a_0 + a_1 \\\\cdot x_1 + a_2 \\\\cdot x_2 + a_3 \\\\cdot x_1^2 + a_4 \\\\cdot x_2^2$ необхідно знайти коефіцієнти $a_0, a_1, a_2, a_3, a_4$, які мінімізують суму квадратів відхилень.')\n",
    "\n",
    "# Підготовка матриці ознак X для регресії\n",
    "# Кожен рядок матриці X має вигляд [1, x1_i, x2_i, x1_i^2, x2_i^2]\n",
    "X_manual = np.column_stack((np.ones(n_samples), x1, x2, x1**2, x2**2))\n",
    "\n",
    "# Виведення перших 5 рядків матриці ознак\n",
    "print(\"\\nПерші 5 рядків матриці ознак X:\")\n",
    "print(X_manual[:5])\n",
    "append_to_md(md_filename, '### Матриця ознак X')\n",
    "append_to_md(md_filename, 'Для застосування методу найменших квадратів створюємо матрицю ознак X, де кожен рядок має вигляд [1, x1_i, x2_i, x1_i^2, x2_i^2]:')\n",
    "append_to_md(md_filename, f'```\\n{pd.DataFrame(X_manual[:5], columns=[\"1\", \"x1\", \"x2\", \"x1^2\", \"x2^2\"]).to_string()}\\n```')\n",
    "\n",
    "# Рішення рівняння методом найменших квадратів: β = (X^T * X)^(-1) * X^T * y\n",
    "# 1. Обчислюємо X^T * X\n",
    "XTX = X_manual.T @ X_manual\n",
    "print(\"\\nМатриця X^T * X:\")\n",
    "print(XTX)\n",
    "append_to_md(md_filename, '### Матриця X^T * X')\n",
    "append_to_md(md_filename, 'Обчислюємо добуток транспонованої матриці X на матрицю X:')\n",
    "append_to_md(md_filename, f'```\\n{pd.DataFrame(XTX).to_string()}\\n```')\n",
    "\n",
    "# 2. Обчислюємо обернену матрицю (X^T * X)^(-1)\n",
    "XTX_inv = np.linalg.inv(XTX)\n",
    "print(\"\\nОбернена матриця (X^T * X)^(-1):\")\n",
    "print(XTX_inv)\n",
    "append_to_md(md_filename, '### Обернена матриця (X^T * X)^(-1)')\n",
    "append_to_md(md_filename, 'Обчислюємо обернену матрицю до X^T * X:')\n",
    "append_to_md(md_filename, f'```\\n{pd.DataFrame(XTX_inv).to_string()}\\n```')\n",
    "\n",
    "# 3. Обчислюємо X^T * y\n",
    "XTy = X_manual.T @ y\n",
    "print(\"\\nВектор X^T * y:\")\n",
    "print(XTy)\n",
    "append_to_md(md_filename, '### Вектор X^T * y')\n",
    "append_to_md(md_filename, 'Обчислюємо добуток транспонованої матриці X на вектор y:')\n",
    "append_to_md(md_filename, f'```\\n{XTy}\\n```')\n",
    "\n",
    "# 4. Обчислюємо β = (X^T * X)^(-1) * X^T * y\n",
    "beta_manual = XTX_inv @ XTy\n",
    "print(\"\\nОтримані коефіцієнти регресії (ручний метод):\")\n",
    "print(f\"a0 (вільний член) = {beta_manual[0]:.4f}\")\n",
    "print(f\"a1 (коефіцієнт при x1) = {beta_manual[1]:.4f}\")\n",
    "print(f\"a2 (коефіцієнт при x2) = {beta_manual[2]:.4f}\")\n",
    "print(f\"a3 (коефіцієнт при x1^2) = {beta_manual[3]:.4f}\")\n",
    "print(f\"a4 (коефіцієнт при x2^2) = {beta_manual[4]:.4f}\")\n",
    "\n",
    "append_to_md(md_filename, '### Отримані коефіцієнти регресії (ручний метод)')\n",
    "append_to_md(md_filename, f'Обчислюємо вектор коефіцієнтів β = (X^T * X)^(-1) * X^T * y:')\n",
    "append_to_md(md_filename, f'- a0 (вільний член) = {beta_manual[0]:.4f}')\n",
    "append_to_md(md_filename, f'- a1 (коефіцієнт при x1) = {beta_manual[1]:.4f}')\n",
    "append_to_md(md_filename, f'- a2 (коефіцієнт при x2) = {beta_manual[2]:.4f}')\n",
    "append_to_md(md_filename, f'- a3 (коефіцієнт при x1^2) = {beta_manual[3]:.4f}')\n",
    "append_to_md(md_filename, f'- a4 (коефіцієнт при x2^2) = {beta_manual[4]:.4f}')\n",
    "\n",
    "# Порівняння з вихідними коефіцієнтами\n",
    "print(\"\\nВихідні коефіцієнти:\")\n",
    "print(f\"a0 = {a0}\")\n",
    "print(f\"a1 = {a1}\")\n",
    "print(f\"a2 = {a2}\")\n",
    "print(f\"a3 = {a3}\")\n",
    "print(f\"a4 = {a4}\")\n",
    "\n",
    "append_to_md(md_filename, '### Порівняння з вихідними коефіцієнтами')\n",
    "table_content = '| Коефіцієнт | Вихідне значення | Отримане значення | Різниця |\\n'\n",
    "table_content += '|------------|------------------|-------------------|--------|\\n'\n",
    "table_content += f'| a0 | {a0} | {beta_manual[0]:.4f} | {abs(a0 - beta_manual[0]):.4f} |\\n'\n",
    "table_content += f'| a1 | {a1} | {beta_manual[1]:.4f} | {abs(a1 - beta_manual[1]):.4f} |\\n'\n",
    "table_content += f'| a2 | {a2} | {beta_manual[2]:.4f} | {abs(a2 - beta_manual[2]):.4f} |\\n'\n",
    "table_content += f'| a3 | {a3} | {beta_manual[3]:.4f} | {abs(a3 - beta_manual[3]):.4f} |\\n'\n",
    "table_content += f'| a4 | {a4} | {beta_manual[4]:.4f} | {abs(a4 - beta_manual[4]):.4f} |'\n",
    "\n",
    "# Обчислення прогнозних значень y\n",
    "y_pred_manual = X_manual @ beta_manual\n",
    "\n",
    "# Розрахунок залишків\n",
    "residuals_manual = y - y_pred_manual\n",
    "\n",
    "# Розрахунок коефіцієнта детермінації R^2\n",
    "SS_total = np.sum((y - np.mean(y))**2)\n",
    "SS_residual = np.sum(residuals_manual**2)\n",
    "R2_manual = 1 - (SS_residual / SS_total)\n",
    "print(f\"\\nКоефіцієнт детермінації R^2 (ручний метод): {R2_manual:.4f}\")\n",
    "\n",
    "append_to_md(md_filename, '### Оцінка якості моделі (ручний метод)')\n",
    "append_to_md(md_filename, f'Коефіцієнт детермінації R^2 = {R2_manual:.4f}')\n",
    "\n",
    "# Критерій Дарбіна-Уотсона для перевірки автокореляції залишків\n",
    "# Формула: DW = Σ(e_t - e_{t-1})^2 / Σe_t^2\n",
    "dw_numerator = np.sum(np.diff(residuals_manual)**2)\n",
    "dw_denominator = np.sum(residuals_manual**2)\n",
    "dw_manual = dw_numerator / dw_denominator\n",
    "\n",
    "print(f\"\\nКритерій Дарбіна-Уотсона (ручний метод): {dw_manual:.4f}\")\n",
    "if dw_manual < 1.5:\n",
    "    dw_interpretation = \"Є позитивна автокореляція залишків\"\n",
    "elif dw_manual > 2.5:\n",
    "    dw_interpretation = \"Є негативна автокореляція залишків\"\n",
    "else:\n",
    "    dw_interpretation = \"Автокореляція залишків відсутня або незначна\"\n",
    "print(dw_interpretation)\n",
    "\n",
    "append_to_md(md_filename, f'Критерій Дарбіна-Уотсона = {dw_manual:.4f}')\n",
    "append_to_md(md_filename, f'Інтерпретація: {dw_interpretation}')\n",
    "\n",
    "# Кореляційне відношення η для оцінки сили нелінійного зв'язку\n",
    "# η = sqrt(1 - Σ(y_i - y_pred_i)^2 / Σ(y_i - y_mean)^2)\n",
    "eta_manual = np.sqrt(R2_manual)\n",
    "\n",
    "print(f\"\\nКореляційне відношення η (ручний метод): {eta_manual:.4f}\")\n",
    "if eta_manual < 0.3:\n",
    "    eta_interpretation = \"Слабкий нелінійний зв'язок\"\n",
    "elif eta_manual < 0.7:\n",
    "    eta_interpretation = \"Помірний нелінійний зв'язок\"\n",
    "else:\n",
    "    eta_interpretation = \"Сильний нелінійний зв'язок\"\n",
    "print(eta_interpretation)\n",
    "\n",
    "append_to_md(md_filename, f'Кореляційне відношення η = {eta_manual:.4f}')\n",
    "append_to_md(md_filename, f'Інтерпретація: {eta_interpretation}')\n",
    "\n",
    "# Візуалізація результатів ручного методу\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Графік фактичних vs. прогнозних значень\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.scatter(y, y_pred_manual, alpha=0.7)\n",
    "ax1.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "ax1.set_xlabel('Фактичні значення y')\n",
    "ax1.set_ylabel('Прогнозні значення y')\n",
    "ax1.set_title('Фактичні vs. Прогнозні значення')\n",
    "\n",
    "# 2. Гістограма залишків\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.hist(residuals_manual, bins=20, alpha=0.7)\n",
    "ax2.axvline(x=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Залишки')\n",
    "ax2.set_ylabel('Частота')\n",
    "ax2.set_title('Розподіл залишків')\n",
    "\n",
    "# 3. QQ-Plot для перевірки нормальності залишків\n",
    "ax3 = fig.add_subplot(223)\n",
    "stats.probplot(residuals_manual, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('QQ-Plot залишків')\n",
    "\n",
    "# 4. Залишки vs. Прогнозні значення\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.scatter(y_pred_manual, residuals_manual, alpha=0.7)\n",
    "ax4.axhline(y=0, color='r', linestyle='--')\n",
    "ax4.set_xlabel('Прогнозні значення')\n",
    "ax4.set_ylabel('Залишки')\n",
    "ax4.set_title('Залишки vs. Прогнозні значення')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('manual_method_results.png')\n",
    "\n",
    "append_to_md(md_filename, '### Візуалізація результатів ручного методу:')\n",
    "append_to_md(md_filename, '![Візуалізація результатів ручного методу](manual_method_results.png)')\n",
    "append_to_md(md_filename, '*Рис. 2. Візуалізація результатів ручного методу: фактичні vs. прогнозні значення, розподіл залишків, QQ-Plot залишків, залишки vs. прогнозні значення*')\n",
    "\n",
    "print(\"\\nРучний (математичний) метод розрахунку завершено. Результати візуалізовано та збережено у файл 'manual_method_results.png'\")\n",
    "\n",
    "# ================= ЧАСТИНА 3: МЕТОД STATSMODELS.API.OLS =================\n",
    "print(\"\\n\\nПочаток виконання методу statsmodels.api.OLS...\")\n",
    "append_to_md(md_filename, '## 3. Метод statsmodels.api.OLS')\n",
    "append_to_md(md_filename, 'Для побудови нелінійної множинної регресії використаємо бібліотеку statsmodels, яка надає зручний інтерфейс для статистичного аналізу даних. Метод OLS (Ordinary Least Squares) реалізує метод найменших квадратів.')\n",
    "\n",
    "# Підготовка даних для statsmodels\n",
    "# Створюємо DataFrame з ознаками\n",
    "X_sm = pd.DataFrame({\n",
    "    'const': 1,  # Додаємо константу для вільного члена (a0)\n",
    "    'x1': x1,\n",
    "    'x2': x2,\n",
    "    'x1_sq': x1**2,\n",
    "    'x2_sq': x2**2\n",
    "})\n",
    "\n",
    "# Виведення перших 5 рядків матриці ознак\n",
    "print(\"\\nПерші 5 рядків матриці ознак X для statsmodels:\")\n",
    "print(X_sm.head())\n",
    "append_to_md(md_filename, '### Матриця ознак X для statsmodels')\n",
    "append_to_md(md_filename, 'Для застосування методу OLS створюємо матрицю ознак X у вигляді DataFrame:')\n",
    "append_to_md(md_filename, f'```\\n{X_sm.head().to_string()}\\n```')\n",
    "\n",
    "# Побудова моделі за допомогою statsmodels.api.OLS\n",
    "model_sm = sm.OLS(y, X_sm).fit()\n",
    "\n",
    "# Виведення результатів\n",
    "print(\"\\nРезультати регресії (statsmodels.api.OLS):\")\n",
    "print(model_sm.summary())\n",
    "\n",
    "# Зберігаємо основні результати у markdown-файл\n",
    "append_to_md(md_filename, '### Результати регресії (statsmodels.api.OLS)')\n",
    "append_to_md(md_filename, 'Результати регресійного аналізу за допомогою statsmodels.api.OLS:')\n",
    "append_to_md(md_filename, '```')\n",
    "append_to_md(md_filename, f'R-squared:                       {model_sm.rsquared:.4f}')\n",
    "append_to_md(md_filename, f'Adj. R-squared:                  {model_sm.rsquared_adj:.4f}')\n",
    "append_to_md(md_filename, f'F-statistic:                     {model_sm.fvalue:.4f}')\n",
    "append_to_md(md_filename, f'Prob (F-statistic):              {model_sm.f_pvalue:.4e}')\n",
    "append_to_md(md_filename, f'Log-Likelihood:                  {model_sm.llf:.4f}')\n",
    "append_to_md(md_filename, f'AIC:                             {model_sm.aic:.4f}')\n",
    "append_to_md(md_filename, f'BIC:                             {model_sm.bic:.4f}')\n",
    "append_to_md(md_filename, '```')\n",
    "\n",
    "# Отримані коефіцієнти\n",
    "beta_sm = model_sm.params\n",
    "print(\"\\nОтримані коефіцієнти регресії (statsmodels.api.OLS):\")\n",
    "print(f\"a0 (const) = {beta_sm['const']:.4f}\")\n",
    "print(f\"a1 (x1) = {beta_sm['x1']:.4f}\")\n",
    "print(f\"a2 (x2) = {beta_sm['x2']:.4f}\")\n",
    "print(f\"a3 (x1_sq) = {beta_sm['x1_sq']:.4f}\")\n",
    "print(f\"a4 (x2_sq) = {beta_sm['x2_sq']:.4f}\")\n",
    "\n",
    "append_to_md(md_filename, '### Отримані коефіцієнти регресії (statsmodels.api.OLS)')\n",
    "append_to_md(md_filename, f'- a0 (const) = {beta_sm[\"const\"]:.4f}')\n",
    "append_to_md(md_filename, f'- a1 (x1) = {beta_sm[\"x1\"]:.4f}')\n",
    "append_to_md(md_filename, f'- a2 (x2) = {beta_sm[\"x2\"]:.4f}')\n",
    "append_to_md(md_filename, f'- a3 (x1_sq) = {beta_sm[\"x1_sq\"]:.4f}')\n",
    "append_to_md(md_filename, f'- a4 (x2_sq) = {beta_sm[\"x2_sq\"]:.4f}')\n",
    "\n",
    "# Порівняння з вихідними коефіцієнтами\n",
    "append_to_md(md_filename, '### Порівняння з вихідними коефіцієнтами')\n",
    "table_content = '| Коефіцієнт | Вихідне значення | Отримане значення | Різниця |\\n'\n",
    "table_content += '|------------|------------------|-------------------|--------|\\n'\n",
    "table_content += f'| a0 | {a0} | {beta_sm[\"const\"]:.4f} | {abs(a0 - beta_sm[\"const\"]):.4f} |\\n'\n",
    "table_content += f'| a1 | {a1} | {beta_sm[\"x1\"]:.4f} | {abs(a1 - beta_sm[\"x1\"]):.4f} |\\n'\n",
    "table_content += f'| a2 | {a2} | {beta_sm[\"x2\"]:.4f} | {abs(a2 - beta_sm[\"x2\"]):.4f} |\\n'\n",
    "table_content += f'| a3 | {a3} | {beta_sm[\"x1_sq\"]:.4f} | {abs(a3 - beta_sm[\"x1_sq\"]):.4f} |\\n'\n",
    "table_content += f'| a4 | {a4} | {beta_sm[\"x2_sq\"]:.4f} | {abs(a4 - beta_sm[\"x2_sq\"]):.4f} |'\n",
    "append_to_md(md_filename, table_content)\n",
    "\n",
    "# Обчислення прогнозних значень y\n",
    "y_pred_sm = model_sm.predict(X_sm)\n",
    "\n",
    "# Розрахунок залишків\n",
    "residuals_sm = model_sm.resid\n",
    "\n",
    "# Розрахунок коефіцієнта детермінації R^2\n",
    "R2_sm = model_sm.rsquared\n",
    "print(f\"\\nКоефіцієнт детермінації R^2 (statsmodels.api.OLS): {R2_sm:.4f}\")\n",
    "\n",
    "# Критерій Дарбіна-Уотсона\n",
    "dw_sm = durbin_watson(residuals_sm)\n",
    "print(f\"\\nКритерій Дарбіна-Уотсона (statsmodels.api.OLS): {dw_sm:.4f}\")\n",
    "if dw_sm < 1.5:\n",
    "    dw_interpretation_sm = \"Є позитивна автокореляція залишків\"\n",
    "elif dw_sm > 2.5:\n",
    "    dw_interpretation_sm = \"Є негативна автокореляція залишків\"\n",
    "else:\n",
    "    dw_interpretation_sm = \"Автокореляція залишків відсутня або незначна\"\n",
    "print(dw_interpretation_sm)\n",
    "\n",
    "# Кореляційне відношення η\n",
    "eta_sm = np.sqrt(R2_sm)\n",
    "print(f\"\\nКореляційне відношення η (statsmodels.api.OLS): {eta_sm:.4f}\")\n",
    "if eta_sm < 0.3:\n",
    "    eta_interpretation_sm = \"Слабкий нелінійний зв'язок\"\n",
    "elif eta_sm < 0.7:\n",
    "    eta_interpretation_sm = \"Помірний нелінійний зв'язок\"\n",
    "else:\n",
    "    eta_interpretation_sm = \"Сильний нелінійний зв'язок\"\n",
    "print(eta_interpretation_sm)\n",
    "\n",
    "append_to_md(md_filename, '### Оцінка якості моделі (statsmodels.api.OLS)')\n",
    "append_to_md(md_filename, f'Коефіцієнт детермінації R^2 = {R2_sm:.4f}')\n",
    "append_to_md(md_filename, f'Критерій Дарбіна-Уотсона = {dw_sm:.4f}')\n",
    "append_to_md(md_filename, f'Інтерпретація: {dw_interpretation_sm}')\n",
    "append_to_md(md_filename, f'Кореляційне відношення η = {eta_sm:.4f}')\n",
    "append_to_md(md_filename, f'Інтерпретація: {eta_interpretation_sm}')\n",
    "\n",
    "# Візуалізація результатів statsmodels.api.OLS\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Графік фактичних vs. прогнозних значень\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.scatter(y, y_pred_sm, alpha=0.7)\n",
    "ax1.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "ax1.set_xlabel('Фактичні значення y')\n",
    "ax1.set_ylabel('Прогнозні значення y')\n",
    "ax1.set_title('Фактичні vs. Прогнозні значення')\n",
    "\n",
    "# 2. Гістограма залишків\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.hist(residuals_sm, bins=20, alpha=0.7)\n",
    "ax2.axvline(x=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Залишки')\n",
    "ax2.set_ylabel('Частота')\n",
    "ax2.set_title('Розподіл залишків')\n",
    "\n",
    "# 3. QQ-Plot для перевірки нормальності залишків\n",
    "ax3 = fig.add_subplot(223)\n",
    "stats.probplot(residuals_sm, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('QQ-Plot залишків')\n",
    "\n",
    "# 4. Залишки vs. Прогнозні значення\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.scatter(y_pred_sm, residuals_sm, alpha=0.7)\n",
    "ax4.axhline(y=0, color='r', linestyle='--')\n",
    "ax4.set_xlabel('Прогнозні значення')\n",
    "ax4.set_ylabel('Залишки')\n",
    "ax4.set_title('Залишки vs. Прогнозні значення')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('statsmodels_method_results.png')\n",
    "\n",
    "append_to_md(md_filename, '### Візуалізація результатів statsmodels.api.OLS:')\n",
    "append_to_md(md_filename, '![Візуалізація результатів statsmodels.api.OLS](statsmodels_method_results.png)')\n",
    "append_to_md(md_filename, '*Рис. 3. Візуалізація результатів statsmodels.api.OLS: фактичні vs. прогнозні значення, розподіл залишків, QQ-Plot залишків, залишки vs. прогнозні значення*')\n",
    "\n",
    "print(\"\\nМетод statsmodels.api.OLS завершено. Результати візуалізовано та збережено у файл 'statsmodels_method_results.png'\")\n",
    "\n",
    "# ================= ЧАСТИНА 4: МЕТОД SKLEARN.LINEAR_MODEL.LINEARREGRESSION =================\n",
    "print(\"\\n\\nПочаток виконання методу sklearn.linear_model.LinearRegression...\")\n",
    "append_to_md(md_filename, '## 4. Метод sklearn.linear_model.LinearRegression')\n",
    "append_to_md(md_filename, 'Для побудови нелінійної множинної регресії використаємо бібліотеку scikit-learn, яка надає зручний інтерфейс для машинного навчання. Для створення поліноміальних ознак використаємо PolynomialFeatures.')\n",
    "\n",
    "# Підготовка даних для sklearn\n",
    "# Створюємо матрицю ознак\n",
    "X_sk_base = np.column_stack((x1, x2))\n",
    "\n",
    "# Використовуємо PolynomialFeatures для створення поліноміальних ознак\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_sk_poly = poly.fit_transform(X_sk_base)\n",
    "\n",
    "# Виведення перших 5 рядків матриці ознак\n",
    "print(\"\\nПерші 5 рядків матриці ознак X для sklearn:\")\n",
    "print(X_sk_poly[:5])\n",
    "\n",
    "# Виведення назв ознак\n",
    "feature_names = poly.get_feature_names_out(['x1', 'x2'])\n",
    "print(\"\\nНазви ознак після перетворення:\")\n",
    "print(feature_names)\n",
    "\n",
    "append_to_md(md_filename, '### Матриця ознак X для sklearn')\n",
    "append_to_md(md_filename, 'Для застосування методу LinearRegression створюємо матрицю ознак X за допомогою PolynomialFeatures:')\n",
    "append_to_md(md_filename, f'```\\n{pd.DataFrame(X_sk_poly[:5], columns=feature_names).to_string()}\\n```')\n",
    "\n",
    "# Побудова моделі за допомогою sklearn.linear_model.LinearRegression\n",
    "model_sk = LinearRegression()\n",
    "model_sk.fit(X_sk_poly, y)\n",
    "\n",
    "# Отримані коефіцієнти\n",
    "beta_sk = model_sk.coef_\n",
    "intercept_sk = model_sk.intercept_\n",
    "\n",
    "print(\"\\nОтримані коефіцієнти регресії (sklearn.linear_model.LinearRegression):\")\n",
    "print(f\"a0 (intercept) = {intercept_sk:.4f}\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"Коефіцієнт при {name} = {beta_sk[i]:.4f}\")\n",
    "\n",
    "# Визначаємо відповідність коефіцієнтів моделі до наших a0, a1, a2, a3, a4\n",
    "a0_sk = intercept_sk\n",
    "a1_sk = beta_sk[0]  # x1\n",
    "a2_sk = beta_sk[1]  # x2\n",
    "a3_sk = beta_sk[2]  # x1^2\n",
    "a4_sk = beta_sk[4]  # x2^2\n",
    "\n",
    "append_to_md(md_filename, '### Отримані коефіцієнти регресії (sklearn.linear_model.LinearRegression)')\n",
    "append_to_md(md_filename, f'- a0 (intercept) = {a0_sk:.4f}')\n",
    "append_to_md(md_filename, f'- a1 (x1) = {a1_sk:.4f}')\n",
    "append_to_md(md_filename, f'- a2 (x2) = {a2_sk:.4f}')\n",
    "append_to_md(md_filename, f'- a3 (x1^2) = {a3_sk:.4f}')\n",
    "append_to_md(md_filename, f'- a4 (x2^2) = {a4_sk:.4f}')\n",
    "\n",
    "# Порівняння з вихідними коефіцієнтами\n",
    "append_to_md(md_filename, '### Порівняння з вихідними коефіцієнтами')\n",
    "table_content = '| Коефіцієнт | Вихідне значення | Отримане значення | Різниця |\\n'\n",
    "table_content += '|------------|------------------|-------------------|--------|\\n'\n",
    "table_content += f'| a0 | {a0} | {a0_sk:.4f} | {abs(a0 - a0_sk):.4f} |\\n'\n",
    "table_content += f'| a1 | {a1} | {a1_sk:.4f} | {abs(a1 - a1_sk):.4f} |\\n'\n",
    "table_content += f'| a2 | {a2} | {a2_sk:.4f} | {abs(a2 - a2_sk):.4f} |\\n'\n",
    "table_content += f'| a3 | {a3} | {a3_sk:.4f} | {abs(a3 - a3_sk):.4f} |\\n'\n",
    "table_content += f'| a4 | {a4} | {a4_sk:.4f} | {abs(a4 - a4_sk):.4f} |'\n",
    "append_to_md(md_filename, table_content)\n",
    "\n",
    "# Обчислення прогнозних значень y\n",
    "y_pred_sk = model_sk.predict(X_sk_poly)\n",
    "\n",
    "# Розрахунок залишків\n",
    "residuals_sk = y - y_pred_sk\n",
    "\n",
    "# Розрахунок коефіцієнта детермінації R^2\n",
    "R2_sk = r2_score(y, y_pred_sk)\n",
    "print(f\"\\nКоефіцієнт детермінації R^2 (sklearn.linear_model.LinearRegression): {R2_sk:.4f}\")\n",
    "\n",
    "# Критерій Дарбіна-Уотсона\n",
    "dw_sk = np.sum(np.diff(residuals_sk)**2) / np.sum(residuals_sk**2)\n",
    "print(f\"\\nКритерій Дарбіна-Уотсона (sklearn.linear_model.LinearRegression): {dw_sk:.4f}\")\n",
    "if dw_sk < 1.5:\n",
    "    dw_interpretation_sk = \"Є позитивна автокореляція залишків\"\n",
    "elif dw_sk > 2.5:\n",
    "    dw_interpretation_sk = \"Є негативна автокореляція залишків\"\n",
    "else:\n",
    "    dw_interpretation_sk = \"Автокореляція залишків відсутня або незначна\"\n",
    "print(dw_interpretation_sk)\n",
    "\n",
    "# Кореляційне відношення η\n",
    "eta_sk = np.sqrt(R2_sk)\n",
    "print(f\"\\nКореляційне відношення η (sklearn.linear_model.LinearRegression): {eta_sk:.4f}\")\n",
    "if eta_sk < 0.3:\n",
    "    eta_interpretation_sk = \"Слабкий нелінійний зв'язок\"\n",
    "elif eta_sk < 0.7:\n",
    "    eta_interpretation_sk = \"Помірний нелінійний зв'язок\"\n",
    "else:\n",
    "    eta_interpretation_sk = \"Сильний нелінійний зв'язок\"\n",
    "print(eta_interpretation_sk)\n",
    "\n",
    "append_to_md(md_filename, '### Оцінка якості моделі (sklearn.linear_model.LinearRegression)')\n",
    "append_to_md(md_filename, f'Коефіцієнт детермінації R^2 = {R2_sk:.4f}')\n",
    "append_to_md(md_filename, f'Критерій Дарбіна-Уотсона = {dw_sk:.4f}')\n",
    "append_to_md(md_filename, f'Інтерпретація: {dw_interpretation_sk}')\n",
    "append_to_md(md_filename, f'Кореляційне відношення η = {eta_sk:.4f}')\n",
    "append_to_md(md_filename, f'Інтерпретація: {eta_interpretation_sk}')\n",
    "\n",
    "# Візуалізація результатів sklearn.linear_model.LinearRegression\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Графік фактичних vs. прогнозних значень\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.scatter(y, y_pred_sk, alpha=0.7)\n",
    "ax1.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "ax1.set_xlabel('Фактичні значення y')\n",
    "ax1.set_ylabel('Прогнозні значення y')\n",
    "ax1.set_title('Фактичні vs. Прогнозні значення')\n",
    "\n",
    "# 2. Гістограма залишків\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.hist(residuals_sk, bins=20, alpha=0.7)\n",
    "ax2.axvline(x=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Залишки')\n",
    "ax2.set_ylabel('Частота')\n",
    "ax2.set_title('Розподіл залишків')\n",
    "\n",
    "# 3. QQ-Plot для перевірки нормальності залишків\n",
    "ax3 = fig.add_subplot(223)\n",
    "stats.probplot(residuals_sk, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('QQ-Plot залишків')\n",
    "\n",
    "# 4. Залишки vs. Прогнозні значення\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.scatter(y_pred_sk, residuals_sk, alpha=0.7)\n",
    "ax4.axhline(y=0, color='r', linestyle='--')\n",
    "ax4.set_xlabel('Прогнозні значення')\n",
    "ax4.set_ylabel('Залишки')\n",
    "ax4.set_title('Залишки vs. Прогнозні значення')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sklearn_method_results.png')\n",
    "\n",
    "append_to_md(md_filename, '### Візуалізація результатів sklearn.linear_model.LinearRegression:')\n",
    "append_to_md(md_filename, '![Візуалізація результатів sklearn.linear_model.LinearRegression](sklearn_method_results.png)')\n",
    "append_to_md(md_filename, '*Рис. 4. Візуалізація результатів sklearn.linear_model.LinearRegression: фактичні vs. прогнозні значення, розподіл залишків, QQ-Plot залишків, залишки vs. прогнозні значення*')\n",
    "\n",
    "print(\"\\nМетод sklearn.linear_model.LinearRegression завершено. Результати візуалізовано та збережено у файл 'sklearn_method_results.png'\")\n",
    "\n",
    "# ================= ЧАСТИНА 5: ПОРІВНЯННЯ МЕТОДІВ =================\n",
    "print(\"\\n\\nПочаток порівняння методів...\")\n",
    "append_to_md(md_filename, '## 5. Порівняння методів')\n",
    "append_to_md(md_filename, 'Порівняємо результати, отримані різними методами регресійного аналізу.')\n",
    "\n",
    "# Порівняння коефіцієнтів\n",
    "append_to_md(md_filename, '### Порівняння коефіцієнтів моделі')\n",
    "table_content = '| Коефіцієнт | Вихідне значення | Ручний метод | statsmodels.api.OLS | sklearn.linear_model.LinearRegression |\\n'\n",
    "table_content += '|------------|------------------|--------------|-------------------|-----------------------------------|\\n'\n",
    "table_content += f'| a0 | {a0} | {beta_manual[0]:.4f} | {beta_sm[\"const\"]:.4f} | {a0_sk:.4f} |\\n'\n",
    "table_content += f'| a1 | {a1} | {beta_manual[1]:.4f} | {beta_sm[\"x1\"]:.4f} | {a1_sk:.4f} |\\n'\n",
    "table_content += f'| a2 | {a2} | {beta_manual[2]:.4f} | {beta_sm[\"x2\"]:.4f} | {a2_sk:.4f} |\\n'\n",
    "table_content += f'| a3 | {a3} | {beta_manual[3]:.4f} | {beta_sm[\"x1_sq\"]:.4f} | {a3_sk:.4f} |\\n'\n",
    "table_content += f'| a4 | {a4} | {beta_manual[4]:.4f} | {beta_sm[\"x2_sq\"]:.4f} | {a4_sk:.4f} |'\n",
    "append_to_md(md_filename, table_content)\n",
    "\n",
    "# Порівняння метрик якості моделі\n",
    "append_to_md(md_filename, '### Порівняння метрик якості моделі')\n",
    "table_content = '| Метрика | Ручний метод | statsmodels.api.OLS | sklearn.linear_model.LinearRegression |\\n'\n",
    "table_content += '|---------|--------------|-------------------|-----------------------------------|\\n'\n",
    "table_content += f'| R^2 | {R2_manual:.4f} | {R2_sm:.4f} | {R2_sk:.4f} |\\n'\n",
    "table_content += f'| Критерій Дарбіна-Уотсона | {dw_manual:.4f} | {dw_sm:.4f} | {dw_sk:.4f} |\\n'\n",
    "table_content += f'| Кореляційне відношення η | {eta_manual:.4f} | {eta_sm:.4f} | {eta_sk:.4f} |'\n",
    "append_to_md(md_filename, table_content)\n",
    "\n",
    "# Візуалізація порівняння методів\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Порівняння коефіцієнтів\n",
    "ax1 = fig.add_subplot(221)\n",
    "coef_names = ['a0', 'a1', 'a2', 'a3', 'a4']\n",
    "coef_values = {\n",
    "    'Вихідні': [a0, a1, a2, a3, a4],\n",
    "    'Ручний метод': [beta_manual[0], beta_manual[1], beta_manual[2], beta_manual[3], beta_manual[4]],\n",
    "    'statsmodels': [beta_sm['const'], beta_sm['x1'], beta_sm['x2'], beta_sm['x1_sq'], beta_sm['x2_sq']],\n",
    "    'sklearn': [a0_sk, a1_sk, a2_sk, a3_sk, a4_sk]\n",
    "}\n",
    "\n",
    "x = np.arange(len(coef_names))\n",
    "width = 0.2\n",
    "ax1.bar(x - 1.5*width, coef_values['Вихідні'], width, label='Вихідні')\n",
    "ax1.bar(x - 0.5*width, coef_values['Ручний метод'], width, label='Ручний метод')\n",
    "ax1.bar(x + 0.5*width, coef_values['statsmodels'], width, label='statsmodels')\n",
    "ax1.bar(x + 1.5*width, coef_values['sklearn'], width, label='sklearn')\n",
    "ax1.set_xlabel('Коефіцієнти')\n",
    "ax1.set_ylabel('Значення')\n",
    "ax1.set_title('Порівняння коефіцієнтів')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(coef_names)\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Порівняння метрик якості\n",
    "ax2 = fig.add_subplot(222)\n",
    "metric_names = ['R^2', 'DW', 'η']\n",
    "metric_values = {\n",
    "    'Ручний метод': [R2_manual, dw_manual, eta_manual],\n",
    "    'statsmodels': [R2_sm, dw_sm, eta_sm],\n",
    "    'sklearn': [R2_sk, dw_sk, eta_sk]\n",
    "}\n",
    "\n",
    "x = np.arange(len(metric_names))\n",
    "width = 0.25\n",
    "ax2.bar(x - width, metric_values['Ручний метод'], width, label='Ручний метод')\n",
    "ax2.bar(x, metric_values['statsmodels'], width, label='statsmodels')\n",
    "ax2.bar(x + width, metric_values['sklearn'], width, label='sklearn')\n",
    "ax2.set_xlabel('Метрики')\n",
    "ax2.set_ylabel('Значення')\n",
    "ax2.set_title('Порівняння метрик якості')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metric_names)\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Порівняння прогнозних значень\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.scatter(y, y_pred_manual, alpha=0.5, label='Ручний метод')\n",
    "ax3.scatter(y, y_pred_sm, alpha=0.5, label='statsmodels')\n",
    "ax3.scatter(y, y_pred_sk, alpha=0.5, label='sklearn')\n",
    "ax3.plot([y.min(), y.max()], [y.min(), y.max()], 'k--')\n",
    "ax3.set_xlabel('Фактичні значення y')\n",
    "ax3.set_ylabel('Прогнозні значення y')\n",
    "ax3.set_title('Порівняння прогнозних значень')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Порівняння залишків\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.boxplot([residuals_manual, residuals_sm, residuals_sk], tick_labels=['Ручний метод', 'statsmodels', 'sklearn'])\n",
    "ax4.set_ylabel('Залишки')\n",
    "ax4.set_title('Порівняння розподілу залишків')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('methods_comparison.png')\n",
    "\n",
    "append_to_md(md_filename, '### Візуалізація порівняння методів:')\n",
    "append_to_md(md_filename, '![Візуалізація порівняння методів](methods_comparison.png)')\n",
    "append_to_md(md_filename, '*Рис. 5. Візуалізація порівняння методів: коефіцієнти, метрики якості, прогнозні значення, розподіл залишків*')\n",
    "\n",
    "# 3D візуалізація порівняння методів\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Створення сітки для поверхні\n",
    "x1_grid = np.linspace(min(x1), max(x1), 30)\n",
    "x2_grid = np.linspace(min(x2), max(x2), 30)\n",
    "X1_grid, X2_grid = np.meshgrid(x1_grid, x2_grid)\n",
    "\n",
    "# Обчислення значень y для кожного методу\n",
    "# Ручний метод\n",
    "Y_grid_manual = (beta_manual[0] +\n",
    "                beta_manual[1] * X1_grid +\n",
    "                beta_manual[2] * X2_grid +\n",
    "                beta_manual[3] * X1_grid**2 +\n",
    "                beta_manual[4] * X2_grid**2)\n",
    "\n",
    "# statsmodels\n",
    "Y_grid_sm = (beta_sm['const'] +\n",
    "            beta_sm['x1'] * X1_grid +\n",
    "            beta_sm['x2'] * X2_grid +\n",
    "            beta_sm['x1_sq'] * X1_grid**2 +\n",
    "            beta_sm['x2_sq'] * X2_grid**2)\n",
    "\n",
    "# sklearn\n",
    "Y_grid_sk = (a0_sk +\n",
    "            a1_sk * X1_grid +\n",
    "            a2_sk * X2_grid +\n",
    "            a3_sk * X1_grid**2 +\n",
    "            a4_sk * X2_grid**2)\n",
    "\n",
    "# Істинні значення\n",
    "Y_grid_true = (a0 +\n",
    "              a1 * X1_grid +\n",
    "              a2 * X2_grid +\n",
    "              a3 * X1_grid**2 +\n",
    "              a4 * X2_grid**2)\n",
    "\n",
    "# Створення 3D графіків\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Істинна поверхня\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "surf1 = ax1.plot_surface(X1_grid, X2_grid, Y_grid_true, cmap='viridis', alpha=0.7)\n",
    "ax1.scatter(x1, x2, y, color='r', alpha=0.5)\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.set_zlabel('Y')\n",
    "ax1.set_title('Істинна поверхня')\n",
    "\n",
    "# 2. Ручний метод\n",
    "ax2 = fig.add_subplot(222, projection='3d')\n",
    "surf2 = ax2.plot_surface(X1_grid, X2_grid, Y_grid_manual, cmap='plasma', alpha=0.7)\n",
    "ax2.scatter(x1, x2, y, color='r', alpha=0.5)\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('X2')\n",
    "ax2.set_zlabel('Y')\n",
    "ax2.set_title('Ручний метод')\n",
    "\n",
    "# 3. statsmodels\n",
    "ax3 = fig.add_subplot(223, projection='3d')\n",
    "surf3 = ax3.plot_surface(X1_grid, X2_grid, Y_grid_sm, cmap='inferno', alpha=0.7)\n",
    "ax3.scatter(x1, x2, y, color='r', alpha=0.5)\n",
    "ax3.set_xlabel('X1')\n",
    "ax3.set_ylabel('X2')\n",
    "ax3.set_zlabel('Y')\n",
    "ax3.set_title('statsmodels.api.OLS')\n",
    "\n",
    "# 4. sklearn\n",
    "ax4 = fig.add_subplot(224, projection='3d')\n",
    "surf4 = ax4.plot_surface(X1_grid, X2_grid, Y_grid_sk, cmap='cividis', alpha=0.7)\n",
    "ax4.scatter(x1, x2, y, color='r', alpha=0.5)\n",
    "ax4.set_xlabel('X1')\n",
    "ax4.set_ylabel('X2')\n",
    "ax4.set_zlabel('Y')\n",
    "ax4.set_title('sklearn.linear_model.LinearRegression')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('3d_comparison.png')\n",
    "\n",
    "append_to_md(md_filename, '### 3D візуалізація порівняння методів:')\n",
    "append_to_md(md_filename, '![3D візуалізація порівняння методів](3d_comparison.png)')\n",
    "append_to_md(md_filename, '*Рис. 6. 3D візуалізація порівняння методів: істинна поверхня, ручний метод, statsmodels.api.OLS, sklearn.linear_model.LinearRegression*')\n",
    "\n",
    "# ================= ЧАСТИНА 6: ВИСНОВКИ =================\n",
    "print(\"\\n\\nФормування висновків...\")\n",
    "append_to_md(md_filename, '## 6. Висновки')\n",
    "append_to_md(md_filename, 'На основі проведеного нелінійного регресійного аналізу можна зробити наступні висновки:')\n",
    "append_to_md(md_filename, '1. **Порівняння методів**: Усі три методи (ручний розрахунок, statsmodels.api.OLS, sklearn.linear_model.LinearRegression) дали дуже близькі результати, що підтверджує правильність їх реалізації.')\n",
    "append_to_md(md_filename, '2. **Точність моделі**: Отримані коефіцієнти регресії близькі до вихідних значень, що свідчить про високу точність моделі. Незначні відхилення пояснюються наявністю випадкового шуму в даних.')\n",
    "append_to_md(md_filename, '3. **Якість моделі**: Високі значення коефіцієнта детермінації R² (близько 0.99) вказують на те, що модель дуже добре описує дані.')\n",
    "append_to_md(md_filename, '4. **Автокореляція залишків**: Значення критерію Дарбіна-Уотсона близько 2.1 свідчить про відсутність автокореляції залишків, що є хорошим показником адекватності моделі.')\n",
    "append_to_md(md_filename, '5. **Нелінійний зв\\'язок**: Високі значення кореляційного відношення η (близько 0.99) вказують на сильний нелінійний зв\\'язок між змінними.')\n",
    "append_to_md(md_filename, '6. **Інтерпретація коефіцієнтів**:')\n",
    "append_to_md(md_filename, '   - a₀ ≈ 3.28: Базове значення y при нульових значеннях x₁ та x₂.')\n",
    "append_to_md(md_filename, '   - a₁ ≈ 2.67: При збільшенні x₁ на одиницю, y збільшується приблизно на 2.67 одиниць (за умови, що інші змінні залишаються незмінними).')\n",
    "append_to_md(md_filename, '   - a₂ ≈ -0.42: При збільшенні x₂ на одиницю, y зменшується приблизно на 0.42 одиниці (за умови, що інші змінні залишаються незмінними).')\n",
    "append_to_md(md_filename, '   - a₃ ≈ 0.45: Додатний коефіцієнт при x₁² вказує на опуклу (параболічну вгору) залежність від x₁.')\n",
    "append_to_md(md_filename, '   - a₄ ≈ 0.27: Додатний коефіцієнт при x₂² вказує на опуклу (параболічну вгору) залежність від x₂.')\n",
    "append_to_md(md_filename, '7. **Можливі шляхи покращення моделі**:')\n",
    "append_to_md(md_filename, '   - Включення взаємодій між змінними (x₁·x₂).')\n",
    "append_to_md(md_filename, '   - Розгляд поліномів вищих порядків.')\n",
    "append_to_md(md_filename, '   - Застосування методів регуляризації для запобігання перенавчанню.')\n",
    "append_to_md(md_filename, '   - Аналіз впливових спостережень та викидів.')\n",
    "\n",
    "print(\"\\nАналіз завершено. Результати збережено у файл 'результати_аналізу.md'\")\n",
    "print(\"Графіки збережено у файли:\")\n",
    "print(\"- 'generated_data_visualization.png'\")\n",
    "print(\"- 'manual_method_results.png'\")\n",
    "print(\"- 'statsmodels_method_results.png'\")\n",
    "print(\"- 'sklearn_method_results.png'\")\n",
    "print(\"- 'methods_comparison.png'\")\n",
    "print(\"- '3d_comparison.png'\")\n",
    "```"
   ],
   "id": "a424ab39c37342e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
