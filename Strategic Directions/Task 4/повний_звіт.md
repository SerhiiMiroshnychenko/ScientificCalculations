# Звіт з практичного заняття
## «Нелінійний парний та множинний регресійний аналіз даних. Оцінка метричних показників»

# Мета заняття

Метою практичного заняття з теми «Нелінійний парний та множинний регресійний аналіз даних. Оцінка метричних показників» є ґрунтовне ознайомлення з теоретичними засадами та практичними аспектами застосування методів нелінійного множинного регресійного аналізу для дослідження взаємозв'язків між залежною змінною та кількома незалежними змінними.

Регресійний аналіз є потужним статистичним інструментом, який дозволяє моделювати та аналізувати взаємозв'язки між змінними. На відміну від лінійної регресії, нелінійна регресія дає можливість описувати більш складні залежності, які не можуть бути адекватно представлені лінійними функціями. Це особливо важливо при роботі з реальними даними, де взаємозв'язки часто мають нелінійний характер.

# Опис процесу побудови нелінійної регресійної моделі

У даному дослідженні розглядається процес побудови нелінійної множинної регресійної моделі для варіанту 4, яка має вигляд:

$$y(x) = a_0 + a_1 \cdot x_1 + a_2 \cdot x_2 + a_3 \cdot x_1^2 + a_4 \cdot x_2^2 + rnd(b)$$

де $a_0, a_1, a_2, a_3, a_4$ - коефіцієнти моделі, а $rnd(b)$ - випадкова складова з нормальним розподілом $N(0, b)$.

Процес побудови нелінійної регресійної моделі складається з кількох послідовних етапів, кожен з яких має важливе значення для отримання адекватних результатів.

На першому етапі було згенеровано синтетичні дані на основі заданої моделі. Для цього використовувалися наступні параметри:

- Кількість спостережень: 100
- Діапазон значень незалежних змінних: $x_1, x_2 \in [-5, 5]$
- Коефіцієнти моделі: $a_0 = 3.4$, $a_1 = 2.7$, $a_2 = -0.5$, $a_3 = 0.45$, $a_4 = 0.25$
- Параметр шуму: $b = 1.10$

Незалежні змінні $x_1$ та $x_2$ були згенеровані з використанням рівномірного розподілу на інтервалі $[-5, 5]$. Для кожної пари значень $(x_1, x_2)$ було обчислено відповідне значення залежної змінної $y$ за формулою моделі з додаванням випадкової складової $rnd(b)$, що має нормальний розподіл з нульовим математичним сподіванням та стандартним відхиленням $b = 1.10$.


Для застосування методу найменших квадратів необхідно сформувати матрицю ознак $X$. У випадку нелінійної множинної регресії з квадратичними членами, матриця ознак має наступний вигляд:

$$X = \begin{pmatrix}
1 & x_{11} & x_{21} & x_{11}^2 & x_{21}^2 \\
1 & x_{12} & x_{22} & x_{12}^2 & x_{22}^2 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_{1n} & x_{2n} & x_{1n}^2 & x_{2n}^2
\end{pmatrix}$$

де $n$ - кількість спостережень.


Для знаходження коефіцієнтів регресійної моделі використовується метод найменших квадратів (МНК). Згідно з цим методом, вектор коефіцієнтів $\beta$ обчислюється за формулою:

$$\beta = (X^T X)^{-1} X^T y$$

де $X^T$ - транспонована матриця ознак, $(X^T X)^{-1}$ - обернена матриця до добутку $X^T X$, а $y$ - вектор значень залежної змінної.

Цей метод мінімізує суму квадратів відхилень між фактичними значеннями залежної змінної та значеннями, передбаченими моделлю.


Після отримання коефіцієнтів регресійної моделі необхідно оцінити її якість. Для цього використовуються різні метрики, зокрема:

- Коефіцієнт детермінації $R^2$, який показує, яку частку дисперсії залежної змінної пояснює модель.
- Критерій Дарбіна-Уотсона для перевірки наявності автокореляції залишків.
- Кореляційне відношення $\eta$ для оцінки сили нелінійного зв'язку між змінними.


У даному дослідженні було використано три різні підходи до побудови нелінійної регресійної моделі:

1. Ручний (математичний) метод з використанням формул методу найменших квадратів.

2. Використання бібліотеки statsmodels.api.OLS, яка реалізує метод найменших квадратів для побудови лінійних та нелінійних моделей.

3. Використання бібліотеки sklearn.linear_model.LinearRegression у поєднанні з sklearn.preprocessing.PolynomialFeatures для генерації поліноміальних ознак.

Порівняння цих методів дозволяє перевірити правильність реалізації ручного методу та оцінити ефективність різних програмних інструментів для регресійного аналізу.

# Отримання коефіцієнтів нелінійної регресійної моделі та їх інтерпретація

У процесі дослідження нелінійної множинної регресійної моделі було отримано коефіцієнти за допомогою трьох різних методів: ручного розрахунку, використання бібліотеки statsmodels.api.OLS та sklearn.linear_model.LinearRegression. Усі три методи дали практично ідентичні результати, що підтверджує правильність їх реалізації.

## Отримані коефіцієнти

Вихідні значення коефіцієнтів для моделі $y(x) = a_0 + a_1 \cdot x_1 + a_2 \cdot x_2 + a_3 \cdot x_1^2 + a_4 \cdot x_2^2 + rnd(b)$ були задані як:

- $a_0 = 3.4$ (вільний член)
- $a_1 = 2.7$ (коефіцієнт при $x_1$)
- $a_2 = -0.5$ (коефіцієнт при $x_2$)
- $a_3 = 0.45$ (коефіцієнт при $x_1^2$)
- $a_4 = 0.25$ (коефіцієнт при $x_2^2$)
- $rnd(b) = 1.10$ (параметр шуму)

У результаті застосування методу найменших квадратів було отримано наступні значення коефіцієнтів:

| Коефіцієнт | Вихідне значення | Ручний метод | statsmodels.api.OLS | sklearn.linear_model.LinearRegression |
|------------|------------------|--------------|-------------------|-----------------------------------|
| $a_0$ | 3.4 | 3.2849 | 3.2849 | 3.2853 |
| $a_1$ | 2.7 | 2.6660 | 2.6660 | 2.6660 |
| $a_2$ | -0.5 | -0.4242 | -0.4242 | -0.4241 |
| $a_3$ | 0.45 | 0.4520 | 0.4520 | 0.4520 |
| $a_4$ | 0.25 | 0.2744 | 0.2744 | 0.2744 |

Як видно з таблиці, отримані коефіцієнти дуже близькі до вихідних значень, що свідчить про високу точність методу найменших квадратів. Невеликі відхилення пояснюються наявністю випадкового шуму в даних.

Інтерпретація отриманих коефіцієнтів регресійної моделі має важливе значення для розуміння характеру взаємозв'язків між змінними. Розглянемо детальну інтерпретацію кожного коефіцієнта:

$a_0 \approx 3.28$ - вільний член рівняння регресії. Він представляє очікуване значення залежної змінної $y$ при нульових значеннях незалежних змінних $x_1$ та $x_2$. Тобто, якщо $x_1 = 0$ і $x_2 = 0$, то $y \approx 3.28$. Вільний член можна розглядати як базове значення залежної змінної за відсутності впливу факторів.

$a_1 \approx 2.67$ - коефіцієнт при змінній $x_1$. Він показує, як змінюється залежна змінна $y$ при зміні $x_1$ на одиницю, за умови, що інші змінні залишаються незмінними.

Позитивне значення цього коефіцієнта вказує на те, що зі збільшенням $x_1$ значення $y$ також збільшується. Зокрема, при збільшенні $x_1$ на одиницю, $y$ збільшується приблизно на 2.67 одиниць.

$a_2 \approx -0.42$ - коефіцієнт при змінній $x_2$. Він показує, як змінюється залежна змінна $y$ при зміні $x_2$ на одиницю, за умови, що інші змінні залишаються незмінними. Від'ємне значення цього коефіцієнта вказує на те, що зі збільшенням $x_2$ значення $y$ зменшується. Зокрема, при збільшенні $x_2$ на одиницю, $y$ зменшується приблизно на 0.42 одиниці.

$a_3 \approx 0.45$ - коефіцієнт при квадратичному члені $x_1^2$. Він відображає нелінійний характер залежності між $x_1$ та $y$.

Додатне значення цього коефіцієнта вказує на опуклу (параболічну вгору) залежність від $x_1$. Це означає, що вплив $x_1$ на $y$ посилюється зі збільшенням абсолютного значення $x_1$. Наприклад, при великих додатних або від'ємних значеннях $x_1$ внесок квадратичного члена стає більш значущим.

$a_4 \approx 0.27$ - коефіцієнт при квадратичному члені $x_2^2$. Аналогічно до $a_3$, він відображає нелінійний характер залежності між $x_2$ та $y$. Додатне значення цього коефіцієнта також вказує на опуклу (параболічну вгору) залежність від $x_2$.

# Оцінка якості моделі за допомогою критерія Дарбіна-Уотсона та кореляційного відношення

Для оцінки якості побудованої нелінійної регресійної моделі було використано кілька метрик, які дозволяють визначити адекватність моделі та силу зв'язку між змінними. Особливу увагу було приділено критерію Дарбіна-Уотсона для перевірки наявності автокореляції залишків та кореляційному відношенню для оцінки сили нелінійного зв'язку.

## Коефіцієнт детермінації $R^2$

Перш за все, було обчислено коефіцієнт детермінації $R^2$, який показує, яку частку дисперсії залежної змінної пояснює модель.

Коефіцієнт детермінації обчислюється за формулою:

$$R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$

де $y_i$ - фактичні значення залежної змінної, $\hat{y}_i$ - прогнозні значення, отримані за допомогою моделі, $\bar{y}$ - середнє значення залежної змінної, $n$ - кількість спостережень.

Для побудованої нелінійної регресійної моделі було отримано наступні значення коефіцієнта детермінації:

| Метод | $R^2$ |
|-------|-------|
| Ручний метод | 0.9844 |
| statsmodels.api.OLS | 0.9844 |
| sklearn.linear_model.LinearRegression | 0.9844 |

Значення $R^2 \approx 0.9844$ свідчить про те, що побудована модель пояснює приблизно 98.44% дисперсії залежної змінної, що є дуже високим показником. Це вказує на відмінну якість моделі та її здатність точно прогнозувати значення залежної змінної на основі незалежних змінних.


Критерій Дарбіна-Уотсона використовується для перевірки наявності автокореляції залишків моделі. Він обчислюється за формулою:

$$DW = \frac{\sum_{i=2}^{n} (e_i - e_{i-1})^2}{\sum_{i=1}^{n} e_i^2}$$

де $e_i$ - залишки моделі, тобто різниця між фактичними та прогнозними значеннями: $e_i = y_i - \hat{y}_i$.

Значення критерію Дарбіна-Уотсона може варіюватися від 0 до 4:
- Значення близько 2 вказує на відсутність автокореляції залишків.
- Значення близько 0 вказує на позитивну автокореляцію.
- Значення близько 4 вказує на негативну автокореляцію.

Для побудованої нелінійної регресійної моделі було отримано наступні значення критерію Дарбіна-Уотсона:

| Метод | Критерій Дарбіна-Уотсона |
|-------|--------------------------|
| Ручний метод | 2.1020 |
| statsmodels.api.OLS | 2.1020 |
| sklearn.linear_model.LinearRegression | 2.1019 |

Отримані значення критерію Дарбіна-Уотсона близькі до 2, що свідчить про відсутність автокореляції залишків. Це є хорошим показником адекватності моделі, оскільки відсутність автокореляції залишків є однією з передумов коректного застосування методу найменших квадратів.

Кореляційне відношення $\eta$ використовується для оцінки сили нелінійного зв'язку між змінними. На відміну від коефіцієнта кореляції Пірсона, який вимірює лише лінійний зв'язок, кореляційне відношення здатне виявити будь-який функціональний зв'язок, включаючи нелінійний.

Кореляційне відношення обчислюється за формулою:

$$\eta = \sqrt{\frac{\sigma_{\hat{y}}^2}{\sigma_y^2}}$$

де $\sigma_{\hat{y}}^2$ - дисперсія прогнозних значень, а $\sigma_y^2$ - дисперсія фактичних значень залежної змінної.

Значення кореляційного відношення знаходиться в діапазоні від 0 до 1:
- Значення близько 0 вказує на відсутність зв'язку між змінними.
- Значення близько 1 вказує на сильний функціональний зв'язок.

Для побудованої нелінійної регресійної моделі було отримано наступні значення кореляційного відношення:

| Метод | Кореляційне відношення $\eta$ |
|-------|-----------------------------|
| Ручний метод | 0.9922 |
| statsmodels.api.OLS | 0.9922 |
| sklearn.linear_model.LinearRegression | 0.9922 |

Отримані значення кореляційного відношення $\eta \approx 0.9922$ свідчать про дуже сильний нелінійний зв'язок між змінними. Це підтверджує адекватність вибору нелінійної регресійної моделі для опису взаємозв'язків між змінними.

На основі проведеного аналізу можна зробити наступні висновки щодо якості побудованої нелінійної регресійної моделі:

1. Високе значення коефіцієнта детермінації $R^2 \approx 0.9844$ свідчить про те, що модель пояснює близько 98.44% дисперсії залежної змінної, що є дуже хорошим показником.

2. Значення критерію Дарбіна-Уотсона близько 2.10 вказує на відсутність автокореляції залишків, що підтверджує коректність застосування методу найменших квадратів.

3. Високе значення кореляційного відношення $\eta \approx 0.9922$ свідчить про сильний нелінійний зв'язок між змінними, що підтверджує адекватність вибору нелінійної моделі.

# Аналіз отриманих результатів

У процесі дослідження нелінійної множинної регресійної моделі було отримано ряд важливих результатів, які потребують детального аналізу та інтерпретації. Розглянемо основні аспекти отриманих результатів та їх значення для розуміння досліджуваної моделі.

## Порівняння методів регресійного аналізу

У дослідженні було використано три різні методи для побудови нелінійної регресійної моделі: ручний (математичний) метод, метод statsmodels.api.OLS та метод sklearn.linear_model.LinearRegression.

Порівняння результатів, отриманих за допомогою цих методів, дозволяє оцінити їх ефективність та точність. Як видно з таблиць порівняння коефіцієнтів та метрик якості моделі, усі три методи дали практично ідентичні результати, що підтверджує правильність їх реалізації та надійність отриманих результатів.

Незначні відмінності у значеннях коефіцієнтів та метрик якості можуть бути пояснені особливостями реалізації алгоритмів та округленням числових значень. Проте ці відмінності настільки малі, що не впливають на загальні висновки щодо моделі.

Одним із ключових аспектів аналізу результатів є оцінка точності відновлення коефіцієнтів моделі. Порівняння отриманих коефіцієнтів з вихідними значеннями дозволяє оцінити ефективність методу найменших квадратів для відновлення параметрів моделі за наявності випадкового шуму.

Як видно з таблиці порівняння коефіцієнтів, отримані значення дуже близькі до вихідних. Найбільше відхилення спостерігається для вільного члена $a_0$ (різниця близько 0.115) та коефіцієнта при $x_2$ (різниця близько 0.076). Для інших коефіцієнтів відхилення менші.

Такі невеликі відхилення є цілком очікуваними, враховуючи наявність випадкового шуму в даних з параметром $b = 1.10$.

Аналіз метрик якості моделі дозволяє оцінити її адекватність та прогностичну здатність. Високе значення коефіцієнта детермінації $R^2 \approx 0.9844$ свідчить про те, що побудована модель пояснює близько 98.44% дисперсії залежної змінної, що є дуже хорошим показником.

Значення критерію Дарбіна-Уотсона близько 2.10 вказує на відсутність автокореляції залишків. Це важливий результат, оскільки відсутність автокореляції є однією з передумов коректного застосування методу найменших квадратів. Якщо б значення критерію суттєво відрізнялося від 2, це могло б свідчити про наявність систематичних помилок у моделі.

Високе значення кореляційного відношення $\eta \approx 0.9922$ свідчить про сильний нелінійний зв'язок між змінними. Це підтверджує адекватність вибору нелінійної моделі для опису взаємозв'язків між змінними. Якщо б зв'язок був переважно лінійним, то включення квадратичних членів не дало б такого значного покращення якості моделі.


Візуалізація результатів регресійного аналізу дозволяє наочно оцінити якість моделі та характер взаємозв'язків між змінними. Графіки фактичних та прогнозних значень показують високу точність прогнозування моделі, що підтверджується тісним розташуванням точок вздовж лінії ідеального прогнозу.

Аналіз розподілу залишків показує, що вони мають приблизно нормальний розподіл з нульовим середнім значенням, що є важливою передумовою для коректного застосування методу найменших квадратів. QQ-Plot залишків також підтверджує нормальність їх розподілу, оскільки точки розташовуються близько до теоретичної лінії нормального розподілу.

Графіки залишків відносно прогнозних значень не виявляють жодних систематичних патернів, що свідчить про відсутність гетероскедастичності (нерівномірності дисперсії залишків) та підтверджує адекватність моделі.

3D-візуалізація регресійної поверхні дозволяє наочно оцінити характер залежності між змінними та порівняти різні методи побудови моделі.

# Висновки щодо адекватності розробленої моделі

На основі проведеного нелінійного регресійного аналізу та оцінки метричних показників можна зробити ряд важливих висновків щодо адекватності розробленої моделі.

Перш за все, слід відзначити високу точність відновлення коефіцієнтів моделі. Отримані значення коефіцієнтів дуже близькі до вихідних, що свідчить про ефективність методу найменших квадратів для оцінки параметрів нелінійної регресійної моделі. Незначні відхилення пояснюються наявністю випадкового шуму в даних і є цілком очікуваними.

Високе значення коефіцієнта детермінації $R^2 \approx 0.9844$ свідчить про те, що розроблена модель пояснює близько 98.44% дисперсії залежної змінної. Це є дуже високим показником, який вказує на відмінну якість моделі та її здатність точно прогнозувати значення залежної змінної на основі незалежних змінних.

Значення критерію Дарбіна-Уотсона близько 2.10 вказує на відсутність автокореляції залишків, що є важливою передумовою для коректного застосування методу найменших квадратів. Відсутність автокореляції свідчить про те, що модель враховує всі систематичні залежності між змінними, і залишки є випадковими.

Високе значення кореляційного відношення $\eta \approx 0.9922$ свідчить про сильний нелінійний зв'язок між змінними, що підтверджує адекватність вибору нелінійної моделі для опису взаємозв'язків між змінними. Це означає, що включення квадратичних членів у модель є обґрунтованим і суттєво покращує її якість.

Аналіз розподілу залишків показує, що вони мають приблизно нормальний розподіл з нульовим середнім значенням, що є ще однією важливою передумовою для коректного застосування методу найменших квадратів. Відсутність систематичних патернів у графіках залишків відносно прогнозних значень свідчить про відсутність гетероскедастичності.

Порівняння результатів, отриманих за допомогою трьох різних методів (ручного розрахунку, statsmodels.api.OLS та sklearn.linear_model.LinearRegression), показує їх практично повну ідентичність, що підтверджує правильність реалізації цих методів та надійність отриманих результатів.

Візуалізація результатів регресійного аналізу дозволяє наочно оцінити якість моделі та характер взаємозв'язків між змінними. Графіки фактичних та прогнозних значень показують високу точність прогнозування моделі, що підтверджується тісним розташуванням точок вздовж лінії ідеального прогнозу.

Таким чином, на основі всіх проведених оцінок можна зробити висновок, що розроблена нелінійна регресійна модель є адекватною і може бути використана для опису та прогнозування залежності між змінними.

# Можливі шляхи покращення розробленої нелінійної регресійної моделі

Незважаючи на високу якість розробленої нелінійної регресійної моделі, існують певні можливості для її подальшого вдосконалення та покращення. Розглянемо основні напрямки, за якими можна розвивати та покращувати модель.

## Включення взаємодій між змінними

Одним із можливих шляхів покращення моделі є включення членів взаємодії між незалежними змінними. У поточній моделі враховуються лише лінійні та квадратичні ефекти кожної змінної окремо, але не враховуються їх взаємодії.

Включення члена взаємодії $x_1 \cdot x_2$ дозволить врахувати можливий ефект, коли вплив однієї змінної залежить від значення іншої змінної. Модель з урахуванням взаємодій може мати вигляд:

$$y(x) = a_0 + a_1 \cdot x_1 + a_2 \cdot x_2 + a_3 \cdot x_1^2 + a_4 \cdot x_2^2 + a_5 \cdot x_1 \cdot x_2 + rnd(b)$$

Включення такого члена може покращити якість моделі, особливо якщо між змінними існує суттєва взаємодія.

Іншим можливим шляхом покращення моделі є включення поліномів вищих порядків. Поточна модель включає лише члени другого порядку ($x_1^2$ та $x_2^2$), але можна розглянути включення членів третього порядку ($x_1^3$, $x_2^3$) або навіть вищих порядків.

Модель з поліномами третього порядку може мати вигляд:

$$y(x) = a_0 + a_1 \cdot x_1 + a_2 \cdot x_2 + a_3 \cdot x_1^2 + a_4 \cdot x_2^2 + a_5 \cdot x_1^3 + a_6 \cdot x_2^3 + rnd(b)$$

Включення таких членів може дозволити моделі краще описувати складні нелінійні залежності, особливо якщо вони мають характер кубічної або іншої поліноміальної функції вищого порядку.

Важливим напрямком покращення моделі є застосування методів регуляризації, які дозволяють запобігти перенавчанню моделі, особливо при включенні великої кількості предикторів або поліномів високих порядків.

Найпоширенішими методами регуляризації є:

1. **Регуляризація L1 (Lasso)** - додає до функції втрат штраф, пропорційний сумі абсолютних значень коефіцієнтів. Це сприяє отриманню розріджених моделей, де багато коефіцієнтів дорівнюють нулю.

2. **Регуляризація L2 (Ridge)** - додає до функції втрат штраф, пропорційний сумі квадратів коефіцієнтів. Це сприяє отриманню моделей з меншими за абсолютною величиною коефіцієнтами.

3. **Еластична сітка (Elastic Net)** - комбінує регуляризацію L1 та L2, що дозволяє отримати переваги обох підходів.

Застосування цих методів може допомогти уникнути перенавчання моделі та покращити її узагальнюючу здатність, особливо при роботі з реальними даними, де кількість спостережень може бути обмеженою.


Ще одним важливим напрямком покращення моделі є аналіз впливових спостережень та викидів. Наявність аномальних значень у даних може суттєво впливати на оцінки коефіцієнтів моделі та погіршувати її якість.

