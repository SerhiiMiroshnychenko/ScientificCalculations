**Тема:** Лінійний множинний регресійний аналіз даних. Оцінка метричних показників.

**Мета:** Метою заняття є ознайомлення з основами лінійного множинного регресійного аналізу даних та методикою оцінки його метричних показників.

**Постановка завдання:**

За спостережуваними даними побудувати рівняння множинної регресії методом найменших квадратів.

Графічно представити результати регресійного аналізу та спостережувані дані. Виконати інтерпретацію коефіцієнтів моделі, оцінити якість побудованої моделі, використовуючи різні характеристики:

- парні коефіцієнти кореляції двох типів, оцінити їх значущість;
- часткові коефіцієнти кореляції, оцінити їх значущість;
- коефіцієнт множинної кореляції;
- скорегований коефіцієнт детермінації;
- оцінка значущості моделі;
- оцінки значущості коефіцієнтів множинної регресії;
- коефіцієнти еластичності, $\beta$-коефіцієнти, $\Delta$-коефіцієнти.

### РОЗВ'ЯЗАННЯ

Задаємося рівнянням регресії згідно з початковими даними 4 Варіанту

$y(x_1, x_2) = 3.4 + 2.7 \cdot x_1 - 0.5 \cdot x_2 + \text{rnd}(1.10)$

$x_1 := -0.7, -0.69 .. 0.8$

$x_2 := 0.5, 0.51 .. 2$

Визначаємо для 151 елемента спостереження

$n := 150$

$i := 0 .. n$

$X_{1_i} := -0.7 + i \cdot 0.01$

$X_{2_i} := 0.5 + i \cdot 0.01$

$Y_i := 3.4 + 2.7 \cdot X_{1_i} - 0.5 \cdot X_{2_i} + \text{rnd}(1.10)$

Розpаховуємо значення коефіцієнтів $b_0$, $b_1$, $b_2$ у лінійному поліномі

$\frac{d}{db_0} \sum_{i=0}^{n} (Y_i - b_0 - b_1 \cdot X_{1_i} - b_2 \cdot X_{2_i})^2 = 0$

$\frac{d}{db_1} \sum_{i=0}^{n} (Y_i - b_0 - b_1 \cdot X_{1_i} - b_2 \cdot X_{2_i})^2 = 0$

$\frac{d}{db_2} \sum_{i=0}^{n} (Y_i - b_0 - b_1 \cdot X_{1_i} - b_2 \cdot X_{2_i})^2 = 0$

$2 \cdot b_0 \cdot (n+1) + 2 \cdot b_1 \cdot \sum_{i=0}^{n} X_{1_i} + 2 \cdot b_2 \cdot \sum_{i=0}^{n} X_{2_i} - 2 \cdot \sum_{i=0}^{n} Y_i = 0$

$2 \cdot b_0 \cdot \sum_{i=0}^{n} X_{1_i} - 2 \cdot \sum_{i=0}^{n} (X_{1_i} \cdot Y_i) + 2 \cdot b_1 \cdot \sum_{i=0}^{n} (X_{1_i})^2 + 2 \cdot b_2 \cdot \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) = 0$

$2 \cdot b_0 \cdot \sum_{i=0}^{n} X_{2_i} - 2 \cdot \sum_{i=0}^{n} (X_{2_i} \cdot Y_i) + 2 \cdot b_2 \cdot \sum_{i=0}^{n} (X_{2_i})^2 + 2 \cdot b_1 \cdot \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) = 0$

Система рівнянь для розрахунку значень коефіцієнтів $b_0, b_1, b_2$

$b_0 \cdot (n+1) + b_1 \cdot \sum_{i=0}^{n} X_{1_i} + b_2 \cdot \sum_{i=0}^{n} X_{2_i} = \sum_{i=0}^{n} Y_i$

$b_0 \cdot \sum_{i=0}^{n} X_{1_i} + b_1 \cdot \sum_{i=0}^{n} (X_{1_i})^2 + b_2 \cdot \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) = \sum_{i=0}^{n} (X_{1_i} \cdot Y_i)$

$b_0 \cdot \sum_{i=0}^{n} X_{2_i} + b_2 \cdot \sum_{i=0}^{n} (X_{2_i})^2 + b_1 \cdot \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) = \sum_{i=0}^{n} (X_{2_i} \cdot Y_i)$

Для розв'язання системи лінійних рівнянь застосовується метод Крамера:

$\Delta := \begin{pmatrix}
n+1 & \sum_{i=0}^{n} X_{1_i} & \sum_{i=0}^{n} X_{2_i} \\
\sum_{i=0}^{n} X_{1_i} & \sum_{i=0}^{n} (X_{1_i})^2 & \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) \\
\sum_{i=0}^{n} X_{2_i} & \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) & \sum_{i=0}^{n} (X_{2_i})^2
\end{pmatrix} \quad \det(\Delta) = 1.727 \cdot 10^{-10}$

$\Delta_0 := \begin{pmatrix}
\sum_{i=0}^{n} Y_i & \sum_{i=0}^{n} X_{1_i} & \sum_{i=0}^{n} X_{2_i} \\
\sum_{i=0}^{n} (X_{1_i} \cdot Y_i) & \sum_{i=0}^{n} (X_{1_i})^2 & \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) \\
\sum_{i=0}^{n} (X_{2_i} \cdot Y_i) & \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) & \sum_{i=0}^{n} (X_{2_i})^2
\end{pmatrix} \quad \det(\Delta_0) = 1.906 \cdot 10^{-9} \quad b_0 := \frac{\det(\Delta_0)}{\det(\Delta)} = 11.034549116996$

$\Delta_1 := \begin{pmatrix}
n+1 & \sum_{i=0}^{n} Y_i & \sum_{i=0}^{n} X_{2_i} \\
\sum_{i=0}^{n} X_{1_i} & \sum_{i=0}^{n} (X_{1_i} \cdot Y_i) & \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) \\
\sum_{i=0}^{n} X_{2_i} & \sum_{i=0}^{n} (X_{2_i} \cdot Y_i) & \sum_{i=0}^{n} (X_{2_i})^2
\end{pmatrix} \quad \det(\Delta_1) = 1.727 \cdot 10^{-9} \quad b_1 := \frac{\det(\Delta_1)}{\det(\Delta)} = 10$

$\Delta_2 := \begin{pmatrix}
n+1 & \sum_{i=0}^{n} X_{1_i} & \sum_{i=0}^{n} Y_i \\
\sum_{i=0}^{n} X_{1_i} & \sum_{i=0}^{n} (X_{1_i})^2 & \sum_{i=0}^{n} (X_{1_i} \cdot Y_i) \\
\sum_{i=0}^{n} X_{2_i} & \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) & \sum_{i=0}^{n} (X_{2_i} \cdot Y_i)
\end{pmatrix} \quad \det(\Delta_2) = -1.431 \cdot 10^{-9} \quad b_2 := \frac{\det(\Delta_2)}{\det(\Delta)} = -8.28571428571429$

Таким чином рівняння регресії приймає наступний вигляд:

$y_i(x_1, x_2) := b_0 + b_1 \cdot x_1 + b_2 \cdot x_2$

$b = \begin{pmatrix}
11.034549116996 \\
10 \\
-8.28571428571429
\end{pmatrix}$

Будуємо графічні залежності та корегуємо коефіцієнти (за необхідності)

$Y_i := b_0 + (b_1 + 0.45) \cdot X_{1_i} + b_2 \cdot X_{2_i} + 2.3 \qquad \sum_{i=0}^{n} (Y_i - Y_{1_i})^2 = 14.955$

Парні коефіцієнти кореляції, що характеризують тісноту зв'язку між двома величинами знайдемо в наступних виразах:

$r_{yx1} := \frac{\frac{1}{n+1} \cdot \sum_{i=0}^{n} (X_{1_i} \cdot Y_i) - (\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{1_i}) \cdot (\frac{1}{n+1} \cdot \sum_{i=0}^{n} Y_i)}{\sqrt{(\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{1_i}^2) - (\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{1_i})^2} \cdot \sqrt{(\frac{1}{n+1} \cdot \sum_{i=0}^{n} Y_i^2) - (\frac{1}{n+1} \cdot \sum_{i=0}^{n} Y_i)^2}} = 0.999999999999997$

$r_{yx2} := \frac{\frac{1}{n+1} \cdot \sum_{i=0}^{n} (X_{2_i} \cdot Y_i) - (\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{2_i}) \cdot (\frac{1}{n+1} \cdot \sum_{i=0}^{n} Y_i)}{\sqrt{(\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{2_i}^2) - (\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{2_i})^2} \cdot \sqrt{(\frac{1}{n+1} \cdot \sum_{i=0}^{n} Y_i^2) - (\frac{1}{n+1} \cdot \sum_{i=0}^{n} Y_i)^2}} = 0.999999999999996$

$r_{x1x2} := \frac{\frac{1}{n+1} \cdot \sum_{i=0}^{n} (X_{1_i} \cdot X_{2_i}) - (\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{1_i}) \cdot (\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{2_i})}{\sqrt{(\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{1_i}^2) - (\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{1_i})^2} \cdot \sqrt{(\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{2_i}^2) - (\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{2_i})^2}} = 0.999999999999999$

Значущість парних коефіцієнтів кореляції можна перевірити із застосуванням t-критерія Стьюдента.
Величина середньоквадратичної похибки парного коефіцієнта кореляції:

$S_{ryx1} := \sqrt{\frac{1 - r_{yx1}^2}{n+1-2}} = 6.46 \cdot 10^{-9}$

$S_{ryx2} := \sqrt{\frac{1 - r_{yx2}^2}{n+1-2}} = 7.525 \cdot 10^{-9}$

$S_{rx1x2} := \sqrt{\frac{1 - r_{x1x2}^2}{n+1-2}} = 3.453 \cdot 10^{-9}$

$t_{кр} := 1.96 \qquad \alpha := 0.05 \qquad P := 1 - \alpha = 0.95 \qquad n = \infty$

$t_{yx1} := \frac{r_{yx1}}{S_{ryx1}} = 1.548 \cdot 10^8 \qquad t_{yx1} > t_{кр} = 1$

$t_{yx2} := \frac{r_{yx2}}{S_{ryx2}} = 1.329 \cdot 10^8 \qquad t_{yx2} > t_{кр} = 1$

$t_{x1x2} := \frac{r_{x1x2}}{S_{rx1x2}} = 2.896 \cdot 10^8 \qquad t_{yx1} > t_{кр} = 1$

Під час цього між функцією відгуку $y$ та факторами $x_1$ та $x_2$ має місце статистичний зв'язок.

Якщо один з коефіцієнтів $r_{x1x2} = 1$, то фактори $x_1$ та $x_2$ не статистично, а функціонально пов'язані між собою. Отже, доцільно один з цих факторів виключити з розгляду. Рекомендовано залишити той фактор, у якого величина коефіцієнта $r_{yj}$ є більшою.

Після визначення величин усіх парних коефіцієнтів кореляції, а також виключення того чи іншого фактора, визначається матриця коефіцієнтів кореляції:

$r := \begin{pmatrix}
1 & r_{yx1} & r_{yx2} \\
r_{yx1} & 1 & r_{x1x2} \\
r_{yx2} & r_{x1x2} & 1
\end{pmatrix} \qquad r = \begin{pmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1
\end{pmatrix}$

та оберненої матриці до кореляційної:

$D := r^{-1}$

$D = \begin{pmatrix}
1.616 \cdot 10^{14} & -1.818 \cdot 10^{14} & 2.02 \cdot 10^{13} \\
-1.818 \cdot 10^{14} & 7.674 \cdot 10^{14} & -5.857 \cdot 10^{14} \\
2.02 \cdot 10^{13} & -5.857 \cdot 10^{14} & 5.655 \cdot 10^{14}
\end{pmatrix}$

Значення часткових коефіцієнтів кореляції:

$D_{00} := (-1)^{0+0} \cdot \begin{vmatrix} D_{1,1} & D_{1,2} \\ D_{2,1} & D_{2,2} \end{vmatrix} \qquad |D_{00}| = 7.651 \cdot 10^{29}$

$D_{11} := (-1)^{1+1} \cdot \begin{vmatrix} D_{0,0} & D_{0,2} \\ D_{2,0} & D_{2,2} \end{vmatrix} \qquad |D_{11}| = 9.095 \cdot 10^{28}$

$D_{22} := (-1)^{2+2} \cdot \begin{vmatrix} D_{0,0} & D_{0,1} \\ D_{1,0} & D_{1,1} \end{vmatrix} \qquad |D_{22}| = 9.095 \cdot 10^{28}$

$D_{01} := (-1)^{0+1} \cdot \begin{vmatrix} D_{1,0} & D_{1,2} \\ D_{2,0} & D_{2,2} \end{vmatrix} \qquad |D_{01}| = -9.095 \cdot 10^{28}$

$D_{02} := (-1)^{0+2} \cdot \begin{vmatrix} D_{1,0} & D_{1,1} \\ D_{2,0} & D_{2,1} \end{vmatrix} \qquad |D_{02}| = 9.095 \cdot 10^{28}$

$r_{yx1 \cdot x2} := - \frac{|D_{01}|}{\sqrt{|D_{00}| \cdot |D_{11}|}} = - \frac{-9.095 \cdot 10^{28}}{\sqrt{7.651 \cdot 10^{29} \cdot 9.095 \cdot 10^{28}}} = -0.345$

$r_{yx2 \cdot x1} := - \frac{|D_{02}|}{\sqrt{|D_{00}| \cdot |D_{22}|}} = - \frac{9.095 \cdot 10^{28}}{\sqrt{7.651 \cdot 10^{29} \cdot 9.095 \cdot 10^{28}}} = -0.345$

Значущість та довірчий інтервал для коефіцієнта часткової кореляції можна визначити аналогічно, як для коефіцієнтів парної кореляції.

Для оцінки тісноти зв'язку між функцією відгуку $y$ та декількома факторами $x_1, x_2$ застосовується коефіцієнт множинної кореляції $R$:

$R := \sqrt{1 - \frac{|D|}{|D_{00}|}} = 0.948$

Величина середньоквадратичної похибки коефіцієнта множинної кореляції:

$m := 2 \qquad S_R := \sqrt{\frac{1 - R^2}{n+1-m-1}} = 0.026$

Значущість коефіцієнта множинної кореляції перевіряється із застосуванням t-критерія Стьюдента:

$t_R := \frac{R}{S_R} = 36.201$

Скорегований (adjusted) коефіцієнт детермінації можна оцінити за таким співвідношенням:

$R_{ скор}^2 := 1 - (1 - R^2) \cdot \frac{n+1-1}{n+1-m-1} = 0.897$

Знайдемо значення стандартної похибки регресії:

$S := \sqrt{\frac{1}{n+1-m-1} \cdot \sum_{i=0}^{n} (Y_i - Y_{1_i})^2} = 0.317$

Оцінка значущості розробленої моделі, тобто наскільки вірною є гіпотеза щодо лінійності регресії між $y$ і $x_j$, здійснюється із застосуванням F-критерія Фішера. За цим спостереженням можна визначити:

$S_{cn} := \frac{R^2 \cdot (n+1-m-1)}{(1 - R^2) \cdot m} = 645.528$

Якщо $F_{cn} > F_{кр}(\alpha; m; n-m-1)$, то вважається, що розроблена модель є значущою, а лінійний зв'язок між показниками є статистично значущим.

Розрахунок критичного значення F-критерія Фішера $F_{кр}$

```python

from scipy.stats import f

alpha = 0.05  # Рівень значущості
n = 150
dfn = m = 2   # Ступені вільності чисельника (кількість факторів)
dfd = n - m - 1 # Ступені вільності знаменника (n - m - 1)

f_critical = f.ppf(1 - alpha, dfn, dfd)
print(f"Критичне значення F-розподілу (F_кр): {f_critical:.3f}")
```
Критичне значення F-розподілу ($F_{кр}$): 3.058

Позаяк 645.528 > 3.058, то зв'язок статистично значущий

Оцінка значущості коефіцієнта множинної регресії (без обліку постійної складової моделі) здійснюється із застосуванням t-критерія Стьюдента:

$t_1 := \frac{b_1}{S \cdot \sqrt{(X_1^T \cdot X_1)^{-1}}} = 170.243$

$t_2 := \frac{b_2}{S \cdot \sqrt{(X_2^T \cdot X_2)^{-1}}} = -425.611$

Якщо $|t_j| > t_{табл}$, то j-й коефіцієнт вважається значущим, інакше фактор, який відповідає за цей коефіцієнт необхідно виключити з моделі:

$t_{кр} := 1.96 \qquad \alpha := 0.05 \qquad P := 1 - \alpha = 0.95 \qquad n = \infty$

$t_1 > t_{кр} = 1 \qquad$ значення $b_1$ є значущим

$t_2 > t_{кр} = 0 \qquad$ значення $b_2$ є значущим

Вплив факторів $x_j$ на $y$ можна оцінити на базі коефіцієнтів еластичності $\varepsilon_j$ та $\beta$-коефіцієнтів:

$\varepsilon_1 := b_1 \cdot \frac{\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{1_i}}{\frac{1}{n+1} \cdot \sum_{i=0}^{n} Y_i} = 0.143$

$\varepsilon_2 := b_2 \cdot \frac{\frac{1}{n+1} \cdot \sum_{i=0}^{n} X_{2_i}}{\frac{1}{n+1} \cdot \sum_{i=0}^{n} Y_i} = -2.959$

$S_1 := \sqrt{\frac{1}{n+1-2} \cdot (\sum_{i=0}^{n} X_{1_i}^2 - \frac{1}{n+1} \cdot (\sum_{i=0}^{n} X_{1_i})^2)} = 0.439$

$S_2 := \sqrt{\frac{1}{n+1-2} \cdot (\sum_{i=0}^{n} X_{2_i}^2 - \frac{1}{n+1} \cdot (\sum_{i=0}^{n} X_{2_i})^2)} = 0.439$

$\beta_1 := b_1 \cdot \frac{S_1}{S} = 13.856$

$\beta_2 := b_2 \cdot \frac{S_2}{S} = -11.481$

Значення коефіцієнтів еластичності $\varepsilon_j$ показує, на скільки відсотків зміниться значення $y$, якщо $x_j$ змінилось на 1%.

Коефіцієнти $\beta_j$ характеризують, на яку частку середньоквадратичного відхилення може бути змінено $y$, якщо $x_j$ змінилось на величину свого середньоквадратичного відхилення.

Частку впливу $j$-го фактора в сумарному впливі всіх факторів на показник $y$ можна оцінити за $\Delta$-коефіцієнтами:

$\Delta_1 := r_{yx1} \cdot \frac{\beta_1}{R^2} = 15.444$

$\Delta_2 := r_{yx2} \cdot \frac{\beta_2}{R^2} = -12.797$

### Потенційні проблеми та зауваження
Мультиколінеарність становить серйозну проблему, оскільки значення коефіцієнта кореляції між x₁ та x₂ близьке до 1, що вказує на сильну лінійну залежність між факторами. Це може негативно впливати на якість та стабільність моделі. Варто також звернути увагу на неприродно високі коефіцієнти кореляції – усі парні коефіцієнти кореляції практично дорівнюють 1, що є рідкісним явищем у реальних даних і може вказувати на особливості генерації даних або обчислювальні помилки. Крім того, існують суперечливі висновки: попри високі коефіцієнти парної кореляції, часткові коефіцієнти кореляції значно нижчі (-0.345), що свідчить про сильний вплив мультиколінеарності. Також викликає занепокоєння значення Δ-коефіцієнтів, адже їхня сума не дорівнює 1 (або 100%), що ускладнює інтерпретацію частки впливу кожного фактора.
## Висновки
Побудована регресійна модель є статистично значущою (за F-критерієм Фішера) і має високий коефіцієнт детермінації (R²скор = 0.897). Обидва фактори (x₁ та x₂) мають статистично значущий вплив на залежну змінну y. Фактор x₂ має більший вплив на залежну змінну за абсолютною величиною коефіцієнта еластичності (|-2.959| > |0.143|). Наявність сильної мультиколінеарності (r(x₁,x₂) ≈ 1.0) може призводити до нестабільності оцінок коефіцієнтів регресії та ускладнює інтерпретацію результатів. Для покращення моделі може бути рекомендовано виключити один із сильно корельованих факторів, застосувати методи регуляризації (Ridge, Lasso) або використати метод головних компонент для зменшення мультиколінеарності.

