{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Оцінка методів визначення важливості ознак\n",
    "\n",
    "**1. p-значення t-тесту Велча (Welch’s t-test)**\n",
    "- **Доцільність:** Підходить для числових ознак у двокласовій задачі (з двома групами успішних/неуспішних замовлень). Перевіряє відмінність середніх двох вибірок без припущення рівності дисперсій. Дає *t*-статистику і *p*-значення для кожної ознаки. При ранжуванні ознак за значенням |*t*| (або за зменшенням *p*) ті, що мають більшу різницю середніх (при одній зі стандартних похибок), будуть вважатися сильнішими.\n",
    "- **Коментар:** Це одновимірний фільтровий метод. Він добре працює за умов приблизної нормальності даних; при великих обсягах вибірки (*n*~32031 проти 54763) будь-яка слабка різниця стане значущою, тож фокус на *змінному ефекті*. t-тест не годиться без змін для категоріальних ознак; для них потрібне кодування або інший тест. Враховуючи наші великі обсяги, *p*-значення можуть бути дуже малими навіть для незначних ефектів, тому будівництво рейтингу слід робити через саму величину *t* або ефект-розміру. Welch-версія робить метод стійким до нерівності дисперсій між класами.\n",
    "\n",
    "**2. p-значення Манна–Уітні (Mann–Whitney U-test)**\n",
    "- **Доцільність:** Також для числових ознак (безпараметричний метод). Оцінює, чи розподіли значень ознаки значуще відрізняються між двома класами, використовуючи рангові суми. Дає *p*-значення для кожної ознаки; ранжування можна виконати за статистикою U або відповідним *z*-або *p*-значенням. Цей метод не потребує нормальності і стійкіший до викидів порівняно з t-тестом.\n",
    "- **Коментар:** За великих вибірок результати Манна–Уітні дуже близькі до t-тесту. У згаданому дослідженні Wilcoxon-Rank Sum (аналог Манна–Уітні) знайшов подібні маркери, що й t-тест, проте з меншою чутливістю до викидів. У практиці ранжування ознак маєте отримати список з ознак за зростанням *p*-значення або спаданням статистики U. Приклад використання такого тесту для відбору релевантних ознак наведено в медаболомічному пакеті R ([Modelling and feature selection • metabolyseR](https://jasenfinch.github.io/metabolyseR/articles/modelling.html#:~:text=Welch%E2%80%99s%20t,response%20variable%20will%20be%20tested)) (зв’язок ознак з двома категоріями). Оскільки дані великі, навіть невеликі відмінності в медіанах дадуть низькі *p*, але це компенсується тим, що метод і не вимагає суворих припущень.\n",
    "\n",
    "**3. d Кохена (Cohen’s d)**\n",
    "- **Доцільність:** Міра ефект-розміру для числових ознак у двокласовому контексті. Обчислюється як різниця середніх між класами, нормована на об’єднане стандартне відхилення. Daє уявлення про *величину* розбіжності між класами незалежно від розміру вибірки. Можна ранжувати ознаки за абсолютним значенням d (чим більше, тим сильніший ефект).\n",
    "- **Коментар:** На відміну від *p*-значень, d Кохена показує реальну силу відмінності ([\n",
    "          7 Proven Metrics to Evaluate Effect Size in Research\n",
    "    ](https://www.numberanalytics.com/blog/7-proven-metrics-evaluate-effect-size-research#:~:text=Cohen%E2%80%99s%20d%20is%20one%20of,clear%20interpretability%20across%20research%20fields)). Наприклад, d=0.2 вважається малим ефектом, 0.5 – помірним, 0.8 – великим ([\n",
    "          7 Proven Metrics to Evaluate Effect Size in Research\n",
    "    ](https://www.numberanalytics.com/blog/7-proven-metrics-evaluate-effect-size-research#:~:text=Cohen%E2%80%99s%20d%20is%20one%20of,clear%20interpretability%20across%20research%20fields)). У наших даних з великим *n* багато різниць будуть статистично значущими, тому саме величина d допоможе зрозуміти, які ознаки найсильніше розділяють класи. Однак для дуже нерівномірних розподілів або коли є сильні відхилення, d може бути нечутливим до форми розподілу. Він також вимагає числових ознак і приблизно нормальних розподілів, але робить акцент на практичній значущості розбіжності ([\n",
    "          7 Proven Metrics to Evaluate Effect Size in Research\n",
    "    ](https://www.numberanalytics.com/blog/7-proven-metrics-evaluate-effect-size-research#:~:text=Cohen%E2%80%99s%20d%20is%20one%20of,clear%20interpretability%20across%20research%20fields)).\n",
    "\n",
    "**4. AUC (Area Under ROC Curve)**\n",
    "- **Доцільність:** Показник здатності однієї ознаки розділяти два класи. Береться значення ознаки як “оцінка” і обчислюється площа під ROC-кривою при цьому єдиному предикторі. Дає від 0.5 (відсутність дискримінації) до 1 (ідеальна). Для числових ознак це поширений підхід: він оцінює ймовірність того, що випадкове позитивне (успішне) замовлення матиме більше значення ознаки, ніж випадкове негативне.\n",
    "- **Коментар:** AUC еквівалентне статистиці Манна–Уітні: насправді обчислення AUC реалізується через еквівалентність з U-тестом ([An AUC-based permutation variable importance measure for random forests | BMC Bioinformatics | Full Text](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-119#:~:text=observation%20from%20class%20Y%20%3D,observation%20from%20the%20other%20class)). Тож у ранжуванні ознак великі значення AUC (наближаються до 1) відповідають сильним розбіжностям між класами. AUC стійка до дисбалансу класів (важить обидва класи однаково) і не потребує припущень про розподіл. Однак для категоріальних ознак AUC не визначена без попереднього кодування або бінінгу. На практиці **AUC** для одновимірної ознаки часто використовують як альтернативу U-тесту, оскільки вони генерують однаковий порядок ознак ([An AUC-based permutation variable importance measure for random forests | BMC Bioinformatics | Full Text](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-119#:~:text=observation%20from%20class%20Y%20%3D,observation%20from%20the%20other%20class)).\n",
    "\n",
    "**5. Інформаційна цінність (Information Value, IV)**\n",
    "- **Доцільність:** Класичний інструмент з кредитного скорингу для бінарної класифікації. Підтримує як категоріальні, так і числові ознаки (останні потрібно попередньо розбити на бінни). Визначає міру роздільної здатності ознаки через оцінку “спостережуваної проти очікуваної” частоти позитивних/негативних випадків у кожному біні. Вихід – єдине число IV, за яким можна ранжувати ознаки: чим вище IV, тим сильніший розподіл класів.\n",
    "- **Коментар:** IV базується на підході Weight of Evidence (WOE). Зазвичай прийняті правила: IV<0.02 – дуже слабкий ознака, 0.02–0.1 – слабкий, 0.1–0.3 – помірний, >0.3 – сильний. Як фільтровий метод він розраховує значення кожного предиктора та дозволяє відсортувати ознаки за їх роздільною здатністю ([Weight of Evidence (WOE) and Information Value (IV) Explained](https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html#:~:text=Information%20value%20,using%20the%20following%20formula)). IV зручно інтерпретувати для нефахівців (є єдиний показник). Проте він чутливий до якості бінування: занадто багато або невиправдані біни можуть спотворити IV. З іншого боку, він добре справляється з категорійними ознаками (використовує їх без додаткових кодувань) ([Weight of Evidence (WOE) and Information Value (IV) Explained](https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html#:~:text=Information%20value%20,using%20the%20following%20formula)). Загалом IV є популярним «швидким фільтром» для сортування ознак за прогнозною силою.\n",
    "\n",
    "**6. Взаємна інформація (Mutual Information)**\n",
    "- **Доцільність:** Міра залежності між ознакою та бінарною мішенню будь-якого типу (числового чи категоріального). Визначається через ентропію і не лінійна: MI(X;Y) ≥ 0 і =0 лише якщо ознаки незалежні. Високе MI означає, що знання значення ознаки дає багато інформації про клас. Для числових ознак зазвичай використовується оцінка MI через k–NN або дискретизацію.\n",
    "- **Коментар:** Mutual Information є потужним фільтром для будь-яких залежностей – як лінійних, так і нетипових. За відсутності зв’язку MI ≈ 0, а більші значення (у бітах або ніт) означають міцнішу залежність ([mutual_info_classif — scikit-learn 1.6.1 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#:~:text=Mutual%20information%20%28MI%29%20,higher%20values%20mean%20higher%20dependency)). У sklearn є функція `mutual_info_classif`, яка оцінює MI ознаки з бінарною мішенню. Зі свого боку Mutual Information дозволяє ранжувати ознаки за спаданням величини MI; це дає змогу відібрати топ-N ознак як фільтром. Особливість MI – воно нечутливе до монотонності чи нормальності, але при цьому складніше інтерпретувати числово (на відміну від AUC). Також MI вимагає достатньо даних для надійної оцінки ентропій (особливо з числовими ознаками) ([mutual_info_classif — scikit-learn 1.6.1 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#:~:text=Mutual%20information%20%28MI%29%20,higher%20values%20mean%20higher%20dependency)).\n",
    "\n",
    "**7. F-статистика ANOVA**\n",
    "- **Доцільність:** Підходить для числових ознак та категоріальної мішені (тільки дві групи у нашому випадку). Аналіз дисперсій порівнює середню ознаки між класами. Для двокласової задачі ANOVA F = (задан міжгрупова дисперсія / всередині груп) фактично еквівалентна t-тесту. Результатом є *F*-значення і *p*F, за якими можна сортувати ознаки.\n",
    "- **Коментар:** Як і t-тест, F-тест є параметричним і вимагає приблизної нормальності, але його легко застосувати через бібліотеки (наприклад, `f_classif` у sklearn). Він підходить коли є більше двох класів (тут їх два – ANOVA переходить у двогруповий порівняльний аналіз). Методи відбору ознак часто використовують SelectKBest із `f_classif`. Приклад: sklearn-функція `f_classif()` повертає F-статистику для кожної ознаки, що можна використовувати в `SelectKBest` ([How to Perform Feature Selection With Numerical Input Data - MachineLearningMastery.com](https://machinelearningmastery.com/feature-selection-with-numerical-input-data/#:~:text=Importantly%2C%20ANOVA%20is%20used%20when,variable%20in%20a%20classification%20task)). Ознаки з низькими F-значеннями можна відкидати, оскільки вони слабо розрізняють класи ([How to Perform Feature Selection With Numerical Input Data - MachineLearningMastery.com](https://machinelearningmastery.com/feature-selection-with-numerical-input-data/#:~:text=Importantly%2C%20ANOVA%20is%20used%20when,variable%20in%20a%20classification%20task)). Через те, що два класи, цей тест дасть аналогічний результат до t-тесту (або Welch), але зручно уніфікований інтерфейсом.\n",
    "\n",
    "**8. Кореляція Спірмена**\n",
    "- **Доцільність:** Невичерпний непараметричний показник (ранговий кореляційний коефіцієнт) для числових/рангових ознак. Оцінює монотонний (не обов’язково лінійний) зв’язок між ранговими перетвореннями ознаки та цілевою змінною.\n",
    "- **Коментар:** Spearman не вимагає нормальності та показує кореляцію рангових порядків. Однак його застосування до бінарної мішені проблематичне: Spearman (як і Pearson) формально потребує порядковості обох змінних. За наших даних (бінарна мішень) кореляція Спірмена не дуже інформативна і навіть не рекомендована ([feature selection - If my target variable is binary, is it better to use Pearson's or Spearman's for my correlation vector? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/121107/if-my-target-variable-is-binary-is-it-better-to-use-pearsons-or-spearmans-for#:~:text=Pearson%27s%20correlation%20coefficient%20measures%20the,either%20of%20these%20correlation%20coefficients)). Якщо ж ознака непараметрична, Spearman може показати яскравіший зв’язок, ніж Pearson (особливо за нелінійної монотонності). Коротко: цей коефіцієнт вказує на силу монотонного зв’язку, але для бінарної мішені його замінюють точнішими мірами (див. Pearson нижче) ([feature selection - If my target variable is binary, is it better to use Pearson's or Spearman's for my correlation vector? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/121107/if-my-target-variable-is-binary-is-it-better-to-use-pearsons-or-spearmans-for#:~:text=Pearson%27s%20correlation%20coefficient%20measures%20the,either%20of%20these%20correlation%20coefficients)). Якщо застосовується, ознаки ранжуються за абсолютним значенням *ρ* Спірмена.\n",
    "\n",
    "**9. Коефіцієнт логістичної регресії**\n",
    "- **Доцільність:** Вбудований підхід **Embedded**: спершу навчаємо логістичну регресію на всіх ознаках, а потім аналізуємо значення коефіцієнтів. Працює з числовими ознаками або закодованими категоріальними. Коефіцієнт показує вплив ознаки на лог-лійкові шанси класу \"успішне\".\n",
    "- **Коментар:** Для порівняння важливості ознак треба стандартизувати фічі (щоб вони були в одному масштабі). Тоді величина абсолютного коефіцієнта безпосередньо відображає вклад ознаки ([How To Get Feature Importance In Logistic Regression | Forecastegy](https://forecastegy.com/posts/feature-importance-in-logistic-regression/#:~:text=When%20the%20features%20are%20standardized%2C,the%20importance%20of%20each%20feature)). Наприклад, після стандартизації `coef_` моделі LR можна брати як міру важливості ([How To Get Feature Importance In Logistic Regression | Forecastegy](https://forecastegy.com/posts/feature-importance-in-logistic-regression/#:~:text=When%20the%20features%20are%20standardized%2C,the%20importance%20of%20each%20feature)). Плюс методу – пов’язує ознаки з ймовірністю успіху; мінус – він лінійний і чутливий до мультиколінеарності. За наявності сильно корельованих ознак коефіцієнти можуть знижуватися (розмазувати вплив). Коефіцієнт логістичної регресії дає не *p*-значення, але його величина і знак підказують, як змінюється ризик за одиничного зміщення ознаки. Важливо: знову ж таки, для ранжування беремо величини |coef| після перетворення ознак.\n",
    "\n",
    "**10. Decision Tree (важливість ознаки за деревом рішень)**\n",
    "- **Доцільність:** Модельне (Tree-based) визначення важливості. Оцінює, наскільки кожна ознака зменшує неоднорідність (недосконалість Gini/энтропії) при поділі в дереві. Підходить для числових і категоріальних ознак (можна дерев’ями кодувати без змін або з одним-hot-encoding).\n",
    "- **Коментар:** У реалізації, наприклад, `DecisionTreeClassifier` у sklearn атрибут `feature_importances_` дає відносну важливість ознак за **середнім спадом нечистоти** (mean decrease impurity) при сплітах. Метод автоматично враховує нелінійні взаємодії і не вимагає припущень про розподіл. Проте він має недолік: імпуріті-важливості можуть бути **упередженими** – переважно до ознак з великою кількістю унікальних значень ([Feature importances with a forest of trees — scikit-learn 1.6.1 documentation](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#:~:text=Feature%20importances%20are%20provided%20by,impurity%20decrease%20within%20each%20tree)). Тобто ознака з багатьма бінрами чи категоріями «потенційно» ділить більше, що збільшує її важливість. Тому результати дерев’яних моделей слід інтерпретувати обережно (особливо окреме дерево змінить рейтинг при випадковому зміні даних). Такий метод зручний, коли потрібні нелінійні розрізнення, але він може давати відмінний від кореляційної картини (особливо якщо дані мають складні залежності).\n",
    "\n",
    "**11. Random Forest (важливість ознаки за випадковим лісом)**\n",
    "- **Доцільність:** Як і рішення дерева, RF (ансамбль з багатьох дерев) надає `feature_importances_` – усереднення спадів нечистоти по всіх деревах. Підтримує всі типи ознак (розрізняє випадково обрані підмножини для навчання кожного дерева).\n",
    "- **Коментар:** Random Forest суттєво більш стабільний, ніж одне дерево, завдяки усередненню. Важливість ознак обчислюється як середнє зменшення критерія розділення (Gini чи ентропії) при поділі на цій ознаці, усереднене по всіх деревах ([Feature importances with a forest of trees — scikit-learn 1.6.1 documentation](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#:~:text=Feature%20importances%20are%20provided%20by,impurity%20decrease%20within%20each%20tree)). Проте базова проблема упередженості залишається ([Feature importances with a forest of trees — scikit-learn 1.6.1 documentation](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#:~:text=Warning)): особливо висококардинальні ознаки можуть виглядати важливішими. Крім того, у RF можна використовувати permutation-важливість (via OOB-перестановок) для уникнення цього зміщення, але за замовчуванням часто беруть імпуріті-важливість. Загалом, **RF** – надзвичайно популярний метод: він фіксує складні нелінії і взаємодії, дає легкий атрибут важливості і демонструє відмінні прогностичні результати, тож є доцільним для ранжування ознак, хоча поступається моделей Shapley/MSHAP у глибшій інтерпретованості.\n",
    "\n",
    "**12. XGBoost (важливість ознаки за XGBoost-моделлю)**\n",
    "- **Доцільність:** Ще один деревний ансамбль, але з градієнтним бустингом. Після тренування моделі XGBoost також може віддати оцінки важливості кожної ознаки. Працює з числовими і закодованими категоріальними ознаками.\n",
    "- **Коментар:** Вбудована функція XGBoost збирає «важливість» ознаки за тим, наскільки кожен її спліт покращує цільову функцію (звичайно «gain» – зменшення лог-лійкової помилки) ([Calculating XGBoost Feature Importance | by Emily K Marsh | Medium](https://medium.com/@emilykmarsh/xgboost-feature-importance-233ee27c33a4#:~:text=A%20benefit%20to%20using%20a,decision%20trees%20within%20the%20model)). На практиці розглядають кілька метрик: weight (частота використання), gain (середнє покращення) та cover (середня кількість об’єктів). Зазвичай рекомендують **gain** як основну міру важливості. Ідея схожа на DT/RF: відбираються splits, які дають найбільший приріст точності. Приклади наводять: XGBoost підсумовує вплив кожного розбиття на ознаці і усереднює по деревах ([Calculating XGBoost Feature Importance | by Emily K Marsh | Medium](https://medium.com/@emilykmarsh/xgboost-feature-importance-233ee27c33a4#:~:text=A%20benefit%20to%20using%20a,decision%20trees%20within%20the%20model)). Плюс методу – він враховує слабші взаємодії послідовно, що здатне виділити навіть неочевидні ознаки. Мінус – для створення рангу потрібно стежити за багатьма деревами і налаштуваннями (learning rate, тощо). Але загалом XGBoost важливість схожа на RF, просто з додатковим бустинг-ефектом: ознаки з великим середнім gain–внеском отримають вищий ранг.\n",
    "\n",
    "**13. Хі-квадрат тест**\n",
    "- **Доцільність:** Класичний непараметричний тест для категоріальних ознак (і бінарної мішені). Перевіряє незалежність двох категоріальних змінних (мішень, ознака) за таблицею спостережуваних частот. Дає статистику χ² та *p*-значення. Для кожної категоріальної ознаки можна обчислити χ² проти класу.\n",
    "- **Коментар:** Цей тест широко використовується як фільтр: *p*-значення χ² свідчать про ступінь зв’язку з мішенню. GeeksforGeeks згадує: “Chi-Square test helps determine, чи є значущий зв’язок між двома категоріальними змінними і цільовою” ([Chi-square test in Machine Learning | GeeksforGeeks](https://www.geeksforgeeks.org/ml-chi-square-test-for-feature-selection/#:~:text=Chi,it%20doesn%E2%80%99t%20follow%20normal%20distribution)). Тож ранжування за спаданням χ² (або зростанням *p*) виокремить ознаки, найбільш асоційовані з класами. Зауважимо: χ² вимагає достатнього очікуваного числа спостережень у клітках (зазвичай ≥5). Для числових ознак його можна застосовувати після дискретизації, але це ускладнює гру (потрібно бінувати). У практиці sklearn в `SelectKBest` для неперервних ознак часто використовують `chi2`, але потрібно попередньо мати невід’ємні значення ознаки та бінування. Загалом, χ² корисний для категорій або уже бінованих числових – якщо *p* мале, ознака «окрилена» важлива. Його результат можна порівнювати з інформаційними мірами (MI, IV) — він теж виявляє залежність розподілів між класами.\n",
    "\n",
    "**14. Кореляція Пірсона**\n",
    "- **Доцільність:** Оцінює *лінійну* залежність між числовою ознакою і бінарною мішенню (у такому разі це по суті **point-biserial** кореляція). Підтримує тільки числові ознаки (цілі 0/1). Імовірно, для ранжування ознак беруть абсолютне значення r.\n",
    "- **Коментар:** Формально Pearson кореляція між безперервною X і бінарною Y є точково-бісеріальною. Вона відображає різницю середніх між двома класами відносно загальної дисперсії. Але маємо застереження: перехід «ціле = 0/1» на кореляцію може бути нестандартним. Дискусія в DS.StackExchange стверджує, що при бінарному таргеті краще використовувати спеціальну формулу point-biserial, а не класичний Pearson ([feature selection - If my target variable is binary, is it better to use Pearson's or Spearman's for my correlation vector? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/121107/if-my-target-variable-is-binary-is-it-better-to-use-pearsons-or-spearmans-for#:~:text=Pearson%27s%20correlation%20coefficient%20measures%20the,either%20of%20these%20correlation%20coefficients)). Тобто Pearson тут – умовно прилад. У будь-якому разі він буде високим, якщо між двома класами велика різниця за середнім і мало розсіяння. Однак його інтерпретація складніша: наприклад, сильний нелінійний зв’язок не відобразиться. Для рангуємо за |r| — ідеально, якщо ми можемо вважати ознаку приблизно нормальною. Будь-які non-linear залежності Pearson ігнорує (тут на допомогу можуть прийти dCor/HHG/MIC). І ще: за цифрової кодування 0/1, Pearson автоматично враховує розподіл класів (point-biserial враховує *p*(0)*p*(1)), отже вона впливає на величину r. З цієї причини **Spearman** може дати інший порядок, особливо при сильно несбалансованих класах ([feature selection - If my target variable is binary, is it better to use Pearson's or Spearman's for my correlation vector? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/121107/if-my-target-variable-is-binary-is-it-better-to-use-pearsons-or-spearmans-for#:~:text=Pearson%27s%20correlation%20coefficient%20measures%20the,either%20of%20these%20correlation%20coefficients)).\n",
    "\n",
    "**15. Distance Correlation (dCor)**\n",
    "- **Доцільність:** Потужна міра залежності будь-якого типу для пар випадкових векторів. Звичайне застосування – числова ознака проти числового таргету (або бінарного, якщо подати як 0/1). Distance correlation = 0 тільки за незалежності, і >0 за будь-якої залежності (лінійної чи нелінійної). Діапазон [0,1].\n",
    "- **Коментар:** Основна перевага distance correlation – **здатність виявляти довільні зв’язки**, навіть дуже складні (включно з нелінійними) ([PiML Toolbox](https://selfexplainml.github.io/PiML-Toolbox/_build/html/guides/data/feature_select.html#:~:text=This%20is%20a%20measure%20of,Y)). Тобто вона узагальнює ідею кореляції: якщо *X* і *Y* незалежні, dCor=0; якщо є залежність, dCor>0. Її недолік – обчислення потребує побудови матриці парних відстаней, що має *O(n²)* за часом ([PiML Toolbox](https://selfexplainml.github.io/PiML-Toolbox/_build/html/guides/data/feature_select.html#:~:text=The%20main%20advantage%20of%20distance,downsample%205000%20samples%20from%20the)). Але для наших ~87k записів зазвичай можна розрахувати її набагато швидше (пакет `dcor` оптимізований). Для ранжування ознак ознаки впорядковують за величиною dCor (більше – сильніший зв’язок). На відміну від Pearson/Spearman, дCor не лінійна, тому для нефункціональних залежностей (напр. «U-подібних») вона покаже силу. На практиці в контексті відбору ознак використовується рідше (через затрати), але здатна виявити те, що інші методи пропустять ([PiML Toolbox](https://selfexplainml.github.io/PiML-Toolbox/_build/html/guides/data/feature_select.html#:~:text=This%20is%20a%20measure%20of,Y)).\n",
    "\n",
    "**16. Heller–Heller–Gorfine (HHG)**\n",
    "- **Доцільність:** Непараметричний тест незалежності двох векторів (ознака та таргет). Границі розроблено для потужного виявлення будь-яких залежностей між ознакою та цільовою змінною. Може застосовуватись для числових ознак і будь-якого типу мішені (включно з бінарною).\n",
    "- **Коментар:** HHG відзначається консистентністю щодо *будь-якої* форми залежності ([hhg.test: Heller-Heller-Gorfine Tests of Independence and Equality of... in HHG: Heller-Heller-Gorfine Tests of Independence and Equality of Distributions](https://rdrr.io/cran/HHG/man/hhg.test.html#:~:text=The%20HHG%20test%20,offer%20significantly%20more%20power%20than)). Іншими словами, якщо між ознакою і таргетом є довільна зв’язність (групова, локально-лінійна, комбінована тощо), HHG-процедура з великою ймовірністю її виявить, на відміну від більш простих тестів. Однак за складністю це тест з пермутаціями (представлений у R-пакеті **HHG**), тому безпосередньо він не дає простого «рейтинг» – скоріше *p*-значення чи статистику тесту. У практичному ранжуванні можна використовувати -log(p) чи статистику HHG. З наукової точки зору HHG перевершує багато альтернатив у потужності, особливо у пошуку нетривіальних зв’язків ([hhg.test: Heller-Heller-Gorfine Tests of Independence and Equality of... in HHG: Heller-Heller-Gorfine Tests of Independence and Equality of Distributions](https://rdrr.io/cran/HHG/man/hhg.test.html#:~:text=The%20HHG%20test%20,offer%20significantly%20more%20power%20than)). Для наших даних це означає: HHG теоретично знайде залежності, які деякі інші методи пропустять. Але на практиці кількість ознак і необхідність пермутацій можуть зробити обчислення важким.\n",
    "\n",
    "**17. Статистика Хеффдінга (Hoeffding’s D)**\n",
    "- **Доцільність:** Ще одна загальна непараметрична міра асоціації. Обчислює статистику D, яка виявляє навіть нелінійні зв’язки між змінними. Підтримує числові ознаки; за бінарної мішені вона все одно здатна оцінити залежність.\n",
    "- **Коментар:** Hoeffding’s D – це старіша концепція «криволінійного» кореляційного показника. Як показано в прикладах SAS, D ≈0 для незалежних змінних, але істотно відрізняється від нуля, якщо між ними існує (навіть квадратична чи інша) залежність, яку Pearson ігнорує ([Examples of using the Hoeffding D statistic - The DO Loop](https://blogs.sas.com/content/iml/2021/05/03/examples-hoeffding-d.html#:~:text=Both%20statistics%20,their%20Pearson%20correlation%20is%200)). У контексті відбору ознак можна брати як міру залежності (чим більше D, тим сильніший зв’язок). Статистика не має знаку (є [−0.5,1]), тому важить лише величину. Перевага – виявляє будь-яку кореляцію; недолік – вимагає інтенсивного підрахунку рангових індексів та є чутливим до викидів. Також результат (D) часто менший за потужніших дCor/HHG. Але ідеологічно Hoeffding D теж «генеральний детектор» залежності, тому його можна використовувати там, де кореляція ≈0.\n",
    "\n",
    "**18. Maximal Information Coefficient (MIC)**\n",
    "- **Доцільність:** Нова інформаційно-ентропійна міра асоціації (клас MINE). Оцінює силу лінійних і нелінійних зв’язків двох змінних на інтервалі [0,1]. Для числових ознак і бінарної цілі можна обчислити MIC, який шукає оптимальне бінування, що максимізує інформацію (взаємну інформацію) між ознакою і класом.\n",
    "- **Коментар:** MIC у теорії призначений знайти *будь-яку* залежність і ранжувати їх, незалежно від форми. Вважається, що MIC здатний виявляти лінійні та складні зв’язки (мінус – вкрай витратне обчислення). Згідно з дослідженнями, MIC показав кращі результати за деякі прості тести, але за малих вибірок його потужність нижча, ніж у distance correlation або HHG ([Maximal information coefficient - Wikipedia](https://en.wikipedia.org/wiki/Maximal_information_coefficient#:~:text=The%20MIC%20belongs%20to%20the,equitability%20which%20is%20illustrated%20by)). Тобто MIC може бути менш чутливим в наших даних при обмеженій кількості випадків в деяких бінах. Попри це, в ідеалі MIC може бути застосований як універсальний показник: чим ближче до 1, тим міцніша залежність. Для практики: MIC подібний до mutual information, але з автоматизованим вибором бінів ([Maximal information coefficient - Wikipedia](https://en.wikipedia.org/wiki/Maximal_information_coefficient#:~:text=In%20statistics%2C%20the%20maximal%20information,between%20two%20variables%20X%20and%C2%A0Y)). Такий метод варто використовувати, якщо підозрюються складні зв’язки, які прості метрики не помічають. Але на відміну від простих статистик, його треба обчислювати через спеціальні бібліотеки (наприклад, minepy).\n",
    "\n",
    "## Групи методів, що дублюють один одного\n",
    "\n",
    "- **Аналіз середніх (Welch та ANOVA):** *t*-тест Велча та ANOVA F-статистика за двох класів мають однакову математичну суть (перевірка різниці середніх). У двохкласовій задачі F-тест фактично є квадратом t (за однакових розмірів вибірок), тому ці методи ранжуватимуть ознаки в подібному порядку. Обидва оцінюють, наскільки середні значення ознаки розрізняються між класами.\n",
    "\n",
    "- **Рангові методи (Mann–Whitney та AUC):** Ці два фільтри тісно пов’язані: AUC на одновимірній ознаці рівно відповідає статистиці Манна–Уітні ([An AUC-based permutation variable importance measure for random forests | BMC Bioinformatics | Full Text](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-119#:~:text=observation%20from%20class%20Y%20%3D,observation%20from%20the%20other%20class)). Іншими словами, якщо ознака успішно розділяє класи, U-тест і AUC дадуть узгоджений результат (високе значення U означає AUC близько до 1). Тому ці методи фактично *дублюють* один одного з погляду ранжування – обидва впорядковують ознаки за силою рангового розділення між класами.\n",
    "\n",
    "- **Кореляційні показники (Pearson, Spearman, логістичний коефіцієнт):** Всі три оцінюють зв’язок між ознакою і мішенню лінійно/монотонно. Кореляції Пірсона та Спірмена є взаємозамінними методами порівняння (Пірсон – лінійна асоціація, Спірмен – рангово-монотонна). Для двокласової мішені Пірсон еквівалентний точково-бісеріальній кореляції, логістичний коефіцієнт – це ще одна форма лінійної оцінки впливу ознаки. Тож у випадках, коли дані мають переважно лінійну або монотонну залежність, ці методи можуть давати схоже ранжування. (Хоча при категоричній мішені ідеальною є бісеріальна кореляція ([feature selection - If my target variable is binary, is it better to use Pearson's or Spearman's for my correlation vector? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/121107/if-my-target-variable-is-binary-is-it-better-to-use-pearsons-or-spearmans-for#:~:text=Pearson%27s%20correlation%20coefficient%20measures%20the,either%20of%20these%20correlation%20coefficients)), в багатьох випадках вищі ранги ознак збігаються між Pірсон-списком і ранжуванням за логістичними коефіцієнтами після стандартизації ([How To Get Feature Importance In Logistic Regression | Forecastegy](https://forecastegy.com/posts/feature-importance-in-logistic-regression/#:~:text=When%20the%20features%20are%20standardized%2C,the%20importance%20of%20each%20feature)).)\n",
    "\n",
    "- **Інформаційні міри (Information Value, Mutual Information, MIC):** Всі вони базуються на ентропії/інформації. IV і Mutual Information обчислюють, наскільки знання ознаки зменшує невизначеність класу. MIC за своєю суттю максимізує взаємну інформацію через вибір бінування. Хоч алгоритми різні, результат – міра «інформаційного» зв’язку з мішенню – близький. У практиці IV і MI часто показують схожі тренди: сильні ознаки обидва оцінять високо. MIC же є узагальненням концепції MI, тому теж корелюватиме з цими методами при сильних патернах, хоча технічно може ранжувати дещо по-іншому за мелких вибірок ([Maximal information coefficient - Wikipedia](https://en.wikipedia.org/wiki/Maximal_information_coefficient#:~:text=The%20MIC%20belongs%20to%20the,equitability%20which%20is%20illustrated%20by)). В будь-якому разі усі ці методи шукають інформативність ознаки щодо цілі, тож вони «дублюють» один одного у сенсі пошуку прогнозної сили.\n",
    "\n",
    "- **Тест незалежності (Chi-square) та інформаційні показники:** Кілька методів (особливо χ², Mutual Information і IV) виявляють відмінності в розподілах класів ознаки. Фактично χ² тест є статистичним критерієм незалежності (узагальнення інформаційної різниці між очікуваними та спостережуваними частотами). Тому χ² і MI (який в bінарному випадку пов’язаний з дивергенцією Кульбака–Лейблера) також можуть давати узгоджені результати щодо найсильніших категоріальних ознак. Іншими словами, χ² і інформаційні показники (IV/MI) часто ідентифікують однакові ознаки як значущі (ось чому їх групують як аналогічні фільтри).\n",
    "\n",
    "- **Альтернативи кореляції (dCor, HHG, Hoeffding, MIC):** Ці чотири методи – це сучасні генералізовані міри асоціації, які намагаються виявити довільні залежності. Вони відрізняються формулами, але мета у них одна – детектувати, чи залежні ознака і мішень. На практиці часто виявляється, що вони *покривають одні й ті ж ситуації*: якщо існує будь-яка складна залежність, майже всі ці методи її помітять. Наприклад, МIС, dCor і HHG за малих вибірок в дослідах показували схожу роботу (хоч МIС може гірше тримати потужність ([Maximal information coefficient - Wikipedia](https://en.wikipedia.org/wiki/Maximal_information_coefficient#:~:text=The%20MIC%20belongs%20to%20the,equitability%20which%20is%20illustrated%20by))). Тому їх можна об’єднати в групу «загальних критеріїв залежності»: вибір конкретного методу для ранжування часто менш важливий, ніж факт, що ця група методів виявлятиме ті самі ключові ознаки.\n",
    "\n",
    "- **Моделі на деревах (Decision Tree, Random Forest, XGBoost):** Хоч формули підрахунку важливості відрізняються, всі ці методи ґрунтуються на деревних моделях та оцінюють внесок ознаки через спліти. DecisionTree та RandomForest зазвичай використовують падіння нечистоти, XGBoost – приріст функції втрат (gain), але концептуально вони всі знаходять ознаки, які найчастіше і найефективніше розрізняють класи через розділи. Таким чином, ці методи утворюють групу: вони *дублюють* один одного у тому сенсі, що при однакових даних вони часто виявлять схожий набір важливих ознак (з незначними відмінностями через параметри навчання). Випадковий ліс і XGBoost іноді розглядають окремо через відмінності алгоритмів (bagging vs boosting), але загалом їх класифікують спільно як «деревні» підходи.\n",
    "\n",
    "Усі вищезгадані групи базуються на спільних статистичних ідейних засадах: наприклад, t-тест та ANOVA – обидва порівнюють середні; Mann–Whitney та AUC – рангові; інформаційні міри – ентропія; кореляції та коефіцієнти – лінійна асоціація. Тому ознаки, визнані важливими в одній зі схожих методик, як правило, будуть важливими і в інших методах тієї ж групи.\n",
    "\n",
    "**Джерела:** Ключові методологічні положення взято з науково-технічних ресурсів і документації (зокрема, щодо взаємозв’язку AUC і Mann–Whitney ([An AUC-based permutation variable importance measure for random forests | BMC Bioinformatics | Full Text](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-119#:~:text=observation%20from%20class%20Y%20%3D,observation%20from%20the%20other%20class)), загальної потужності методів HHG dCor MIC ([Maximal information coefficient - Wikipedia](https://en.wikipedia.org/wiki/Maximal_information_coefficient#:~:text=The%20MIC%20belongs%20to%20the,equitability%20which%20is%20illustrated%20by)) ([hhg.test: Heller-Heller-Gorfine Tests of Independence and Equality of... in HHG: Heller-Heller-Gorfine Tests of Independence and Equality of Distributions](https://rdrr.io/cran/HHG/man/hhg.test.html#:~:text=The%20HHG%20test%20,offer%20significantly%20more%20power%20than)), а також приклади використання t-тесту і логістичної регресії для ранжування ознак ([How To Get Feature Importance In Logistic Regression | Forecastegy](https://forecastegy.com/posts/feature-importance-in-logistic-regression/#:~:text=When%20the%20features%20are%20standardized%2C,the%20importance%20of%20each%20feature))). Ці посилання ілюструють, що зазначені методи дійсно спираються на однакові статистичні ідеї та часто дають узгоджені результати при відборі важливих ознак."
   ],
   "id": "c678e85ca355f88d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d0b28f06d523df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
